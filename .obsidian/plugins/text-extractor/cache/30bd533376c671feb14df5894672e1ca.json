{"path":"کانال حکایت/محتوای کانال/ویدیوها/6/Telegram Desktop/seifi-ehsan-converting-a-1-k-static-rayleigh-channel.pdf","text":"Converting a 1 × K Static Rayleigh Channel to K Parallel AWGN Using Media-based Modulation Ehsan Seiﬁ† and Amir K. Khandani‡ ‡E&CE Dept. University of Waterloo, Waterloo, ON, Canada, †Ciena Inc. Ottawa, ON, Canada Abstract—The idea of media-based modulation (MBM) [1] [2] is to embed information in the variations of the transmission media (channel states). Using a single traditional antenna sur- rounded by a closure with w radio frequency (RF) walls, MBM creates a set of 2w states for the end-to-end channel, and the data is mapped into the index of these channel states. Each channel state results in an independent complex channel gain to each receive antenna, which speciﬁes an MBM constellation point coordinate. In a rich scattering environment, MBM constellation points are independent of each other, with coordinates that follow an independent identically distributed (i.i.d.) complex Gaussian density. In a 1 × K MBM, this property mimics the random code-book generation for signaling over K parallel additive white Gaussian noise (AWGN) channels. Accordingly, it is shown in [2] that the capacity of a 1 × K MBM system with one unit of transmit energy and AWGN variance σ2 over each receive antenna is equal to K times the capacity of an AWGN with a signal to noise ratio of snr = 1/σ2. The current article provides an alternative proof based on a novel formulation that reveals several interesting features of MBM. It is shown that the capacity in a 1 × K MBM, as a random variable deﬁned over the sample space of MBM constellation of cardinality M , follows a normal distribution with a mean of K log(1 + snr) and variance of (K/M )(snr/(1 + snr))2 → 0 as the M → ∞. This entails, in contrast to legacy MIMO where the singularity of the channel matrix governs the outage probability, in MBM the outage is determined by the realized energy of the MBM constellation, and for any multiplexing gain r < K, the outage probability decreases exponentially fast as the number of points increases. I. INTRODUCTION The idea of media-based modulation (MBM), falling under the more general umbrella of index modulation, is based on embedding information in the variations of the transmission media (channel state). This is in contrast to legacy wireless systems where data is embedded in a radio frequency (RF) source prior to the transmit antenna. Particularly, MBM relies on a single transmit antenna placed within a closure sur- rounded by walls that each acts as an on-off mirror. Transmit antenna sends a single pulse-shaped tone. An on-mirror would pass the incident wave, and an off-mirror would reﬂect it towards the interior of the RF closure. As a result, the wave from the transmit antenna bounces back and forth within the RF closure and in the process, propagates outside from on- mirrors. For an RF closure with w RF mirrors, this system creates a set of 2 w states for the end-to-end channel, each state representing an index of an MBM constellation point. A primary distinction of MBM, in comparison with other known methods categorized under index modulation, is the exponential increase in the number of channel states in terms of the number of parasitic RF elements (called RF mirrors) which surround a single transmit antenna. This property results in several advantages vs. legacy systems, including ”additivity of information over multiple receive antennas”, and ”inherent diversity over a static fading channel”. MBM is particularly suitable for transmitting high data rates using single transmit and multiple receive antennas (single input-multiple output media-based modulation, or SIMO-MBM). For a review of spatial and index modulation, the reader is referred to [3]. Fig. 1 shows a 1 × K = Q/2 SIMO-MBM system with K receive antennas and Q real dimensions at the receiver. There are M messages and log2 M RF mirrors. Each message index m ∈ {0, ..., M − 1} selects a unique on-off combination for the RF mirrors, and consequently a different K-dimensional complex channel gain cm will be realized at the receiver. Building on the arguments in [2], showing that in a static Rayleigh fading channel mutual information of a 1 × K SIMO-MBM converges to the capacity of K parallel AWGN channels, this article further provides the asymptotic distribu- tion of mutual information. We will show that in a 1 × K SIMO-MBM, mutual information is asymptotically normally concentrated around K log(1 + snr). That is, I ∼ N (K log(1 + snr), σ2 M ), (1) and the variance vanishes according to σ2 M = K M ( snr 1 + snr )2 , (2) with an increase in the number of constellation points M . In contrast to legacy MIMO where the singularity of the channel matrix determines the outage probability [4], in MBM the outage is determined by the realized energy of the pro- jected signal points. Furthermore, for any multiplexing gain r < K, the outage probability decreases exponentially fast as the number of points M increases. This is because the realized energy concentrates around its statistical average snr. Using the Taylor expansion for functionals (functions acting on functions), we closely approximate the entropy of Gaussian mixture in terms of its mixing components by the ﬁrst two terms in the Taylor expansion and bounding the rest. We show the remaining terms converge to zero in probability as op(1/ √M ). This approximation for the entropy eliminates the need to compute high-dimensional integrals. The leading term of the entropy is determined by the Euclidean norm of the mixing components and we use this to compute mutual information in MBM. 2022 IEEE International Symposium on Information Theory (ISIT) 978-1-6654-2159-1/22/$31.00 ©2022 IEEE31092022 IEEE International Symposium on Information Theory (ISIT) | 978-1-6654-2159-1/22/$31.00 ©2022 IEEE | DOI: 10.1109/ISIT50566.2022.9834835 Fig. 1. Media-based system. II. SYSTEM MODEL Denote the set of all channel gains (due to all possible on- off combinations of RF mirrors) by CM := {c0, c2, ..., cM −1}. Throughout the paper we will refer to CM as the set of MBM constellation points. In a static Rayleigh fading channel, the elements of MBM constellation set, cm, are independent and identically distributed (i.i.d.) according to multivariate normal distribution N (0, snrI2K), with probability density ϕsnr(c) = 1 (2πsnr)K exp ( − ∥c∥ 2 2snr ). (3) Here, snr is the statistical average of signal energy at each receive dimension. The channel gains are random but held ﬁxed in successive transmissions over time. Even though the transmitter selects the message index m, it is does not know the realized value of channel gains projected at the receiver. Receiver, however, is trained using pilot symbols with the knowledge of the constellation set CM . Assuming additive white Gaussian noise (AWGN), the noisy projected signals at the receiver y is equal to y = cm + z. (4) AWGN noise vector z has i.i.d. components distributed ac- cording to N (0, I2K). III. MBM MUTUAL INFORMATION IN A STATIC RAYLEIGH FADING CHANNEL Let us assume constellation points are selected with equal probability of 1/M . The empirical probability mass function over the constellation set is ˆPCM = 1 M M∑ i=1 δ(ci), (5) where δ(ci) is the dirac delta measure at point ci. Let c denote the random variable drawn from distribution ˆPCM . MBM mutual information I(c; c + z) is a random variable whose value depends on particular realization of MBM constellation set CM . In the presence of Additive White Gaussian Noise (AWGN) with unit variance, the output distribution is given by convolution with Gaussian density, indicated by ˆPCM ∗ ϕ1. Accordingly, empirical mutual information is I(CM ) := I(c; c + z) = h( ˆPCM ∗ ϕ1) − h(ϕ1). (6) Function h denotes the differential entropy. The output distri- bution, ˆPCM ∗ ϕ1, is a Gaussian mixture, where the mixture components are the realized constellation points. In a static Rayleigh fading channel, as the cardinality of the constellation set grows, MBM asymptotically achieves the capacity of K parallel AWGN channels. We will show mutual information, as a random variable over the sample space of MBM constellations of cardinality M , is concentrated around the capacity of K parallel Gaussian channel according to I (CM ) = K log (1 + snr) + K M M∑ i=1 ( 1 + ∥ci∥ 2 1 + snr − 1 ) + op ( 1 √M ) . (7) The second term is the deviation of the realized energy from its statistical average snr. Consequently, I(CM ) follows a central limit law, and asymptotically converges to a normal distribution centered around K log(1 + snr). More precisely, denote the deviations from AWGN capacity by ∆(CM ) := K log(1 + snr) − I(CM ), then √M ∆ (CM ) D → N (0, K( snr 1 + snr )) . (8) The asymptotic variance is solely a function of signal to noise ratio and K. Such a formulation also characterizes the MBM outage capacity. Outage is deﬁned as the event that the mutual information of a realization does not support a target rate R, i.e., {CM : I(CM ) < R} . (9) Due to the normal asymptotic of the deviation, in the regime of large constellation sets, outage probability is exponentially sharp and given by Pout := Pr {CM : I(CM ) < R} (10) = Q ( √M (K log(1 + snr) − R) √Ksnr/(1 + snr) ), (11) where Q is the tail distribution function of the standard normal distribution. IV. GAUSSIAN MIXTURE ENTROPY The following identity holds for two arbitrary distributions g(z) and f (z). h(f ) − h(g) = ∫ (g − f ) log f dz + DKL(g ∥ f ), (12) where DKL denotes the Kullback-Leibler divergence, DKL (g ∥ f ) = ∫ g log g f dz. (13) One can interpret (12) as functional Taylor expansion (or von Misses expansion [5]) of second order for Shannon entropy. If 2022 IEEE International Symposium on Information Theory (ISIT) 3110 functional h(f ) is expanded about g, the second term of the series is given by integral in (12) and the DKL gives the rest of the terms. Let us specialize (12) to get the expression for the entropy of Gaussian mixture distribution. By substituting g = ˆPCM ∗ϕ1 and f = ϕ1+snr, we obtain K log 2πe(1 + snr) − h( ˆPCM ∗ ϕ1) = ∫ ( ˆPCM ∗ ϕ1(z) − ϕ1+snr(z)) log ϕ1+snr(z) dz + DKL( ˆPCM ∗ ϕ1 ∥ ϕ1+snr). (14) Since difference in differential entropy of output signals is equal to difference in mutual information, (14) is also the deviation of MBM mutual information from AWGN capacity. ∆(CM ) = K log(1 + snr) − I(CM ) (15) = K log 2πe(1 + snr) − h( ˆPCM ∗ ϕ1) (16) = ∫ ( ˆPCM ∗ ϕ1(z) − ϕ1+snr(z) ) log ϕ1+snr(z) dz + DKL( ˆPCM ∗ ϕ1 ∥ ϕ1+snr). (17) We obtain the asymptotic distribution of random variable ∆(CM ) by separately analysing the asymptotic of the two terms in the sum. ∆(CM ) = ∆ ′(CM ) + ∆′′(CM ), (18) where ∆′(CM ) := ∫ ( ˆPCM ∗ ϕ1(z)−ϕ1+snr(z)) log ϕ1+snr(z)dz, (19) and ∆′′(CM ) := DKL( ˆPCM ∗ ϕ1 ∥ ϕ1+snr). (20) In section V, we show that √M ∆′(CM ) converges to normal distribution and √M ∆′′(CM ) converges to zero in probability. More speciﬁcally, √M ∆′ (CM ) D → N ( 0, K( snr 1 + snr )2). (21) √M ∆′′ (CM ) P → 0. (22) Claims (21) and (22) together establish the asymptotic con- vergence of 1 × K MBM mutual information to the capacity of K parallel Gaussian channels, that is, I(CM ) ∼ N ( K log(1 + snr), K M ( snr 1 + snr )2). (23) V. CONVERGENCE TO AWGN: ASYMPTOTIC NORMALITY A. Asymptotics of ∆′(CM ) Straight forward integration under Gaussian measure gives the following closed form expression for ∆′ (CM ). ∫ ( ˆPCM ∗ ϕ1(z) − ϕ1+snr(z)) log ϕ1+snr(z) dz = ∫ ( ˆPCM ∗ ϕ1(z)) log ϕ1+snr(z) dz + h(ϕ1+snr) (24) = K M M∑ i=1 (1 − 1 + 1 2K ∥ci∥ 2 1 + snr ). (25) Since MBM constellation points are i.i.d. and E∥ci∥2 = 2Ksnr, (25) is sum of i.i.d. random variables with zero mean. Hence, by central limit theorem (CLT), √M ∆′(CM ) converges to normal distribution. Furthermore, ∥ci∥ 2/snr is distributed according to a chi-squared distribution with 2K degrees of freedom. Therefore, the asymptotic variance of (25) is given by expectation under chi-squared distribution, σ2 M (snr, K) := 1 M E [ K 2(1 − 1 + 1 2K ∥ci∥ 2 1 + snr )2] (26) = K M ( snr 1 + snr )2. (27) Equation (25) implies that the leading asymptotic behavior of outage is determined by how the realized constellation energy (channel gains) deviates from the expected energy snr. A higher realized energy translates to a higher mutual information. B. Asymptotics of ∆′′ (CM ) Next, we establish that the sequence √M ∆′′ (CM ) con- verges to zero in probability. Applying the inequality log(1 + x) ≤ x, for x ≥ 0, gives the following known relationship between Kullback-Leibler and χ 2 divergence of two distribu- tions. 0 ≤ DKL(g ∥ f ) = ∫ g log g f dz (28) ≤ ∫ g ( g f − 1) dz (29) = −1 + ∫ g2 f dz (30) := χ 2 (g ∥ f ) . (31) Since both DKL and χ 2 are always positive, to prove √M ∆′′ (CM ) P → 0, it is sufﬁcient to show that √M χ2 ( ˆPCM ∗ ϕ1) P → 0. (32) 2022 IEEE International Symposium on Information Theory (ISIT) 3111 The divergence χ 2 possesses a closed form for divergence between a Gaussian mixture distribution and a Gaussian dis- tribution. χ 2 ( ˆPCM ∗ ϕ1 ∥ ϕ1+snr) = ∫ ( ˆPCM ∗ ϕ1)2 ϕ −1 1+snrdz− 1 = 1 M 2 ∫ ∑ i ϕ1 (z − ci) ∑ j ϕ1 (z − cj) ϕ −1 1+snr(z)dz− 1 = 1 M 2 ∑ i ∑ j ∫ ϕ1(z − ci)ϕ1(z − cj)ϕ −1 1+snr(z)dz − 1 = 1 M 2 ∑ i ∑ j A(ci, cj) − 1 = 1 M 2 ∑ i A(ci, ci) + 1 M 2 ∑ i ∑ j>i A(ci, cj) − 1, (33) where A(ci, cj) is deﬁned as, A(ci, cj) := ∫ ϕ1(z − ci)ϕ1(z − cj)ϕ −1 1+snr(z)dz. (34) The ﬁrst term in (33) is the sum of M i.i.d. random variables multiplied by 1/M 2. Let VM := 1 M 2 ∑ i A(ci, ci). (35) Note that A(ci, ci) ≥ 0, hence E \f \f√M VM \f \f = EA(ci.ci) √M . (36) Appendix A shows that EA(ci, ci) < ∞, which in turn entails E \f \f√M VM \f \f = 0 as M → ∞. Furthermore, since convergence in r-th moment for r ≥ 1 implies convergence in probability (by Markov’s inequality), it follows √M VM P → 0. (37) The second term is permutation symmetric summation which can be thought of as U-statistics of second order with kernel A(ci, cj). Hoeffding introduced U-statics and studied its asymptotics in [6]. The variance for U-statistics has an explicit form (e.g. see [7], chapter 12) which is described next. Let UM := 1 M 2 ∑ i ∑ j>i A(ci, cj) − 1. (38) The variance of such a second order U-statistics is given by Var(UM ) = ζ1 M + ζ2 M (M − 1) , (39) where ζ1 := Var(E [ A(ci, cj)\f \fci]), (40) and ζ2 := Var(A(ci, cj)). (41) Fig. 2. Histogram of mutual information for MBM with 512 constellation points, K = 1, SNR = 5 (dB), and K log2(1 + snr) = 2.0574. Appendix B establishes that ζ1 = 0, and ζ2 < ∞, which means Var(UM ) = O(1/M 2). This proves that Var( √M UM ) = 0 as M → ∞. Convergences in probability follows by convergence in second moment. Therefore, √M UM P → 0. (42) Recalling from (33) that χ 2( ˆPCM ∗ ϕ1 ∥ ϕ1+snr) = VM + UM , together with (37) and (42), we have, √M χ 2(c m) P → 0. (43) This in turn concludes √M ∆′′ (CM ) P → 0. VI. NUMERICAL RESULTS As an example, Fig. 2 depicts the simulated histogram for MBM mutual information, for a constellation sets with 512 points (i.e. 9 RF mirrors). The histogram closely follows a normal distribution with variance 1.1 × 10−3, while the analytical variance (27) predicts 1.12 × 10−3. The simulated values provided in Table I demonstrate how the mean and variance of ∆(CM ) = K log(1 + snr) − I(CM ) vanishes as the number of RF mirrors increases. Notice that the variance of deviations, M E∆2(CM ), are decreasing and converge to the value obtained via (27) for M = ∞. TABLE I EXAMPLE OF MBM MUTUAL INFORMATION MEAN AND VARIANCE AS A FUNCTION OF NUMBER OF RF MIRRORS∗. log2 M 6 7 8 9 ∞ M E [∆(CM )]2 0.755 0.666 0.629 0.592 0.578 ∗∗ E∆(CM ) 0.062 0.033 0.016 0.007 0 ∗K = 1, SNR = 5 (dB), and K log2(1 + snr) = 2.0574. ∗∗ Obtained via analytical expression in (27). 2022 IEEE International Symposium on Information Theory (ISIT) 3112 APPENDIX A EXPECTED VALUE FOR A(ci, ci) Replacing both arguments with ci in (34), yields EA(ci, cj) = ∫ Eϕ 2 1(z − ci)ϕ−1 1+snr(z)dz. (44) Using the identity ϕ 2 a = ϕ a 2 (4πa)K , (45) it follows, EA(ci, cj) = 1 (4πa)K ∫ Eϕ 1 2 (z − ci)ϕ−1 1+snr(z)dz (46) = 1 (4πa)K ∫ ϕ 1 2 +snr(z)ϕ −1 1+snr(z)dz < ∞. (47) Bound (47) is valid because 1 2 + snr < 1 + snr. APPENDIX B VALUES FOR ζ1 AND ζ2 A. Case ζ1 = 0 To prove ζ1 = Var(E [ A(ci, cj) \f \fci]) is equal zero, ﬁrst let us compute the conditional expectation E [ A(ci, cj) \f \fci] = E [ ∫ ϕ1(z − ci)ϕ1(z − cj)ϕ−1 1+snr(z)dz\f \fci ] (48) = ∫ ϕ1(z − ci)E [ ϕ1(z − cj)] ϕ −1 1+snr(z)dz (49) = ∫ ϕ1(z − ci)ϕ1+snr(z)ϕ −1 1+snr(z)dz (50) = ∫ ϕ1(z − ci)dz = 1. (51) Since E[A(ci, cj)\f \fci] is a constant, its variance ζ1 = 0. B. Case ζ2 < ∞ The following sequence of operations establishes that ζ2 is bounded. EA 2(ci, cj) = E [ ∫ ϕ1(z−ci)ϕ1(z−cj)ϕ −1 1+snr(z)dz ]2 . (52) Replacing the product of integrals by integrals of products gives, E ∫ ∫ 2∏ k=1 [ ϕ1(zk − ci)ϕ1(zk − cj)ϕ −1 1+snr(zk)dzk] . (53) Because ci and cj are independent, expectation of products is equal to product of expectations. ∫ ∫ E [ 2∏ k=1 ϕ1(zk − ci)] E [ 2∏ k=1 ϕ1(zk − cj)] × 2∏ k=1 ϕ −1 1+snr(zk)dzk (54) Since the two random variables are identically distributed, the two expectations are equal. ∫ ∫ [ E 2∏ k=1 ϕ1(zk − ci) ]2 2∏ k=1 ϕ −1 1+snr(zk)dzk (55) The product of two Gaussian densities is also a Gaussian function with new mean and variance (for details, see [8]). Consequently with some algebra, we get ∫ ∫ [Eϕ2(z1 −z2)ϕ 1 2 (ci − z1 + z2 2 ) ]2 2∏ k=1 ϕ −1 1+snr(zk)dzk (56) Eϕ1/2 (ci − z+z2 2 ) is obtained via convolution. ∫ ∫ ϕ 2 2(z1 − z2)ϕ2 1 2 +snr( z1 + z2 2 ) 2∏ k=1 ϕ −1 1+snr(zk)dzk (57) We proceed by using (45). ∫ ∫ 1 (8π)K 1 (2π(1 + 2snr))K ϕ1(z1 − z2) × ϕ 1+2snr 4 ( z1 + z2 2 ) 2∏ k=1 ϕ−1 1+snr(zk)dzk (58) The double integrals can be decomposed by change of vari- ables u := z1 − z2 and v := 1 2 (z1 + z2). The Jacobian for this linear transformation equals 1. Change of variables gives, 1 (16π2(1 + 2snr))K ∫ ∫ ϕ1(u)ϕ 1+2snr 4 (v) × ϕ −1 1+snr(v − u 2 )ϕ −1 1+snr(v + u 2 )dudv (59) = 1 (16π2(1 + 2snr))K ∫ ϕ1(u)ϕ −1 2(1+snr)(u)du × ∫ ϕ 1+2snr 4 (v)ϕ−1 1+snr 2 (v)dv = ( (1 + snr)2 1 + 2snr )2K<∞. (60) REFERENCES [1] A. K. Khandani, “Media-based modulation: A new approach to wireless transmission,” in 2013 IEEE International Symposium on Information Theory, 2013, pp. 3050–3054. [2] ——, “Media-based modulation: Converting static rayleigh fading to awgn,” in 2014 IEEE International Symposium on Information Theory, 2014, pp. 1549–1553. [3] N. Ishikawa, S. Sugiura, and L. H. Hanzo, “50 years of permutation, spa- tial and index modulation: From classic rf to visible light communications and data storage,” IEEE Communications Surveys & Tutorials, vol. 20, pp. 1905–1938, 2018. [4] L. Zheng and D. Tse, “Diversity and multiplexing: a fundamental tradeoff in multiple-antenna channels,” IEEE Transactions on Information Theory, vol. 49, no. 5, pp. 1073–1096, 2003. [5] R. v. Mises, “On the Asymptotic Distribution of Differentiable Statistical Functions,” The Annals of Mathematical Statistics, vol. 18, no. 3, pp. 309 – 348, 1947. [Online]. Available: https://doi.org/10.1214/aoms/ 1177730385 [6] W. Hoeffding, “A class of statistics with asymptotically normal distribu- tions,” Annals of Mathematical Statistics, vol. 19, no. 3, pp. 293–325, 1948. [7] A. W. v. d. Vaart, Asymptotic Statistics, ser. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998. [8] P. A. Bromiley, “Products and convolutions of gaussian probability density functions,” 2013. 2022 IEEE International Symposium on Information Theory (ISIT) 3113","libVersion":"0.2.3","langs":""}