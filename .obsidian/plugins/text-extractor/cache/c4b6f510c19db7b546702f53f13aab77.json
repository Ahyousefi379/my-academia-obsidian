{"path":"کانال حکایت/محتوای کانال/ویدیوها/6/Telegram Desktop/@DataScience_ir - Mathematics of ML.pdf","text":"MATHEMATICS of MACHINE LEARNING by MARTIN LOTZ Mathematics Institute The University of Warwick March 2020 Contents Contents ii 1 Introduction 1 2 Overview of Probability 11 I Statistical Learning Theory 21 3 Binary Classiﬁcation 23 4 Finite Hypothesis Sets 27 5 Probably Approximately Correct 31 6 Learning Shapes 35 7 Rademacher Complexity 39 8 VC Theory 45 9 The VC Inequality 49 10 General Loss Functions 53 11 Covering Numbers 57 12 Model Selection 61 II Optimization 65 13 Optimization 67 14 Convexity 79 15 Lagrangian Duality 83 16 KKT Conditions 87 ii 17 Support Vector Machines I 91 18 Support Vector Machines II 95 19 Iterative Algorithms 101 20 Convergence 105 21 Gradient Descent 109 22 Extensions of Gradient Descent 115 23 Stochastic Gradient Descent 119 III Deep Learning 125 24 Neural Networks 127 25 Universal Approximation 133 26 Convolutional Neural Networks 137 27 Robustness 143 28 Generative Adversarial Nets 147 Bibliography 153 1 Introduction “No computer has ever been designed that is ever aware of what it’s doing; but most of the time, we aren’t either.” — Marvin Minsky, 1927-2016 Learning is the process of transforming information and experience into knowledge and understand- ing. Knowledge and understanding are measured by the ability to perform certain tasks independently. Machine Learning is therefore the study of algorithms and models for computer systems to carry out certain tasks independently, based on the results of a learning process. Learning tasks can range from solving simple classiﬁcation problems, such as handwritten digit recognition, to more complex tasks, such as medical diagnosis or driving a car. Machine learning is part of the broader ﬁeld of Artiﬁcial Intelligence, but distinguishes itself from more traditional approaches to problem solving, in which machines follow a strict set of rules they are provided with. As such, it is most useful for tasks such as pattern recognition, that may be simple for humans but where precise rules are hard to come by with, or for tasks that allow for simple rules, but where the complexity of the problem makes any rule-based approach computationally infeasible. An illustrative example of the latter is the recent success of DeepMind’s AlphaGo 1, a computer program based on reinforcement learning, at achieving super-human performance at the board game Go (围棋). Even though the rules of the game are simple, the task of beating the best human players seemed impossible only a decade ago due to the daunting complexity that ensues from the number of possible positions. A General Framework for Learning Machine learning lies at the intersection of approximation theory, probability theory, statistics, and optimization theory. We illustrate the interplay of these ﬁelds with a few basic examples. In its most basic form, the goal of machine learning is to come up with (learn) a function h : X → Y, where X is a space of inputs or features, and Y consists of outputs or responses. The input space X is usually modelled as a metric space (such as Rd), and the inputs could represent images, texts, emails, gene sequences, networks, ﬁnancial time series, or demographic data. The output could consist of quantitative values, such as a temperature or the amount of a certain substance in the body, or of 1https://deepmind.com/research/case-studies/alphago-the-story-so-far 1 qualitative or categorical values, such as {YES, NO} or {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. The ﬁrst type of problem is usually called regression, while the latter is called classiﬁcation. The function h is sometimes called a hypothesis, a predictor, or a classiﬁer. A classiﬁer h that takes only two values (typically 0 and 1, or −1 and 1) is called a binary classiﬁer. In a machine learning scenario, a function h is chosen from a predetermined set of functions H, called the hypothesis space. Machine learning problems can be subdivided into supervised and unsupervised learning problems. In supervised learning, we have at our disposal a collection of input-output data pairs {xi, yi}n i=1 ⊂ X × Y, and the goal is to learn a function h : X → Y from this data. The collection of pairs {(xi, yi)}n i=1 is called the training set. In unsupervised learning, one does not have access to a training set. The prototypical example of an unsupervised learning task is clustering, where the tasks is to subdivide a given data set into groups based on similarities. This course will deal mainly with supervised learning. Example 1.1 (Digit recognition). Given a dataset of pixel matrices, each representing a grey-scale image, with associated labels telling us for each image the number it represents, the task is to use this data to train a computer program to recognise new numbers (see Figure 1.1). Such classiﬁcation tasks are often carried out using deep neural networks, which constitute a powerful class of non-linear functions. Figure 1.1: The MNIST (Modiﬁed National Institute of Standards and Technology, http://yann. lecun.com/exdb/mnist/) dataset is a large collection of images of hand-written digits, and is a frequently used benchmark in machine learning. Example 1.2. (Clustering) In clustering applications, one observes data {xi}n i=1, and the goal is to subdivide the data into a number of distinct groups based on similarity, where similarity is measured using a distance function. Figure 1.2 shows an example of an artiﬁcial clustering problem and a possible 3 2 1 0 1 2 3 0 2 4 6 8 3 2 1 0 1 2 3 0 2 4 6 8 Figure 1.2: A collection of random points on the plane. The image on the right shows the four clusters as determined by the k-means algorithm. solution. The notion of distance used depends on the application. For example, for binary sequences or DNA sequences one can use the Hamming metric, which simply counts the positions at which two sequences differ. Clustering is used extensively in genetics, biology and medicine. For example, clustering can be used to identify groups of genes (segments of DNA with a function) that encode proteins which are part a common biological pathway. Other uses of clustering are market segmentation and community detection in networks. Approximation Theory One often makes the simpliﬁed assumption that the observed training data comes from an unknown function f : X → Y. The goal is to approximate the function f with a function h from a hypothesis class H, based only on the knowledge a ﬁnite set of samples {xi, yi}n i=1, where we assume yi ≈ f (xi) for i ∈ [n] := {1, . . . , n}. Which class of functions is adequate depends on the application at hand, as well as on computational and statistical considerations. In many cases a linear function will do well, while in other situations polynomials or more complex functions, like neural networks, are better suited. Example 1.3. (Linear regression) Linear Regression is the problem of ﬁnding a relationship of the form Y = β0 + β1X1 + · · · + βpXp, where the X1, . . . , Xp are covariates that describe certain characteristics of a system and Y is the response. Given a set of input-output pairs (xi, yi), arranged in a matrix X and a vector y, we can guess the correct β = (β0, β1, . . . , βp)⊤ by solving the least-squares optimization problem minimize β ∥Xβ − y∥2 2. Figure 1.3 shows an example of linear regression. Example 1.4. (Text classiﬁcation) In text classiﬁcation, the task is to decide to which of a given set of categories a given text belongs. The training data consists of a bag of words: this is a large sparse matrix, whose columns represent words and the rows represent articles, with the (i, j)-th entry containing the number of times word j is contained in text i. 2 4 6 8 10 12 Mass 2 4 6 8 10 12Basal metabolic rate 0 2 4 6 8 10 12 14 Mass 2 4 6 8 10 12 Figure 1.3: The relationship of mass to the logarithm of the basal metabolic rate in mammals. The data consists of 573 samples taken from the PanTHERA database, and the example featured in the episode Size Matters of the BBC series Wonders of Life. The right images shows the regression line determined using linear least squares. Goal Soup Article 1 5 0 Article 2 1 7 For example, in the above set we would classify the ﬁrst article as \"Sports\" and the second one as \"Food\". One such training dataset is the Reuters Corpus Volume I (RCV1) 2, an archive of over 800,000 categorised newswire stories. A typical binary classiﬁer for such a problem would be a linear classiﬁer of the form h(x) = w⊤x + b, with w ∈ Rd and b ∈ R. Given a text, represented as a row of the dataset x, it is classiﬁed into one of two classes {+1, −1}, depending on whether h(x) > 0 or h(x) < 0. Example 1.5. (Deep Neural Networks) Neural networks are functions of the form fℓ ◦ fℓ−1 ◦ · · · ◦ f1, where each fk is the componen-wise composition of an activation function σ with a linear map Rdk−1 → Rdk , x 7→ W kx + bk. An activation function could be the sigmoid σ(x) = 1/(1 + e−x), which takes values between (0, 1), and which can be interpreted as “selecting” certain coordinates of a vector depending on whether they are positive or negative (see Figure 1.4). The coefﬁcients wk ij of the matrix W k in each layer are the weights, while the entries of bk are called the bias terms. The weights and bias terms are to be adapted in order to ﬁt observed input-output pairs. A neural network is usually represented as a graph, see Figure 1.5. Neural networks have been extremely successful in pattern recognition, and are widely used in applications ranging from natural language processing to machine translation and medical diagnostics. One of the earliest theoretical results in approximation theory is a theorem by Weierstrass that shows that we can approximate any continuous function on an interval to arbitrary precision by polynomials. 2 Figure 1.4: The sigmoid function Figure 1.5: A neural network. Each layer correspond to applying a linear map to the outputs of the previous layer, followed by an activation function. Each arrow represents a weight. For example, the transition from the ﬁrst layer to the second is a map R3 → R4, and the weight associated with the arrow from the second node in layer 1 to the ﬁrst node in layer 2 is the (1, 2)-entry in the matrix deﬁning the corresponding linear map. Theorem 1.6 (Weierstrass). Let f be a continuous function on [a, b]. Then for any ϵ > 0 there exists a polynomial p(x) such that ∥f − p∥∞ = max x∈[a,b] |f (x) − p(x)| ≤ ϵ. This theorem is remarkable because it shows that we can approximate any continuous function on a compact interval using only a ﬁnite number of parameters, the coefﬁcients of a polynomial. The problem with this theorem is that it gives no bound on the size of the polynomial, which can be rather large. It also does not give a procedure of actually computing such an approximation, let alone ﬁnding one efﬁciently. We will see that neural networks have the same approximation properties, i.e., for every continuous function on an interval can be approximated to arbitrary accuracy by a neural network. There are many variations on such results for approximating a class of functions through a simpler class, and we will be interested in cases where such approximations can be efﬁciently computed. One way of ﬁnding good approximating functions is by using optimization methods. Optimization The notion of best ﬁt is formalized by using a loss function. A loss function L : Y × Y → R+ measures the mismatch between a prediction on a given input x ∈ X and an element y ∈ Y. The empirical risk of a function h : X → Y is the average loss L(h(xi), yi) over the training data, ˆR(h) := 1 n n∑ i=1 L(h(xi), yi) One would then aim to ﬁnd a function h among a set of candidate function H that minimizes the loss when applying the function to the training data: minimize h∈H ˆR(h). (1.1) Problem (1.1) is an optimization problem. Minimizing over a set of functions my look abstract, but functions in H are typically parametrized by few parameters. For example, when the class H consists of linear functions of the form β0 +β1x1 +· · ·+βpxp, as in Example 1.3, then the optimization problem (1.1) amounts to minimizing a function over Rp+1. In the case of neural networks, Example 1.5, one optimizes over the weights wk ij and bias terms bk i . The form of the loss function depends on the problem at hand and is usually derived from statistical considerations. Two common candidates are the square error for regression problems, which applied to a function h : Rd → R takes the form L(h(x), y) = (h(x) − y)2, and the indicator loss function, which takes the general form L(h(x), y) = 1{h(x) ̸= y} = { 1 if h(x) = y 0 if h(x) ̸= y . As this function is not continuous, in practice one often encounters continuous approximations. A binary classiﬁer is often implemented by a function h : X → [0, 1] that provides a probability of an input belonging to a class. If h(x) > 1/2, then x is assigned to class 1, while if h(x) ≤ 1/2, then x is assigned to class 0. A common loss function for this setting is the log-loss function, or cross-entropy, L(h(x), y) = −y log(h(x)) − (1 − y) log(1 − h(x)) = { − log(h(x)) if y = 1 − log(1 − h(x)) if y = 0 . (1.2) The function is designed to take on large values if the class predicted by h(x) does not match y, and can be interpreted in the context of maximum-likelihood estimation. Finding or approximating a minimizer of a function falls into the realm of numerical optimization. While for linear regression we can solve the relevant optimization problem (least-squares minimization) in closed form, for more involved problems such as neural networks we use optimization algorithms such as gradient descent: we start with an initial guess and try to minimize our function by taking steps in direction of steepest descent, that is, along the negative gradient of the function. In the case of composite functions such as neural networks, computing the gradient requires the chain rule, which leads to the famous backpropagation algorithm for training a neural network that will be discussed in detail. There are many challenges related to optimization models and algorithms. The function to be minimized may have many local minima or saddle points, and algorithms that look for minimizers may ﬁnd any one of these, instead of a global minimizer. The functions to be minimized may not be differentiable, and methods from the ﬁeld of non-smooth optimization come into play. The biggest challenge for optimization algorithms in the context of machine learning, however, lies in the particular form of the objective function: it is given as a sum of many terms, one for each data point. Evaluating such a function and computing its gradient can be time and memory consuming. An old class of algorithms that includes stochastic gradient descent circumvents this issue by not computing the gradient of the whole function at each step, but only of a small random subset of the terms. These algorithms work surprisingly well, considering that they do not make use of all the information available at every step. Statistics Suppose we have a binary classiﬁcation task at hand, with Y = {0, 1}. We could learn the following function from our data: h(x) = { yi if x = xi, 1 otherwise. This is called learning by memorization, since the function simply memorizes the value yi for every seen example xi. The empirical risk with respect to the unit loss function for this problem is ˆR(h) = 1 n n∑ i=1 1{h(xi) ̸= yi} = 0. Nevertheless, this is not a good classiﬁer: it will not perform very well outside of the training set. This is an example of overﬁtting: when the function is adapted too closely to the seen data, it does not generalize to unseen data. The problem of generalization is the problem of ﬁnding a classiﬁer that works well on unseen data. To make the notion of generalization more precise, we assume that the training data points (xi, yi) are realizations of a pair of random variables (X, Y ), sampled from an (unknown) probability distribution on the product X × Y. The variables X and Y are in general not independent (otherwise there would be nothing to learn), but are related by a relationship of the form Y = f (X) + ϵ, where ϵ is a random perturbation with expected value E[ϵ] = 0. One could interpret the presence of the random noise ϵ as indicative of uncertainty or missing information. For example, when trying to predict a certain disease based on genetic markers, the genetic data might simply not carry enough information to always make a correct prediction. The function f is called the regression function. It is the conditional expectation of Y given a value of X, f (x) = E[Y | X = x]. Given a classiﬁer h ∈ H and a loss function L, the generalization risk is the expected value R(h) := E[L(h(X), Y )]. If L(h(x), y) = 1{h(x) ̸= y} is the unit loss, then this is simply P{h(X) ̸= Y }, i.e., the probability of misclassifying a randomly chosen input-output pair. The training data can be modelled as sampling form n pairs of random variables (X1, Y1), . . . , (Xn, Yn) that are identically distributed and independent copies of (X, Y ). Given a classiﬁer h, the empirical risk becomes a random variable ˆR(h) = 1 n n∑ i=1 L(h(Xi), Yi). The expected value of this random variable is E[ ˆR(h)] = 1 n n∑ i=1 E[L(h(Xi), Yi)] = E[L(h(X), Y )] = R(h), where we used the linearity of expectation and the fact that the (Xi, Yi) are independent and identically distributed (i.i.d). The empirical risk ˆR(h) is thus an unbiased estimator of the generalization risk R(h). Example 1.7. The loss function is often chosen so that the problem of empirical risk minimization becomes a maximum likelihood estimation problem. Consider the example where Y takes values in {0, 1} with probability P{Y = 1 | X = x} = f (x). Conditioned on X = x, Y is a Bernoulli random variable with parameter p = f (x), and the log-loss function (1.2) is precisely the negative log-likelihood function for the problem of estimating the parameter p. When looking for a good hypothesis h, all we have at our disposal is the empirical risk function constructed from the training data. It turns out that the quality of an empirical risk minimizer ˆh from a hypothesis class H can be measured by the estimation error, which compares the generalization risk of ˆh to the smallest possible generalization risk in H, and the approximation error, which measures how small the generalization risk can become within H. There is usually a trade-off between these to errors: if the class of functions H is large, then it is likely to contain functions with small generalization risk and thus have small approximation error, but the empirical risk minimizer ˆh is likely to “overﬁt” the data and not generalize well. On the other hand, if H is small (in the extreme case, consisting of only one function), then the empirical risk minimizer is likely to be close to the best possible hypothesis in H, but the approximation error will be large. Figure 1.6 shows an example in which data from a function with noise is approximated by polynomials of different degrees. 0.0 0.2 0.4 0.6 0.8 1.0 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.0 0.2 0.4 0.6 0.8 1.0 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.0 0.2 0.4 0.6 0.8 1.0 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Fitted model True function Figure 1.6: The data consists of 15 samples from the graph of a cosine function with added noise. The three displays show an approximation with a linear function, with a polynomial of degree 5, and with a polynomial of degree 15. The linear function has a large error on both the training set and in relation to the true function. The polynomial of degree 15, on the other hand, has zero error on the training data (a polynomial of degree d can ﬁt d + 1 points with distinct x-values exactly), but it will likely perform poorly on new data. This is an example of overﬁtting: more parameters in the model will not necessarily lead to a better performance. The ﬁeld of Statistical Learning Theory aims to understand the relation between the generalization risk, the empirical risk, the capacity of a hypothesis class H, and the number of samples n. In particular, notions such as the capacity of a hypothesis class are given a precise meaning through concepts such as VC dimension, Rademacher complexity, and covering numbers. Notes The ideas from approximation theory, optimization and statistics that underlie modern machine learning are old. Linear regression was known to Legendre and Gauss. The Weierstrass Approximation Theorem was published by Weierstrass in [31], see [28, Chapter 6] for an account and more history on approximation theory. Neural networks go back to the seminal work by McCulloch and Pitts from 1943 [16], followed by Rosenblatt’s Perceptron [22]. The term “Machine Learning” was ﬁrst coined by Arthur Samuel in 1959 [24]; at the time, “Cybernetics” was still widely used. Gradient descent was known to Cauchy, and the most important algorithm for deep learning today, Stochastic Gradient Descent, was introduced by Robbins and Monro in 1951 [21]. The ﬁeld of Statistical Learning Theory arose in the 1960s through seminal work by Vapnik and Chervonenkis, see [29] for an overview. For an account of mathematical learning theory, see [5]. Research in machine learning exploded in the 1990s, with striking new results and applications appearing at breathtaking pace. Still, apart from some of the more theoretical developments in learning theory and high-dimensional probability, these breakthroughs rarely relied on mathematics that was not available 50 years ago. So what has changed since the early days of cybernetics? The main reason for the sudden surge in popularity is the availability of vast amounts of data, and equally important, the computational resources to process the data. New applications have in turn led to new mathematical problems, and to new connections between various ﬁelds. 2 Overview of Probability In this lecture we review relevant notions from probability theory, with an emphasis on conditional expectation. Probability spaces A probability space is a triple (Ω, F, P), consisting of a set Ω, a σ-algebra F of subsets of Ω, called events, and a probability measure P. That F is a σ-algebra means that it contains ∅ and Ω and that it is closed under countable unions and complements. The probability measure P is a non-negative function P : F → [0, 1] such that P(∅) = 0, P(Ω) = 1, and P ( ⋃ i Ai ) = ∑ i P(Ai) for a countable collection {Ai} with Ai ∩ Aj = ∅ for i ̸= j. We interpret P(A ∪ B) as the probability of A or B happening, and P(A ∩ B) as the probability of A and B happening. Note that (A ∪ B)c = Ac ∩ Bc, where Ac is the complement of A in Ω. If the Ai are not necessarily disjoint, then we have the important union bound P ( ⋃ i Ai ) ≤ ∑ i P(Ai). This bound is sometimes also referred to as the zero-th moment method. We say that an event A holds almost surely if P(A) = 1 (note that this does not mean that the complement of A in Ω is empty). Random variables A random variable is a measurable map X : Ω → X , where X is typically R, Rd, N, or a ﬁnite set {0, 1, . . . , k}. For a measurable set A ⊂ X , we write P(X ∈ A) := P({ω ∈ Ω : X(ω) ∈ A}). We will usually use upper-case letters X, Y, Z for random variables, lower-case letters x, y, z for the values that these can take, and x, y, z if these are vectors in some Rd. 11 Example 2.1. A random variable speciﬁes which events “we can see”. For example, let Ω = {1, 2, 3, 4, 5, 6} and deﬁne X : Ω → {0, 1} by setting X(ω) = 1{ω ∈ {4, 6}}, where 1 denotes the indicator function. Then P(X = 1) = 1 3 , P(X = 0) = 2 3 . If all the information we get about Ω is from X, then we can only determine whether the result of rolling a die gives an even number greater than 3 or not, but not the individual result. The map A 7→ P(X ∈ A) for subsets of X is called the distribution of the random variable. The distribution completely describes the random variable, and there will often be no need to refer to the domain Ω. If F : X → Y is another measure map, then F (X) is again a random variable. In particular, if X is a subset of Rd, we can add and multiply random variables to obtain new random variables, and if X and Y are two distinct random variables, then (X, Y ) is a random variable in the product space. In the latter case we also write P(X ∈ A, Y ∈ B) instead of P((X, Y ) ∈ A × B). Note that this also has an interpretation in terms of intersections of events: it is the probability that both X ∈ A and Y ∈ B. A discrete random variable takes countable many values, for example in a ﬁnite set {1, . . . , k} or in N. In such a case it makes sense to talk about the probability of individual outcomes, such as P(X = k) for some k ∈ X . An absolutely continuous random variable takes values in R or Rd for d > 1, and is deﬁned as having a density ρ(x) = ρX (x), such that P(X ∈ A) = ∫ A ρ(x) dx. In the case where X = R, we consider the cumulative distribution function (cdf) P(X ≤ t) for t ∈ R. The complement, P(X > t) (or P(X ≥ t)), is referred to as the tail. Many applications are concerned with ﬁnding good bounds on the tail of a probability, as the the tail often models the probability of rare events. If X is absolutely continuous, then the probability of X taking a particular single value vanishes, P(X = a) = 0. For a random variable Z = (X, Y ) taking values in X × Y, we can consider the joint density ρZ(x, y), but also the individual densities of X and Y , for which we have ρX (x) = ∫ Y ρZ(x, y) dy. The ensuing distributions for X and Y are called the marginal distributions. Example 2.2. Three of the most common distributions are: • Bernoulli distribution Ber(p), taking values in {0, 1} and deﬁned by P(X = 1) = p, P(X = 0) = 1 − p for some p ∈ [0, 1]. We can replace the range X = {0, 1} by any other two-element set, for example {−1, 1}, but then the relation to other distributions may not hold any more. • Binomial distribution Bin(n, p), taking values in {0, . . . , n} and deﬁned by P(X = k) = ( n k ) pk(1 − p) n−k (2.1) for k ∈ {0, 1, . . . , n} and some p ∈ [0, 1]. We can also write a binomial random variable as a sum of Bernoulli random variables, X = X1 + · · · + Xn, since X = k if and only if k of the summands have the value 1. • Normal distribution N (µ, σ2), also referred to as Gaussian, with mean µ and variance σ2, deﬁned on R and with density γ(x) = 1 √2πσ e− (x−µ)2 2σ2 . This is the most important distribution in probability and statistics, as most other distributions can be approximated by it. Expectation The expectation (or mean, or expected value) of a discrete random variable is deﬁned as E[X] = ∑ k∈X k · P(X = k). For an absolutely continuous random variable with density ρ(x), it is deﬁned as E[X] = ∫ X ρ(x) dx. Note that the expectation does not always need to exist since the sum or integral need not converge. When we require it to exist, we often write this as E[X] < ∞. Example 2.3. The expectation of a Bernoulli random variable with parameter p is E[X] = p. The expectation of a Binomial random variable with parameters n and p is E[X] = np. For example, if one were to ﬂip a biased coin that lands on heads with probability p, then this would correspond to the number of heads one would “expect” after n coin ﬂips. The expectation of the normal distribution N (µ, σ2) is µ. This is the location on which the “bell curve” is centred. One of the most useful properties is linearity of expectation. If X1, . . . , Xn are random variables taking values in a subset of Rd and a1, . . . , an ∈ R, then E[a1X1 + · · · + anXn] = a1 E[X1] + · · · + an E[Xn]. Example 2.4. The expected value of a Bernoulli random variable with parameter p is E[X] = 1 · P(X = 1) + 0 · P(X = 0) = p. The linearity of expectation then immediately gives the expectation of the Binomial distribution with parameters n and p. Since such a random variable can be written as X = X1 + · · · + Xn, with Xi Bernoulli, we get E[X] = E[X1 + · · · + Xn] = E[X1] + · · · + E[Xn] = np. This would be (slightly) harder to deduce from the direct deﬁnition (2.1), when one would have to use the binomial theorem. If F : X → Y is a measurable function, then the expectation of the random variable F (X) can be expressed as E[F (X)] = ∫ X F (x)ρ(x) dx (2.2) in the case of an absolutely continuous random variable, and similarly in the discrete case. 1 An important special case is the indicator function F (X) = 1{X ∈ A} = { 1 X ∈ A 0 X ̸∈ A. Then E[1{X ∈ A}] = P(X ∈ A), (2.3) as can be seen by applying (2.2) to the indicator function. The identity (2.3) is useful, as it allows to properties of the expectation, such as linearity, in the study of probabilities of events. The expectation also has the following monotonicity property: if 0 ≤ X ≤ Y , where X, Y are real-valued random variables, then E[X] ≤ E[Y ]. Another important identity for random variables is the following. Assume X is absolutely continuous, takes values in R, and X ≥ 0. Then E[X] = ∫ ∞ 0 P(X > t) dt. Using this identity, one can deduce bounds on the expectation from bounds on the tail of a probability distribution. The variance of a random variable is the expectation of the square deviation from the mean: Var(X) = E[(X − E[X]) 2]. The variance measures the “spread” of a distribution: random variables with a small variance are more likely to stay close to their expectation. Example 2.5. The variance of the normal distribution is σ2. The variance of the Bernoulli distribution is p(1 − p) (verify this!), while the variance of the Binomial distribution is np(1 − p). The variance scales as Var(aX + b) = a2Var(X). In particular, it is translation invariant. The variance is in general not additive (but it is, if the random variables are independent). Independence A set of random variables {Xi} taking values in the same range X is called independent if for any subset {Xi1, . . . , Xik } and any subsets Aj ⊂ X , 1 ≤ j ≤ k, we have P(Xi1 ∈ A1, . . . , Xik ∈ Ak) = P(Xi1 ∈ A1) · · · P(Xik ∈ Ak). In words, the probability of any of the events happening simultaneously is the product of the probabilities of the individual events. A set of random variables {Xi} is said to be pairwise independent if every subset of two variables is independent. Note that pairwise independence does not imply independence. 1We will not always list the formulas for both the discrete and continuous, when the form of one of these cases can be easily guessed from the form of the other case. In any case, the sum in the discrete setting is also just an integral with respect to the discrete measure. Example 2.6. Assume you toss a fair coin two times. Let X be the indicator variable for heads on the ﬁrst toss, Y the indicator variable for heads on the second toss, and Z the random variable that is 1 if X = Y and 0 if X ̸= Y . Taken individually, each of these random variables is a Bernoulli random variable with p = 1/2. They are also pairwise independent, as is easily veriﬁed, but not independent, since P(X = 1, Y = 1, Z = 1) = 1 4 ̸= 1 8 = P(X = 1)P(Y = 1)P(Z = 1). Intuitively, the information that X = 1 and Y = 1 already implies Z = 1, so adding this constraint does not alter the probability on the left-hand side. We say that a set of random variables {Xi} is i.i.d. if they are independent and identically distrib- uted. This means that each Xi can be seen as a copy of X1 that is independent of it, and in particular all the Xi have the same expectation and variance. One of the most important results in probability (and, arguably, in nature) is the (strong) law of large numbers. Given random variables {Xi}, deﬁne the sequence of averages as X n = 1 n (X1 + · · · + Xn). Since each random variable is, by deﬁnition, a function on a sample space Ω, we can consider the pointwise limit lim n→∞ X n, which is the random variable that for each ω ∈ Ω takes the limit limn→∞ X n(ω) as value.2 Theorem 2.7 (Law of Large Numbers). Let {Xi} be a sequence of i.i.d. random variables with E[X1] = µ < ∞. Then the sequence of averages X n converges almost surely to µ: P( lim n→∞ X n = µ) = 1. Example 2.8. Let each Xi be a Bernoulli random variable with parameter p. One could think this as ﬂipping a coin that will show heads with probability p. Then X n is the average number of heads when ﬂipping the coin n times. The law of large numbers asserts that as n increases, this average approaches p almost surely. Intuitively, when ﬂipping the coin a billion times, the number of heads we get divided by a billion will be indistinguishable from p: if we do not know p we can estimate it in this way. Some useful inequalities In applications it is often not possible to get precise expressions for a probability we are interested in, most often because we don’t know the exact distribution we are dealing with and only have access to parameters such as the expectation or the variance. There are several useful inequalities that help us bound the tail or deviation probabilities. For the following, we assume X ⊂ R. • Jensen’s Inequality Let f : R → R be a convex function, that is, f (λx + (1 − λ)y) ≤ λf (x) + (1 − λ)f (y) for λ ∈ [0, 1]. Then f (E[X]) ≤ E[f (X)]. 2That this is indeed a random variable in the formal sense follows from measure theory, we will not be concerned with those details. • Markov’s Inequality (“ﬁrst moment method”) For X ≥ 0 and λ > 0, P(X ≥ λ) ≤ E[X] λ . • Chebyshev’s Inequality (“second moment method”) For λ > 0, P(|X − E[X]| ≥ λ) ≤ Var(X) λ2 . • Exponential Moment Inequality For any s, λ ≥ 0, P(X ≥ λ) ≤ e−sλ E[esX ]. Note that both the Chebyshev and the exponential moment inequality follow from the Markov inequality applied to certain transformations of X. Conditional probability and expectation Given events A, B ⊂ Ω with P(B) ̸= 0, the conditional probability of A conditioned on B is deﬁned as P(A | B) = P(A ∩ B) P(B) . One interprets this as the probability of A if we assume B. That is, if we observed B, then we replace the whole set Ω by B and consider B to be the new space of events, considering only the part of events A that lie in B. We can rearrange the expression for conditional probability to P(A ∩ B) = P(A | B)P(B), from which we get the sometimes useful identity P(A) = P(A | B)P(B) + P(A | Bc)P(Bc), (2.4) where Bc denotes the complement of B in Ω. Since by exchanging the role of A and B we get P(A ∩ B) = P(B | A)P(A), we arrive at the famous Bayes rule for conditional probability: P(A | B) = P(B | A)P(A) P(B) , deﬁned whenever both A and B have non-zero probability. These concepts clearly extend to random variables, where we can deﬁne, for example, P(X ∈ A | Y ∈ B) = P(X ∈ A, Y ∈ B) P(X ∈ B) . Note that if X and Y are independent, then the conditional probability is just the normal probability of X: knowing that Y ∈ B does not give us any additional information about X! If we ﬁx an event such as {Y ∈ B}, then we can deﬁne the conditioning of the random variable X to this event as the random variable X ′ with distribution P(X ′ ∈ A) = P(X ∈ A | Y ∈ B). In particular, P(X ∈ A | Y ∈ B) + P(X ̸∈ A | Y ∈ B) = 1. Example 2.9. Consider the case of testing for doping at a sports event. Let X be the indicator variable for the presence of a certain drug, and Y the indicator variable for whether the person tested has taken the drug. Assume that the test is 99% accurate when the drug is present and 99% accurate when the drug is not present. We would like to know the probability that a person who tested positive actually took the drug, namely P(Y = 1 | X = 1). Translated into probabilistic language, we know that P(X = 1 | Y = 1) = 0.99, P(X = 0 | Y = 1) = 0.01 P(X = 0 | Y = 0) = 0.99, P(X = 1 | Y = 0) = 0.01. Assuming that only 1% of the participants have taken the drug, which translates to P(Y = 1) = 0.01, we ﬁnd that the overall probability of a positive test result is, using (2.4), P(X = 1) = P(X = 1 | Y = 0)P(Y = 0) + P(X = 1 | Y = 1)P(Y = 1) = 0.01 · 0.99 + 0.99 · 0.01 = 0.0198. Hence, using Bayes’ rule, we conclude that P(Y = 1 | X = 1) = P(X = 1 | Y = 1)P(Y = 1) P(X = 1) = 0.99 · 0.01 0.0198 = 0.5. That is, we get the surprising result that even though our test is very unlikely to give false positives and false negatives, the probability that a person tested positive has actually taken the drug is only 50%. The reason is that the event itself is highly unlikely. We now come to the notion of conditional expectation. Let X, Y be random variables. If X is discrete, then the conditional expectation of X conditioned on an event Y = y is deﬁned as E[X | Y = y] = ∑ k kP(X = k | Y = y). (2.5) This is simply the expectation of the random variable X ′ with distribution P(X ′ ∈ A) = P(X ∈ A | Y = y). Intuitively, we assume that Y = y is given/has been observed, and consider the expectation of X under this additional knowledge. Example 2.10. Assume we are rolling dice, let X be the random variable giving the result, and let Y be the indicator variable for the event that the result is at most 4. Then E[X] = 3.5 and E[X | Y = 1] = 2.5 (verify this!). This is the expected value if we have the additional information that the result is at most 4. In the absolutely continuous case we can deﬁne a conditional density ρX|Y =y(x) = ρX,Y (x, y) ρY (y) , (2.6) where ρX,Y is the joint density of (X, Y ) and ρY the density of Y . The conditional expectation is then deﬁned E[X | Y = y] = ∫ X xρX|Y =y(x) dx. (2.7) We replace the density ρX of X with an updated density ρX|Y =y that takes into account that a value Y = y has been observed when computing the expectation of X. When looking at (2.5) and (2.7), we get a different number E[X | Y = y] for each y ∈ Y, where we assume Y to be the space where Y takes values. Hence, we can deﬁne a random variable E[X | Y ] on Y as follows: E[X | Y ](y) = E[X | Y = y]. If X = f (Y ) is completely determined by Y , then clearly E[X | Y ](y) = E[X | Y = y] = E[f (Y ) | Y = y] = E[f (y) | Y = y] = f (y), since the expected value of a constant is just that constant, and hence E[X | Y ] = f (Y ) as a random variable. Using the deﬁnition of the conditional density (2.6), Fubini’s Theorem and expression (2.7), we can write the expectation of X as E[X] = ∫ X xρX (x) dx = ∫ X x ∫ Y ρ(X,Y )(x, y) dy dx = ∫ Y (∫ X xρ(X,Y )(x, y) dx) dy = ∫ Y (∫ X xρ(X|Y =y)(x) dx) ρY (y) dy = ∫ Y E[X | Y = y]ρY (y) dy. One can interpret this as saying that we get the expected value of X by integrating the expected values conditioned on Y = y with respect to the density of Y . In the discrete case, the identity has the form E[X] = ∑ y E[X | Y = y]P(Y = y). The above identities can be written more compactly as E[E[X | Y ]] = E[X]. In the context of machine learning, we assume that we have a (hidden) map f : X → Y from an input space to an output space, and that, for a given input x ∈ X , the observed output is y = f (x) + ϵ, where ϵ is random noise with E[ϵ] = 0. If we consider the input as a random variable X, then the output is random variable Y = f (X) + ϵ, with E[ϵ | X] = 0 (“the expected value of ϵ, when knowing X, is zero”). We are interested in the value E[Y | X]. For this, we get E[Y | X] = E[f (X) | X] + E[ϵ | X] = f (X), since f (X) is completely determined by X. Concentration of measure A very important reﬁnement of Chebyshev’s inequality are concentration inequalities, which state that the probability of exceeding the expectation is exponentially small. A prototype of such an inequality is Hoeffding’s Bound. Theorem 2.11 (Hoeffding’s Inequality). Let X1, . . . , Xn be independent random variables taking values in [0, 1], and let Sn = 1 n ∑n i=1 Xi be the average. Then for t ≥ 0, P (|Sn − E[Sn]| ≥ t) ≤ 2e−2nt2. Note the implication of this: if we have a sequence of random variables {Xi} bounded in [0, 1] (for example, the result of repeated, identical experiments or observations) then as n increases, the probability that the average of the random variables deviates from its mean decreases exponentially with n. In particular, if the random variables all thave the same expectation µ, then (by linearity of expectation) we have E[X n] = µ, and the probability of straying from this value becomes very small very quickly! Notes Even though probability theory and statistics are central to machine learning, probabilistic concepts are often not used rigorously. For example, one frequently encounters expressions such as P(X | Y ) which, taken literally, do not make sense. Depending on context, such an expression may refer to either the conditional expectation E[X | Y ], the conditional probability P(X ∈ A | Y ∈ B), or the conditional density ρX|Y =y(x). It turns out that for most practical purposes it does not really matter, but it is just something that a mathematics student used to rigorous deﬁnitions should be aware of. A good general introduction to probability theory is §1.1 of [27]. Good references for concentration of measure and related topics are [3, 30]. Part I Statistical Learning Theory 21 3 Binary Classiﬁcation In this lecture we begin the study of statistical learning theory in the case of binary classiﬁcation. We will characterize the best possible classiﬁer in the binary case, and relate notions of classiﬁcation error to each other. Binary Classiﬁcation A binary classiﬁer is a function h : X → Y = {0, 1}, where X is a space of features. The fact that we use {0, 1} it not very important, and in many cases we will also consider classiﬁers taking values in {−1, 1} where convenient. Binary classiﬁers arise in a variety of applications. In medical diagnostics, for example, a classiﬁer could take an image of a skin mole and determine if it is benign or if it is melanoma. Typically, can arise from a function X → [0, 1] that assigns to every input x a probability p. If p > 1/2, then x is assigned to class 1, and if p ≤ 1/2 it is assigned to class 0. In the context of binary classiﬁcation we usually use the unit loss function L(h(x), y) = 1{h(x) ̸= y} = { 1 h(x) ̸= y 0 h(x) = y. The unit loss function does not distinguish between false positives and false negatives. A false positive is a pair (x, y) with h(x) = 1 but y = 0, and a false negative is a pair for which h(x) = 0 but y = 1. We would like to learn a classiﬁer from a set of observations {(xi, yi)}n i=1 ⊂ X × Y. (3.1) The classiﬁer should not only match the data, but generalize in order to be able to classify unseen data. For this, we assume that the points in (3.1) are drawn from a probability distribution on X × Y, and replace each data point (xi, yi) in (3.1) with a copy (Xi, Yi) of a random variable (X, Y ) on X × Y. We are after a classiﬁer h such that the expected value of the empirical risk ˆR(h) = 1 n n∑ i=1 1{h(Xi) ̸= Yi} (3.2) 23 is small. We can write this expectation as E[ ˆR(h)] (1) = 1 n n∑ i=1 E[1{h(Xi) ̸= Yi}] (2) = 1 n n∑ i=1 P(h(Xi) ̸= Yi) (3) = P(h(X) ̸= Y ) =: R(h), where (1) uses the linearity of expectation, (2) expresses the expectation of an indicator function as probability, and (3) uses the fact that all the (Xi, Yi) are identically distributed. The function R(h) is the risk: it is the probability that the classiﬁer gets something wrong. Example 3.1. Assume that the distribution is such that Y is completely determined by X, that is, Y = f (X). Then R(h) = P(h(X) ̸= f (X)), and R(h) = 0 if h = f almost everywhere. If X is a ﬁnite or compact set with the uniform distribution, then R(h) simply measures the proportion of the input space on which h fails to classify inputs correctly. While for certain tasks such as image classiﬁcation there may be a unique label to each input, in general this need not be the case. In many applications, the input does not carry enough information to completely determine the output. Consider, for example, the case where X consists of whole genome sequences and the task is to predict hypertension (or any other condition) from it. The genome clearly does not carry enough information to make an accurate prediction, as other factors also play a role. To account for this lack of information, deﬁne the regression function f (X) = E[Y |X] = 1 · P(Y = 1|X) + 0 · P(Y = 0|X) = P(Y = 1|X). Note that if we write Y = f (X) + ϵ, then E[ϵ|X] = 0, because f (X) = E[Y |X] = E[f (X)|X] | {z } =f (X) + E[ϵ|X]. The Bayes classiﬁer While in Example 3.1 we could choose (at least in principle) h(x) = f (x) and get R(h) = 0, in the presence of noise this is not possible. However, we could deﬁne a classiﬁer h∗ by setting h∗(x) = { 1 f (x) > 1 2 0 f (x) ≤ 1 2 , We call this the Bayes classiﬁer. The following result shows that this is the best possible classiﬁer. Theorem 3.2. The Bayes classiﬁer h∗ satisﬁes R(h ∗) = inf h R(h), where the inﬁmum is over all measurable h. Moreover, R(h∗) ≤ 1/2. Proof. Let h be any classiﬁer. To compute the risk R(h), we ﬁrst condition on X and then average over X: R(h) = E[1{h(X) ̸= Y }] = E [E[1{h(X) ̸= Y }|X]] . (3.3) For the inner expectation, we have E[1{h(X) ̸= Y }|X] = E[1{h(X) = 1, Y = 0} + 1{h(X) = 0, Y = 1}|X] = E[(1 − Y )1{h(X) = 1}|X] + E[Y 1{h(X) = 0}|X] (1) = 1{h(X) = 1} E[(1 − Y )|X] + 1{h(X) = 0} E[Y |X] = 1{h(X) = 1}(1 − f (X)) + 1{h(X) = 0}f (X). To see why (1) holds, recall that the random variable E[Y 1{h(X) = 0}|X] takes values E[Y 1{h(x) = 0}|X = x], and will therefore only be non-zero if h(x) = 0. We can therefore pull the indicator function out of the expectation. Hence, using (3.3), R(h) = E[1{h(X) = 1}(1 − f (X)) | {z } (1) + 1{h(X) = 0}f (X)] | {z } (2) . (3.4) For (1), we decompose 1{h(X) = 1}(1 − f (X)) =1{h(X) = 1, f (X) > 1/2}(1 − f (X)) + 1{h(X) = 1, f (X) ≤ 1/2}(1 − f (X)) ≥1{h(X) = 1, f (X) > 1/2}(1 − f (X)) + 1{h(X) = 1, f (X) ≤ 1/2}f (X), (3.5) where the inequality follows since (1 − f (X)) ≥ f (X) if f (X) ≤ 1/2. By the same reasoning, for (2) we get 1{h(X) = 0}f (X) ≥1{h(X) = 0, f (X) > 1/2}(1 − f (X)) + 1{h(X) = 0, f (X) ≤ 1/2}f (X). (3.6) Combining the inequalities (3.5) and (3.6) within the bound (3.4) and collecting the terms that are multiplied with f (X) and those that are multiplied with (1 − f (X)), we arrive at R(h) ≥ E[1{f (X) ≥ 1/2}f (X) + 1{f (X) > 1/2}(1 − f (X))] = E[1{h∗(X) = 0}f (X) + 1{h ∗(X) = 1}(1 − f (X))] = R(h ∗), where the last equality follows from (3.4) applied to h = h∗. The characterization (3.4) also shows that R(h ∗) = E[1{f (X) > 1/2}(1 − f (X))] + 1{f (X) ≤ 1/2}f (X)] = E [min{f (X), 1 − f (X)}] ≤ 1 2 , which completes the proof. We have seen in Example 3.1 that the Bayes risk is 0 if Y is completely determined by X. At the other extreme, if the response Y does not depend on X at all, then the Bayes risk is 1/2. This means that for every input, the best possible classiﬁer consists of “guessing” without any prior information, which means that we have a 50% chance of being correct! The error E(h) = R(h) − R(h ∗) is called the excess risk or error of h with respect to the best possible classiﬁer. Approximation and Estimation We conclude this lecture by relating notions of risk. In what follows, we assume that a class of classiﬁers H is given, from which we are allowed to choose. We denote by ˆhn the classiﬁer obtained by minimizing the empirical risk ˆR(h) over H, that is ˆR(ˆhn) = inf h∈H n∑ i=1 1{h(Xi) ̸= Yi}, where the (Xi, Yi) are i.i.d. copies of a random variable (X, Y ) on X × Y. Note that ˆhn is what we will typically obtain by computation from samples (xi, yi). The way it is deﬁned, it depends on n, the class of functions H, and the random variables (Xi, Yi), and as such is itself a random variable. We want ˆhn to generalize well, that is, we want R(ˆhn) = P(ˆhn(X) ̸= Y ) to be small. We know that the smallest possible value this risk can attain is given by R(h∗), where h∗ is the Bayes classiﬁer. We can decompose the difference between the risk of ˆhn and that of the Bayes classiﬁer as follows: R(ˆhn) − R(h∗) = R(ˆhn) − inf h∈H R(h) | {z } Estimation error + inf h∈H R(h) − R(h ∗) | {z } Approximation error The ﬁrst compares the performance of ˆhn against the best possible classiﬁer within the class H, while the second is a statement about the power of the class H. We can reduce the estimation error by making the class H smaller, but then the approximation error increases. Ideally, we would like to ﬁnd bounds on the estimation error that converge to 0 as the number of samples n increases. Notes 4 Finite Hypothesis Sets Given a ﬁxed hypothesis set H, we would like to study the classiﬁer ˆh computed from n random samples (Xi, Yi) by minimizing the empirical risk over the class H, ˆR(ˆh) = inf h∈H ˆR(h), where ˆR(h) = 1 n n∑ i=1 1{h(Xi) ̸= Yi}. Hence, for any h, ˆR(h) is a random variable that depends on the number of samples n and on the underlying probability distribution. The classiﬁer ˆh is also a random variable, and depends on n, the class H, and the underlying distribution. This is the object that we can compute from observed data. If h∗ denotes the Bayes classiﬁer, then we would like to bound the excess risk E(ˆh) = R(ˆh) − R(h ∗), (A) where we recall that R(h) = P(h(X) ̸= Y ). As opposed to ˆR, R is not a random variable: it depends solely on the probability distribution. Moreover, for any ﬁxed h, E[ ˆR(h)] = R(h). Denote by h a best possible classiﬁer in H, that is, the one that generalizes best: R(h) = inf h∈H R(h). The parameter h depends only on the class H and the probability distribution. A less ambitious goal is to bound the difference R(ˆh) − R(h). (B) We emphasize here that ˆR(h) and R(ˆh) are both random variables, and ˆR(ˆh) is a random variable in two ways. Bounds on (A) and (B) are therefore probabilistic. More precisely, for any given tolerance δ ∈ (0, 1), we want to ﬁnd constants C(n, δ) and C′(n, δ) such that R(ˆh) − R(h ∗) ≤ C(n, δ) and R(ˆh) − R(h) ≤ C′(n, δ) holds with probability 1 − δ. Ideally, the constants should also depend on properties of the set H, for example the size of H if this set is ﬁnite. In addition, we would like the constants to decrease to 0 as n → ∞. In this lecture we will derive bounds on (B) in the case where H is a ﬁnite set. 27 Risk bounds for ﬁnite sets of classiﬁers In this section we prove the following bound. Theorem 4.1. Let H = {h1, . . . , hK} be a ﬁnite dictionary. Then for δ ∈ (0, 1), P  R(ˆh) − inf h∈H R(h) ≤ √ 2 log ( 2K δ ) n   ≥ 1 − δ. This important result shows that (with high probability) we can bound the estimation error by a term that is logarithmic in the size of H, and proportional to n−1/2, where n is the number of samples. For ﬁxed or moderately growing K, this error goes to zero as n goes to inﬁnity. If we denote by h the minimizer of R(h) over H, then we can write the estimation error as R(ˆh) − R(h) = ≤0 z }| { ˆR(ˆh) − ˆR(h) +R(ˆh) − ˆR(ˆh) + ˆR(h) − R(h) ≤ 2 sup h∈H |R(h) − ˆR(h)|. (4.1) As a ﬁrst step towards bounding the supremum, we need to bound the difference |R(h) − ˆR(h)| of an individual, ﬁxed h. The key ingredient for such a bound is a concentration of measure inequality known as Hoeffding’s bound. Theorem 4.2 (Hoeffding’s Inequality). Let Z1, . . . , Zn be independent random variables taking values in [0, 1], and let Zn = (1/n) ∑n i=1 Zi be the average. Then for t ≥ 0, P (|Zn − E[Zn]| > t ) ≤ 2e−2nt2. Using Hoeffding’s Inequality we obtain the following bound on the difference between the empirical risk and the risk of a classiﬁer. Lemma 4.3. For any classiﬁer h and δ ∈ (0, 1), | ˆR(h) − R(h)| ≤ √ log(2/δ) 2n holds with probability at least 1 − δ. Proof. Set Zi = 1{h(Xi) ̸= Yi}. Then Zn = 1 n n∑ i=1 Zi = ˆR(h), E[Zn] = E[ ˆR(h)] = R(h), and the Zi satisfy the conditions of Hoeffding’s inequality. Set δ = 2e−2nt2 and resolve for t, which gives t = √ log(2/δ) 2n . Hence, by Hoeffding’s inequality, P(| ˆR(h) − R(h)| > t) = P(|Zn − E[Zn]| > t) ≤ δ and therefore, by taking the complement, P(| ˆR(h) − R(h)| ≤ t) = 1 − P(| ˆR(h) − R(h)| > t) ≥ 1 − δ, which was claimed. Proof of Theorem 4.1. The goal is to bound the supremum sup h∈H | ˆR(h) − R(h)|. (4.2) For this, we use the union bound. Indeed, for each hi we can apply Lemma 4.3 with δ/K to show that P ( | ˆR(hi) − R(hi)| > t 2 ) ≤ δ K , where t = √ 2 log(2K/δ) n . The probability of (4.2) being bounded by t can be expressed equivalently as P(sup h∈H | ˆR(h) − R(h)| ≤ t/2) = P(| ˆR(h1) − R(h1)| ≤ t/2, . . . , | ˆR(hK) − R(hK)| ≤ t/2). Since the right-hand side is an intersection of events, the complement of this event is the union of the events | ˆR(hi) − R(hi)| > t, and we can apply the union bound: P (sup h∈H | ˆR(h) − R(h)| > t/2) ≤ K∑ i=1 P(| ˆR(hi) − R(hi)| > t/2) ≤ K · δ K = δ. Therefore, with probability at least 1 − δ we have 2 sup h∈H | ˆR(h) − R(h)| ≤ √ 2 log(2K/δ) n , and using (4.1) the claim follows. One drawback of the bound in Theorem 4.1 is that it does not take into account properties of the underlying distribution. In particular, it is the same in the case where Y is completely determined by X as it is in the case in which Y is completely independent on X. Intuitively, in the ﬁrst situation we would hope to get better rates of convergence than in the second. We will see that using concentration inequalities such as the Bernstein inequality, that take into account the variance of the random variables, we can get better rates of convergence in situation in which the “variance” is not too big. Notes 5 Probably Approximately Correct As before, we consider a ﬁxed dictionary H and select one classiﬁer ˆh that optimizes the empirical risk ˆR(h). Recall: • The empirical risk ˆR and the classiﬁer ˆh depend on the data (Xi, Yi), 1 ≤ i ≤ n, and are random variables. In particular, they depend on n; • The risk R(h) = E[ ˆR(h)] depends on the underlying distribution on X × Y, but not on n. We have seen that if H = {h1, . . . , hK} is ﬁnite, then with probability 1 − δ, we have R(ˆh) − inf h∈H R(h) ≤ √ 2 log(K) + 2 log(2/δ) n . (5.1) Note that log(K) is proportional to the bit size of K: this is the amount of bits needed to represent numbers up to K, and can be seen as a measure of complexity for the set H (the “space” necessary to represent K elements). Bounds such as (5.1) are called generalization bounds. Probably Approximately Correct Learning An alternative point of view to generalization bounds would be to ask, for given accuracy ϵ > 0 and conﬁdence δ ∈ (0, 1), how many samples n are needed to get an accuracy of ϵ with conﬁdence δ: P(R(ˆh) − inf h∈H R(h) ≤ ϵ) ≥ 1 − δ. Assuming h∗ ∈ H and Y = f (X), we have R(h∗) = 0, and h∗ would be the correct classiﬁer. The classiﬁer ˆh is then probably (with probability 1−δ) approximately (up to an misclassiﬁcation probability of at most ϵ) correct. This leads us to the notion of Probably Approximately Correct (PAC) learning. In what follows, we denote by size(H) the complexity of representing an element of H. This is not a precise deﬁnition, but depends on the case at hand. For example, if H = {h1, . . . , hK} is a ﬁnite set, then we can index this set using K numbers. On a computer, numbers up to K can be represented as binary numbers using ⌈log2(K)⌉ bits, and hence (up to a constant factor) size(H) = log(K) would be adequate here. Similarly, we denote by size(X ) the complexity of representing an element of the input space. For example, if X ⊂ Rd, then we would use d as size parameter (possibly multiplied by a constant factor to account for the size of representing a real number in ﬂoating point arithmetic on a computer). Note that size(X ) or size(H) is not the same as the cardinality of these sets! 31 Deﬁnition 5.1. (PAC Learning 1) A hypothesis class H is called PAC-learnable if there exists a classiﬁer ˆh ∈ H depending on n random samples (Xi, Yi), i ∈ {1, . . . , n}, and a polynomial function p(x, y, z, w), such that for any ϵ > 0 and δ ∈ (0, 1), for all distributions on X × Y, P ( R(ˆh) ≤ inf h∈H R(h) + ϵ ) ≥ 1 − δ holds whenever n ≥ p(1/ϵ, 1/δ, size(X ), size(H)). We also say that H is efﬁciently PAC-learnable, if the algorithm that produces ˆh from the data runs in time polynomial in 1/ϵ, 1/δ, size(X ) and size(H). Remark 5.2. In our context, to say that an algorithm “runs in time p(n)” means that the number of steps, with respect to some suitable model of computation, is bounded by p(n). Note that in this deﬁnition we disregard speciﬁc constants in the lower bound on n, but only require that it is polynomial. In computer science, polynomial time or space is considered efﬁcient, while problems that require exponential time and/or space to solve are considered inefﬁcient. For example, sorting n numbers can be performed in O(n log(n)) operations and is efﬁcient, while it is not known if ﬁnding the shortest route through n cities (the Traveling Salesman Problem) can be solved in a number of computational steps that is polynomial in n. This is the subject of the famous P vs NP conjecture. In the case of a ﬁnite hypothesis space H with K elements, we have seen that H is PAC-learnable, since n ≥ ( 2 ϵ2 ) ( log(K) + log ( 2 δ )) , which is polynomial in all the relevant parameters. Generalization bounds and noise We conclude by commenting brieﬂy on an improvement of the generalization bound (5.1) when incor- porating assumptions on the distribution. While the bound (5.1) incorporates the number of samples and the size of H, it does not take into account properties of the distribution, for example, the uncertainty ϵ = Y − f (X), where f (X) = E[Y |X] is the regression function. Let γ ∈ (0, 1/2] and assume that |f (X) − 1/2| ≥ γ almost surely. This condition is known as Massart’s noise condition. If γ = 1/2, then f (X) is either 1 or 0 and we are in the deterministic case, where Y is completely determined by X. If, on the other hand, γ ≈ 0, then we are barely placing any restrictions on f (X), and we are allowing for the case where f (X) is close to 0, and hence where Y is almost independent of X. Theorem 5.3. Let H = {h1, . . . , hK} be a ﬁnite dictionary and assume that h∗ ∈ H, where h∗ is the Bayes classiﬁer. Then for δ ∈ (0, 1), P ( R(ˆh) − R(h ∗) ≤ log ( K δ ) γn ) ≥ 1 − δ. 1In some references, such as the book “Foundations of Machine Learning” by Mohri, Rostamizadeh and Talwalkar, this version of PAC learning is called Agnostic PAC Learning. In the PAC learning context, we see that n ≥ 1 γϵ (log(K) + log(1/δ)) samples are necessary to approximate the Bayes classiﬁer up to a factor of ϵ with conﬁdence 1 − δ. We also see here that the number of samples needed increases as γ → 0, reﬂecting the fact that in the presence of high uncertainty, more observations are needed than if we have low uncertainty. The proof of this result relies on a concentration of measure result similar to Hoeffding’s inequality, called Bernstein’s inequality. Theorem 5.4 (Bernstein Inequality). Let {Zi}n i=1 be centred random variables (that is, E[Zi] = 0) with |Zi| ≤ c and set σ2 = 1 n ∑n i=1 Var(Zi). Then for t > 0, P ( 1 n n∑ i=1 Zi > t ) ≤ e− nt2 2(σ2+ct/3) . We outline the idea of the proof. The proof proceeds by deﬁning the random variables Zi(h) = 1{h∗(Xi) ̸= Yi} − 1{h(Xi) ̸= Yi} for each h ∈ H. The average and expectation of these random variables is then ˆR(h ∗) − ˆR(h) = 1 n n∑ i=1 Zi(h), R(h ∗) − R(h) = E[Zi(h)]. Based on this, one gets a bound R(ˆh) − R(h ∗) ≤ 1 n n∑ i=1(Zi(ˆh) − E[Zi(ˆh)] | {z } Zi(ˆh) ) ≤ max h∈H 1 n n∑ i=1 Zi(h). (5.2) The random variables Zi(h) are centred, satisfy |Zi(h)| ≤ 2 =: c and we can bound the variance by Var(Zi(h)) = Var(Zi(h)) ≤ P(h(Xi) ̸= h ∗(Xi)) =: σ2(h). We can now apply Bernstein’s inequality to the probability that the sum (5.2) exceeds a certain value for each individual h, and use a union bound to get a corresponding bound for the maximum that involves the variance σ2(ˆh). Using the property that h∗ ∈ H, one can also derive a lower bound on the excess risk in terms of the variance, and hence combine both bounds to get the desired result. Notes 6 Learning Shapes So far we have considered learning with ﬁnite dictionaries of classiﬁers H. We now illustrate an example where H is not ﬁnite, and show how PAC-learnability and generalization bounds can be derived in this setting. We then move on to the more general framework of Rademacher complexity. Learning Rectangles Assume that our data describes a rectangle: the input space X is a subset of R2, and the function f : X → {0, 1} is the indicator function of a closed rectangle B, so that for any point x, f (x) = 1 if x is in the rectangle, and 0 if not. Suppose that all we have access to is a random sample of labelled points, (xi, yi), i ∈ {1, . . . , n} (see Figure 6.1, left panel). 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5Figure 6.1: The blue points are in the true rectangle, while the red points are outside of it. The smaller blue rectangle represents the computed classiﬁer ˆh, while the shaded are corresponds to the true rectangle that generated the original labelling. We compute a candidate ˆh : R2 → {0, 1} as the indicator function of the smallest enclosing rectangle of the point with label 1 (i.e., the blue points in Figure 6.1). It is clear that if we have lots of sampled points, then we should get a good approximation to the “true” rectangle that generated the data, while with few points this is not possible. How can we quantify this? Let ϵ > 0 and δ ∈ (0, 1) be given, and let X be a random point with associated label Y = f (X). Then P(f (X) = 1) is the probability measure of the true rectangle, while R(ˆh) = P(ˆh(X) ̸= f (X)) 35 is the risk of ˆh. We would like to ﬁnd out the number of samples that would ensure P(R(ˆh) ≤ ϵ) ≥ 1 − δ. First, note that since the rectangle deﬁned by ˆh is always contained in the true rectangle that we would like to discover, we can only get false negatives from ˆh (that is, if ˆh(x) = 1 then x is in the true rectangle, but there may be points x in the true rectangle for which ˆh(x) = 0). Hence, R(ˆh) = P(ˆh(X) ̸= f (X)) = P(ˆh(X) = 1, f (X) = 0) | {z } =0 +P(ˆh(X) = 0, f (X) = 1) = P(ˆh(X) = 0, f (X) = 1). If we denote by ˆB = {x ∈ R2 : ˆh(x) = 1} the computed smallest enclosing rectangle, then the risk R(ˆh) can be described more geometrically as R(ˆh) = P(X ∈ B\\ ˆB), namely the probability of an input being in the true rectangle but not in the computed one. Now let ϵ > 0 and δ ∈ (0, 1) be given. If P(X ∈ B) ≤ ϵ, then clearly also R(ˆh) ≤ ϵ. Assume therefore that P(X ∈ B) > ϵ. Denote by Ri, i ∈ {1, 2, 3, 4}, the smallest sub-rectangles or B with P(X ∈ Ri) ≥ ϵ/4 that bound each of the four sides of B, respectively (see Figure 6.2). We could, for example, start with the whole rectangle and move one of its sides towards the opposite side for as long as the measure is not less than ϵ/4. 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Figure 6.2: Four boundary regions with probability mass ϵ/4 each. Denote by R◦ i the rectangles with their inward-facing sides removed. Then clearly the probability measure of the union of these sets is P(X ∈ ⋃i R◦ i ) ≤ ϵ, since the measure of each of the R◦ i is at most ϵ/4. If the computed rectangle ˆB intersects all the Ri, then P(X ∈ B\\ ˆB) = P ( X ∈ ⋃ i R◦ i \\ ˆB ) ≤ ϵ. We now need to show that the probability that ˆB does not intersect all the rectangles is small: P(∃i : ˆB ∩ Ri = ∅) = P ( ⋃ i { ˆB ∩ Ri = ∅} ) ≤ 4∑ i=1 P( ˆB ∩ Ri = ∅), where we used the union bound. The probability that ˆB does not intersect one of the rectangles Ri, each of which has probability mass ϵ/4, is equal to the probability that the n randomly sampled points that gave rise to ˆB do not fall in Ri. For each of these points, the probability of not falling into Ri is at most 1 − ϵ/4, so the probability that none of the points falls into Ri is (1 − ϵ/4)n. Hence, P(∃i : ˆB ∩ Ri = ∅) ≤ 4(1 − ϵ/4) n ≤ 4e−nϵ/4, where we used the inequality 1 − x ≤ e−x. Setting the righ-hand side to δ, we conclude that if n ≥ 4 ϵ log ( 4 δ ) then R(ˆh) ≤ ϵ holds with probability at least 1 − δ. 0 200 400 600 800 1000 0.00 0.05 0.10 0.15 0.20 0.25 Average approximation error 0 200 400 600 800 1000 0.0 0.5 1.0 1.5 2.0 2.5 Computed approximation error Theoretical bound Figure 6.3: The left graphs shows the average risk as n increases. The right right graph shows the risk bound ϵ when given a conﬁdence 1 − δ = 0.99 (blue curve), and the theoretical generalization bound as derived in the example. Remark 6.1. Note that we did not make any assumptions on the probability distribution when deriving the bound on n in the rectangle-learning example. If the distribution is absolutely continuous with respect to the Lebesgue measure on R2, then we could have required the probability measures of the rectangles to be exactly ϵ/4, but the way the proof is written it also applies to distributions that are not supported on all of R2, such as the uniform distribution on a compact subset of R2 that may or may not cover the area of B, or a discrete distribution supported on countably many points. The requirement P(X ∈ B) > ϵ still ensures that enough probability mass is contained within the conﬁnes of B for the argument to work. We may, however, end up looking at degenerate cases where, for example, all the probability mass is on an edge, one of the rectangles Ri is an edge or the whole of B, etc. Note that in such cases the intuitive view of the generalization risk as the “area” of the complement B\\ ˆB is no longer accurate! In practice we will only consider distributions that are natural to the situation under consideration. Notes 7 Rademacher Complexity The key to deriving generalization bounds for sets of classiﬁers H was to bound the maximum possible difference between the empirical risk and its expected value, sup h∈H | ˆR(h) − R(h)| = sup h∈H \f \f \f \f \f 1 n n∑ i=1 1{h(Xi) ̸= Yi} − E[1{h(Xi) ̸= Yi}] \f \f \f \f \f . (A) For ﬁnite H, the probability that this quantity exceeds a given t can be bounded using the union bound, but this approach does not work for inﬁnite sets. We therefore derive a different method that is based on intrinsic complexity measures of H. The ﬁrst such measure that we will encounter is the Rademacher complexity. Rademacher Complexity In the following, we write Zi = (Xi, Yi), i ∈ {1, . . . , n}, for the set of (random) pairs of samples and labels in Z = X × Y, with points in Z denoted by z = (x, y). To every h ∈ H we associate a function g : Z → {0, 1} by setting g(z) = 1{h(x) ̸= y}, and we denote the class of these functions by G.1 Using this notation, we get sup h∈H | ˆR(h) − R(h)| = sup g∈G 1 n \f \f \f \f \f n∑ i=1 g(Zi) − E[g(Zi)] \f \f \f \f \f . We will bound this expression in terms of a of a property of the set G, the Rademacher complexity. For what follows, we say that a random variable has the Rademacher distribution if it takes the values 1 or −1 with probability 1/2 each. When an expression potentially depends on different random quantities, for example random variables X and Y , we write EX to denote the expectation with respect to only X. Deﬁnition 7.1. (Rademacher Complexity) Let G be a class of real-valued functions on Z. Let σ = (σ1, . . . , σn) be a random vector, such that the σi are independent Rademacher random variables, and let z ∈ Z. The empirical Rademacher complexity of the family of functions G with respect to z is deﬁned as ˆRz(G) = Eσ [ sup g∈G 1 n n∑ i=1 σi · g(zi) ] . (7.1) 1We could, and will eventually, use any other loss function. 39 The Rademacher complexity is the expectation: Rn(G) = EZ[ ˆRZ(G)]. (7.2) Remark 7.2. Note that the expected value in (7.1) is only over the random signs, and that the point z is ﬁxed. It is only in (7.2) that we replace the point by a random vector Z = (Z1, . . . , Zn) and take the expectation. As the distribution of σ is discrete, we could also rewrite the empirical Rademacher complexity as average: ˆRz(G) = 1 2n ∑ σ∈{−1,1}n ( sup g∈G 1 n n∑ i=1 σi · g(zi) ) . Note also that the deﬁnition works for any set of real-valued functions g, not only those that arise from the loss function applied to a classiﬁer. In the literature there are various variations on the notion of Rademacher complexity. It can be deﬁned simply for sets: Given a set S ⊂ Rn, the Rademacher complexity of S is deﬁned as R(S) = Eσ [ sup x∈S 1 n n∑ i=1 σixi ] . (7.3) Our notion is a special case: when S = {(g(z1), . . . , g(zn)) : g ∈ G}, then we have Rz(G) = R(S). Using this notion of complexity, we can derive the following bound. Theorem 7.3. Let δ ∈ (0, 1) be given. Then with probability at least 1 − δ, sup g∈G ( E[g(Z)] − 1 n n∑ i=1 g(Zi) ) ≤ 2Rn(G) + √ log(1/δ) 2n . (7.4) Before going into the proof, we list some examples. Example 7.4. Assume H = {h} consists of only one function. Then for any point (z1, . . . , zn) ∈ Z n, the values g(zi) = 1{h(xi) ̸= yi} form a ﬁxed 0-1 vector . The empirical Rademacher complexity is 1 n Eσ [ n∑ i=1 σig(zi) ] = 0, since for each i, g(zi) and −g(zi) appear an equal number of times when averaging over all possible sign vectors. Example 7.5. Let H be the set of all binary classiﬁers. It follows that for any (z1, . . . , zn) ∈ Z n, the set of vectors (g(z1), . . . , g(zn)) for g ∈ G runs through all binary 0-1 vectors. The empirical Rademacher complexity of the set of functions G is thus the same as the Rademacher complexity of the hypercube S = [0, 1]n as a set. Note that for each sign vector σ we can always pick out a function g such that g(zi) = 1 if σi = 1 and g(zi) = 0 if σi = −1, and this function maximizes the sum ∑i σig(zi). From this observation it is not hard to conclude that ˆRz(G) = 1/2. Example 7.6. We will see that for a ﬁnite set H = {h1, . . . , hK} and z ∈ Z n, we get the bound ˆRz(G) ≤ r√ 2 log(K) n , where r = maxg∈G √∑n i=1 g(zi)2. This bound is known as Massart’s Lemma. Example 7.7. The Rademacher complexity of a set S equals the Rademacher complexity of the convex hull of S. For example, the Rademacher complexity of the hypercube [0, 1]n equals the Rademacher complexity of the set of its vertices. Since there are 2n vertices, Example 7.6 gives the bound √ 2 log(2) (we used that r = √n here). We saw in Example 7.5 that the exact value is 1/2. The proof of Theorem 7.3 depends on yet another concentration of measure inequality for averages of random variables, namely McDiarmid’s inequality. Theorem 7.8 (McDiarmid’s Inequality). Let {Zi} be a set of independent random variables deﬁned on a space Z, let {ci} ⊂ R be constants with ci > 0, and let f : Z → R be a function such that for all i ∈ {1, . . . , n} and zi ̸= z′ i, |f (z1, . . . , zi, . . . , zn) − f (z1, . . . , z′ i, . . . , zn)| ≤ ci. Then for all t > 0, the following inequality holds: P (f (Z1, . . . , Zn) > E[f (Z1, . . . , Zn)] + t) ≤ e − 2t2 ∑n i=1 c2 i P (f (Z1, . . . , Zn) < E[f (Z1, . . . , Zn)] − t) ≤ e − 2t2 ∑n i=1 c2 i (7.5) Using the union bound we can combine the two inequalities (7.5) to one inequality for the absolute value |f (Z1, . . . , Zn) − E[f (Z1, . . . , Zn)]|, with an additional factor of 2 in front of the exponential bound. Note that McDiarmid’s inequality contains Hoeffding’s inequality as a special case when f is the average. Proof of Theorem 7.3. Deﬁne the function Φ(z1, . . . , zn) = sup g∈G ( E[g(Zi)] − 1 n n∑ i=1 g(zi) ) . Then for i ∈ {1, . . . , n} and zi ̸= z′ i, and using the fact that the difference of suprema is not bigger than the supremum of a difference, we get Φ(z1, . . . , zi, . . . , zn) − Φ(z1, . . . , z′ i, . . . , zn) ≤ sup g∈G 1 n (g(z′ i) − g(zi)) ≤ 1 n , from which we conclude that |Φ(z1, . . . , zi, . . . , zn) − Φ(z1, . . . , z′ i, . . . , zn)| ≤ 1 n . The function Φ thus satisﬁes the conditions of McDiarmid’s inequality with ci = 1/n, and from this inequality we get P(Φ(Z1, . . . , Zn) − E[Φ(Z1, . . . , Zn)] > t) ≤ e−2nt2. Setting the right-hand bound to δ and resolving for t, we conclude that with probability at least 1 − δ, Φ(Z1, . . . , Zn) ≤ E[Φ(Z1, . . . , Zn)] + √ log(1/δ) 2n . The last, and crucial, step is to bound the expected value on the right-hand side with the Rademacher complexity. The idea is to introduce identical but independent copies Z′ i of the random variables Zi. Denote by Z and Z′ the vectors of random variables Zi and Z′ i, respectively. We will use repeatedly the fact that if f (Z′) only depends on the random variables in Z′, then f (Z′) = EZ[f (Z′)], that is we can pull an expression “into the expectation” if the terms involved are independent of the variables over which the expectation is taken. We will also use the linearity of expectation repeatedly without explicitly saying so. We can then set EZ[Φ(Z1, . . . , Zn)] = EZ [ sup g∈G ( E[g(Z)] − 1 n n∑ i=1 g(Zi) )] = EZ [ sup g∈G ( EZ′ [ 1 n n∑ i=1 g(Z′ i) ] − 1 n n∑ i=1 g(Zi) )] = EZ [ sup g∈G ( EZ′ [ 1 n n∑ i=1 g(Z′ i) − 1 n n∑ i=1 g(Zi) ])] ≤ EZ [ EZ′ [ sup g∈G 1 n n∑ i=1(g(Z′ i) − g(Zi)) ]] = EZ,Z′ [ sup g∈G 1 n n∑ i=1(g(Z′ i) − g(Zi)) ] , where for the inequality we used the fact that the sup of an expectation is not more than the expectation of the sup. We next use an idea known as symmetrization. The key observation is that each summand g(Z′ i) − g(Zi) is just as likely to be positive as it is to be negative. In other words, if we replace g(Z′ i) − g(Zi) with its negative, g(Zi) − g(Z′ i), then the above expectation does not change. More generally, we can pick any sign vector σ = (σ1, . . . , σn) with σi ∈ {−1, 1}, and will then have EZ,Z′ [ sup g∈G 1 n n∑ i=1 σi · (g(Z′ i) − g(Zi)) ] = EZ,Z′ [ sup g∈G 1 n n∑ i=1(g(Z′ i) − g(Zi)) ] . Now we use a “sheep counting trick”2, and sum these terms over all possible sign patterns, dividing by the total number of sign patterns, 2n: EZ,Z′ [ sup g∈G 1 n n∑ i=1(g(Z′ i) − g(Zi)) ] = 1 2n ∑ σ∈{−1,1}n EZ,Z′ [ sup g∈G 1 n n∑ i=1 σi(g(Z′ i) − g(Zi)) ] = Eσ,Z,Z′ [ sup g∈G 1 n n∑ i=1 σi(g(Z′ i) − g(Zi)) ] , where for the last equality we simply rewrote the average as an expectation over a vector of Rademacher random variables. We can now bound the supremum of the differences by the difference of suprema in order to get Eσ,Z,Z′ [ sup g∈G 1 n n∑ i=1 σi(g(Z′ i) − g(Zi)) ] ≤ Eσ,Z′ [ sup g∈G 1 n n∑ i=1 σig(Z′ i) ] + Eσ,Z [ sup g∈G 1 n n∑ i=1(−σig(Zi)) ] = 2Rn(G). 2When a shepherd wants to count her sheep, she counts the legs and divides the result by four. The last equality shows why we averaged over all sign vectors: by symmetry, averaging over −σ is the same as averaging over σ. The empirical Rademacher complexity ˆRz is a function of (z1, . . . , zn), and by the same argument as for the Φ function in the proof of Theorem 7.3 one can show that if z′ arises from z by changing zi to z′ i, then | ˆRz′(G) − ˆRz(G)| ≤ 1 n . We can therefore apply McDiarmid’s inequality to the random variable ˆRZ(G) to conclude that ˆRZ(G) ≤ Rn(G) + √ log(1/δ) 2n (7.6) with probability at least 1 − δ. One can combine (7.6) with (7.4) using the union bound to get a generalization bound analogous to (7.4) but in terms of the empirical Rademacher complexity (with slightly different parameters). To conclude, note that the inequalities (7.4) and (7.6) are one-sided inequalities: they bound a difference but not the absolute value of this difference. These can easily be adapted to give a bound on the supremum of the absolute difference | ˆR(h) − R(h)|. As a consequence of Example 7.5 we see that when considering the set of all binary classiﬁers, we do not get a generalization bound that converges to 0 as n → ∞ using the Rademacher complexity bound. Notes 8 VC Theory As usual we operate on a pair of input-output spaces Z = X × Y with Y = {−1, 1}. Let H be a set of classiﬁers with associated set G = {g : Z → {0, 1} : g(z) = 1{h(x) ̸= y}, h ∈ H}. We saw that we could bound the maximum difference between the generalization risk R(h) and the empirical risk ˆR(h) of a classiﬁer using the Rademacher complexity Rn(h): sup h∈H(R(h) − ˆR(h)) ≤ 2Rn(G) + √ log(1/δ) 2n . (8.1) Instead of considering the set G, we can also consider the Rademacher complexity of H itself. For what follows, assume that the classiﬁers in H take the values {−1, 1}. Lemma 8.1. The Rademacher complexities of H and G satisfy Rn(G) = 1 2 Rn(H). The proof depends on writing 1{h(x) ̸= y} = (1−yh(x))/2, and is left as an exercise. The deﬁnition of Rademacher complexity involved taking the expectation with respect to a distribution on the space Z that we do not know. We saw, however, that in some examples we could bound this expectation in a way that does not depend on the distribution. We next develop this idea in a more principled way, deriving generalization bounds in terms of parameters of the set of classiﬁers H that do not make reference to the underlying distribution. The theory is named after Vladimir Vapnik and Alexey Chervonenkis, who developed it in the 1960s. Vapnik-Chervonenkis Theory In binary classiﬁcation, a classiﬁer h can take at most two possible values on a ﬁxed input. Denote by x = (x1, . . . , xn) ∈ X n an n-tuple of inputs and set h(x) := (h(x1), . . . , h(xn)) ∈ {−1, 1}n for the corresponding vector of values of the classiﬁer. For any ﬁxed x ∈ X n, we could get up to 2n different values h(x) as h runs through H. Note that his is a ﬁnite bound even if the set H is inﬁnite! A possible classiﬁcation h(x) is called a dichotomy and one way to measure the expressiveness or richness of a set of classiﬁers H would be to count the possible dichotomies. 45 Example 8.2. Let X = R and let H be the set of indicator functions of closed half-lines: for each a ∈ R, h+ a (x) = { 1 x ≥ a −1 x < a , h − a (x) = { 1 x ≤ a −1 x > a . Given two distinct samples, {x1, x2}, there are 4 possible dichotomies: for each pattern ρ ∈ {−1, 1}2 we can ﬁnd h ∈ H such that the tuple (h(x1), h(x2)) = ρ. For three distinct points {x1, x2, x3} ⊂ R this is no longer possible. If we assume, for example, that x1 < x2 < x3, then any classiﬁer h with h(x1) = −1 and h(x2) = 1 will automatically also satisfy h(x3) = 1. Clearly, if H consists of only one element, then for every x ∈ X n there is only one possible dichotomy, while if H consists of all classiﬁers then there are 2n possible dichotomies if the entries of x are distinct. Somewhere in between these two extreme cases, the number of dichotomies may depend on x in more intricate ways. Deﬁnition 8.3. Let H be a set of classiﬁers h : X → Y. The growth function ΠH is deﬁned as ΠH(n) = max x∈X n |{h(x) : h ∈ H}| . Note that this function depends only on the set H. We can use it to bound the Rademacher complexity of a set of functions G. The bound depends on a result known as Massart’s Lemma, which is derived in the Exercises. Recall that the Rademacher complexity of a set S ⊂ Rn is deﬁned as R(S) = 1 n Eσ [ sup x∈S n∑ i=1 σixi ] , where σ = (σ1, . . . , σn) is a vector of i.i.d. Rademacher random variables (P(σi = +1) = P(σi = −1) = 1/2) and x = (x1, . . . , xn). Lemma 8.4. (Massart’s Lemma) If S = {x1, . . . , xK} ⊂ Rn consists of K elements, then R(S) ≤ r · √ 2 log(K) n , where r = maxx∈S ∥x∥ and ∥x∥ := √∑n i=1 x2 i . Theorem 8.5. The Rademacher complexity of H is bounded by Rn(H) ≤ √ 2 log(ΠH(n)) n . Proof. Consider a ﬁxed tuple x = (x1, . . . , xn) such that Πn(H) = |{h(x) : h ∈ H}| . The vectors (h(x1), . . . , h(xn)) all have norm √ n, and the claim follows from Massart’s Lemma. Combining this with (8.1) we immediately arrive at the following bound. Corollary 8.6. For all h ∈ H, R(h) ≤ ˆR(h) + √ 2 log(ΠH(n)) n + √ log(1/δ) 2n (8.2) Let H be a set of classiﬁers and S = {xi}n i=1 ⊂ X a set of inputs. Then S is shattered by H, if |{h(x) : h ∈ H}| = 2 n, that is, if all dichotomies are possible. Example 8.7. If H is the set of all binary classiﬁers and all the samples in S are distinct, then S is shattered by H. Example 8.8. If H = {h1, h2} consists of only two classiﬁers, one of which is constant 1 and the other is constant 0, then a subset S ⊂ X of samples is shattered by H if and only if |S| = 1, that is, we only look at one sample. There exists a subset S = {xi}n i=1 ⊂ X that can be shattered by H if and only if ΠH(n) = 2 n. If the number of samples n increases, it can become harder to ﬁnd a subset that can be shattered. While in the case where H consists of all binary classiﬁers we always have ΠH(n) = 2n, in the case where H consists of indicator functions of half-lines we saw that three distinct points on the line cannot be shattered. The maximum possible n such that a subset of n points can be shattered is the VC dimension. Deﬁnition 8.9. The Vapnik-Chernovenkis (VC) dimension of a set of classiﬁers H is VC(H) = max{n ∈ N : ΠH(n) = 2 n}. If VC(H) = d, then there exists a set of d samples that can be shattered by H: all possible dichotomies occur when applying classiﬁers in H to these n samples. Note that this does not mean, however, that all sets of n samples can be shattered by H. We will see that if H has VC dimension VC(H) = d, then we can bound log(ΠH(n)) ≤ d log ( ed n ) , which allows to rephrase the bound (8.2) in terms of the VC dimension. We will then study plenty of examples (including practical ones) where we can compute or bound the VC dimension. Notes 9 The VC Inequality The VC dimension of a hypothesis set H is the maximal cardinality of a subset of the input space X that is shattered by H: VC(H) = max{n ∈ N : ΠH(n) = 2 n}, where the growth function ΠH(n) counts the number of possible dichotomies, ΠH(n) = max x∈X n |{(h(x1), . . . , h(xn)) : h ∈ H}| . The VC dimension is a combinatorial quantity that depends only on H. It acts as a surrogate for cardinality when dealing with inﬁnite sets. VC dimension of families of sets The notion of VC dimension was deﬁned for classes of functions H. Equivalently, we can identify each h with the indicator function of a set A ⊂ X and consider the set of sets A = {A ⊂ X : h(x) = 1 ⇔ x ∈ A}. Given a subset S ⊂ X , we say that A shatters S, if every subset of S (including the empty set) can be obtained by intersecting S with elements of A: {A ∩ S : A ∈ A} = P(S) := {S′ : S′ ⊂ S}. In other words, we can use the collection A to select any subset of S. It should be clear that if S is shattered by A, then so is any subset of S. In this context, the growth function is deﬁned analogously, ΠA(n) = max{|{A ∩ S : A ∈ A}| : S ⊂ X , |S| = n}, as the maximal number of subsets of a set of cardinality n that can be selected using A. The VC dimension of the set system A is VC(A) = max{n ∈ N : ΠA(n) = 2 n}. Note that the VC dimension is monotone in the sense that it does not decrease when A is enlarged. One of the most important results in VC theory is a bound on ΠA(n) in terms of the VC dimension. This result is usually attributed to Sauer (who credits Perles) and Shelah, but was discovered independently by Vapnik & Chervonenkis. 49 Lemma 9.1. If VC(A) = d, then for n ≥ d, ΠA(n) ≤ d∑ i=0 ( n i ) ≤ ( en d )d . Proof. The proof of the ﬁrst inequality is by induction on n + d. If d = 0 and n = 0, then ΠA(0) = 1 (only the empty set can be considered) and the bound is valid. The statement is also easily veriﬁed for n = 1 and d ≤ 1. Assume now that n > 0 and d > 0, and that the statement holds for the pairs (d, n − 1) and (d − 1, n − 1). Let S ⊂ X be a subset with |S| = n, such that |{A ∩ S : A ∈ A}| = ΠA(n), and select an arbitrary element s ∈ S. Consider the sets A ′ = {A\\{s} : A ∈ A}, A ′′ = {A ∈ A : s ̸∈ A, A ∪ {s} ∈ A}. Note that VC(A′) ≤ VC(A) ≤ d, and VC(A′′) ≤ VC(A) − 1 = d − 1. The ﬁrst inequality follows from the fact that if A′ shatters a set T , then so does A. The second inequality follows from the fact that if a set T is shattered by A′′, then the set T ∪ {s} is shattered by A. Consider the map {A ∩ S : A ∈ A} → {A ∩ S : A ∈ A ′} = {A ∩ S\\{s} : A ∈ A}, A ∩ S 7→ A ∩ S\\{s}. This map is one-to-one, except in the case where A ∈ A′′. For such A, both s ̸∈ A and ˜A = A ∪ {s} ∈ A hold, and therefore A ∩ S ̸= ˜A ∩ S, but A ∩ S\\{s} = ˜A ∩ S\\{s}. It follows that |{A ∩ S : A ∈ A}| = |{A ∩ S\\{s} : A ∈ A ′}| + |{A ∩ S : A ∈ A ′′}| = |{A ∩ (S\\{s}) : A ∈ A ′}| + |{A ∩ (S\\{s}) : A ∈ A ′′}|. By the induction hypothesis, |{A ∩ (S\\{s}) : A ∈ A ′}| ≤ ΠA′(n − 1) ≤ d∑ i=0 (n − 1 i ), |{A ∩ (S\\{s}) : A ∈ A ′′}| ≤ ΠA′′(n − 1) ≤ d−1∑ i=0 ( n − 1 i ) , so that combining the terms we get |{A ∩ S : A ∈ A}| ≤ 1 + d∑ i=1 [( n − 1 i ) + ( n − 1 i − 1 )] = d∑ i=0 (n i ) . For the second claimed inequality, we extend the sum to n and multiply each summand by (n/d)d−i, to obtain d∑ i=0 ( n i ) ≤ n∑ i=0 ( n i ) ( n d )d−i = ( n d )d n∑ i=0 ( n i ) ( d n )i = ( n d )d ( 1 + d n )n ≤ ( en d )d , where we used the inequality 1 + x ≤ ex at the end. The VC Inequality A central result in statistical learning is an inequality that relates the VC dimension of a set of classiﬁers to the difference between the empirical and the generalization risk. Theorem 9.2. (VC Inequality) Let H be a set of classiﬁers h : X → {−1, 1} with VC dimension VC(H) = d, and let δ ∈ (0, 1). Then with probability at least 1 − δ, sup h∈H R(h) − ˆR(h) ≤ √ 2d log ( en d ) n + √ log(1/δ) 2n . Proof. In Corollary 8.6, Lecture 8, we saw that sup h∈H R(h) − ˆR(h) ≤ √ 2 log (ΠH(n)) n + √ log(1/δ) 2n . The claim now follows directly from Lemma 9.1. We remark that the bounds here were all stated as one-sided bounds: that is, they are stated as bounds on the difference R(h) − ˆR(h) and not the absolute value of the difference. We can get two-sided bounds by making adjustments in the derivation of bounds using the Rademacher complexity (using the second case of McDiarmid’s inequality) and arrive at the following bound, which holds with probability at least 1 − δ: sup h∈H \f \f \fR(h) − ˆR(h) \f \f \f ≤ √ 2d log ( en d ) n + √ log(2/δ) 2n Note that the only difference is the factor of 2/δ, which is a consequence of combining two one-sided inequalities to one two-sided inequality using the union bound. Rectangle learning revisited Let H be the set of functions that take the value 1 on rectangles in the plane X = R2 and −1 otherwise1. The question is: 1Depending on context, we will consider H as consisting of functions into {0, 1} or into {−1, 1}, this does not alter any of the results involving the VC dimension. Given n, can we ﬁnd a conﬁguration of n points in the plane such that for any labelling we can ﬁnd a rectangle containing those points labelled with 1? This is clearly possible when n = 2 (just choose two distinct points) and n = 3 (choose three points that form a triangle such as △). For n = 4 there are 16 possible labellings, and if we arrange the points in diamond form ⋄, then all labellings can be captured by rectangles (try this!). For n = 5 this is no longer possible: take the smallest enclosing rectangle of ﬁve points {xi}5 i=1. This rectangle will contain (at least) one of the xi on each boundary (if not, we could make it smaller). If each xi, i ∈ {1, . . . , 4}, lies on a different boundary, we can assign 1 to these points and −1 to x5. This dichotomy cannot be realized by a rectangle: any rectangle containing {xi}4 i=1 must also contain their smallest enclosing rectangle, hence also x5. +1 +1 +1 +1 -1 Figure 9.1: A dichotomy that is not captured by rectangles. Notes 10 General Loss Functions So far we looked at the problem of binary classiﬁcation. We considered a set H of classiﬁers h : X → Y, where Y was a set with two elements ({−1, 1} or {0, 1}, for example). Given a distribution on X × Y, we considered the generalization risk, R(h) = E[1{h(X) ̸= Y }] = P(h(X) ̸= Y ), (10.1) and the empirical risk, ˆR(h) = 1 n n∑ i=1 1{h(Xi) ̸= Yi}, (10.2) which is based on a sequence of random observations (Xi, Yi), i ∈ {1, . . . , n}. For each set of realizations {(xi, yi)} one can (in principle) construct a classiﬁer ˆh ∈ H that minimizes the empirical risk (10.2). We are ultimately interested in the generalization risk R(ˆh) and not the empirical risk ˆR(ˆh) (this would just tell us something about how well our classiﬁer works on the training data). Speciﬁcally, we are interested in how close the risk R(ˆh) is to the optimal generalization risk R(ˆh) = inf h∈H R(h). To analyse this, we split the difference into two parts: R(ˆh) − inf h∈H R(h) ≤ R(ˆh) − ˆR(ˆh) + ˆR(h) − inf h∈H R(h) ≤ 2 sup h∈H \f \f \f ˆR(h) − R(h)\f \f \f . The term within the sup to be bounded is just the difference between the average of i.i.d. random variables and their expectation. To bound this difference we used: • Hoeffding’s or Bernstein’s inequality and the union bound for ﬁnite H, to obtain a bound in terms of log(|H|); • McDiarmid’s inequality applied directly to the supremum and symmetrization to obtain a bound in terms of the Rademacher complexity. The Rademacher complexity could then be bounded, via Massart’s inequality, in terms of the growth function of H, which in turn could be bounded again, via the Sauer-Shelah Lemma, in terms of the VC dimension of H. It is natural to ask how much of this depends on the fact that we used binary classiﬁcation and the unit loss 1{h(X) ̸= Y }, and to what extent these bounds generalize to other types of classiﬁers and loss functions. 53 General Loss Functions Consider a set of function H = {h : X → Y} with Y = [−1, 1] (or any other subset of R), and a loss function L : Y × Y → R≥0. Besides the unit-loss that we studied so far, examples include: • L(x, y) = (x − y)2 (quadratic loss); • L(x, y) = |x − y| (ℓ1-loss); • L(x, y) = |x − y|p (ℓp-loss); • L(x, y) = log(1 + e−xy) (log-loss). Sometimes the loss function is scaled to ensure that it is in a certain range. For example, for the quadratic loss, (h(x) − y)2/2 ∈ [0, 1] if h(x) ∈ [−1, 1] and y ∈ [−1, 1]. The generalization risk is deﬁned as R(h) = E[L(h(X), Y )], while the empirical risk is ˆR(h) = 1 n n∑ i=1 L(h(Xi), Yi). Just as in the binary case with unit loss, we denote by h a classiﬁer with optimal generalization risk in H, and by ˆh a minimizer of ˆR(h). Also as in the binary case, we can bound R(ˆh) − R(h) ≤ 2 sup h∈H \f \f \f ˆR(h) − R(h)\f \f \f . Consider the set of functions L ◦ H := {g : Z → [0, 1] : g(z) = L(h(x), y)}. This plays the role of the set G deﬁned in Lecture 6. For z = (z1, . . . , zn), with zi = (xi, yi), we can deﬁne the empirical Rademacher complexity as ˆRz(L ◦ H) = Eσ [ sup g∈L◦H 1 n n∑ i=1 σig(zi) ] , and the Rademacher complexity as Rn(L ◦ H) = EZ [ ˆRZ(L ◦ H] . Assuming that L(x, y) ∈ [0, 1] (which we can by scaling the loss function accordingly) one can use the McDiarmid’s bounded difference inequality and the same symmetrisation argument as in the binary case to derive the bound sup h∈H \f \f \f ˆR(h) − R(h) \f \f \f ≤ 2Rn(L ◦ H) + √ log(2/δ) 2n , which holds with probability at least 1 − δ. For ﬁnite sets of classiﬁers H = {h1, . . . , hK} we get the same cardinality bounds as in the binary case. Theorem 10.1. Let H = {h1, . . . , hK} be a set of classiﬁers h : X → [−1, 1], and let L be a loss function taking values in [0, 1]. Then Rn(L ◦ H) ≤ √ 2 log(K) n . Proof. Fix z = (z1, . . . , zn). Then by Massart’s Lemma (Lemma 8.4 in Lecture 8), the empirical Rademacher complexity is bounded by ˆRz(L ◦ H) ≤ r · √ 2 log(K) n , where r = sup{∥x∥ : x ∈ S}, S = {g(z1), . . . , g(zn)) : g ∈ L ◦ H}. Since by assumption g(z) = L(h(x), y) ∈ [0, 1], r ≤ √n and the result follows by taking the expectation over Z. For inﬁnite H we cannot repeat the arguments used in the case of binary classiﬁers with unit loss. The bounds based on growth function and VC dimension depend on the fact that the number of possible dichotomies is ﬁnite and this is no longer the case when considering functions h with inﬁnite range. One way of dealing with such a situation is to approximate an inﬁnite set by a ﬁnite set in such a way, that every element of the inﬁnite set is close to a point in the ﬁnite subset. Notes 11 Covering Numbers In this lecture we consider hypothesis set H = {h : X → Y = [−1, 1]} and a loss function L : Y × Y → [0, 1]. Deﬁne the set of functions L ◦ H := {g : Z → [0, 1] : g(z) = L(h(x), y)}. This set plays the role of the set G deﬁned in Lecture 6. For z = (z1, . . . , zn), with zi = (xi, yi), we can deﬁne the empirical Rademacher complexity as ˆRz(L ◦ H) = Eσ [ sup g∈L◦H 1 n n∑ i=1 σig(zi) ] , where the expectation is over all sign vector σ = (σ1, . . . , σn) with independent σi that satisfy P{σi = +1} = P{σi = −1} = 1/2. The Rademacher complexity is deﬁned as as Rn(L ◦ H) = EZ [ ˆRZ(L ◦ H)] , where the expectation is over all n-tuples Z = (Z1, . . . , Zn), where Zi = (Xi, Yi). As in the case of binary classiﬁcation with unit loss, one can use McDiarmid’s bounded difference inequality and a symmetrisation argument to derive the bound sup h∈H \f \f \f ˆR(h) − R(h) \f \f \f ≤ 2Rn(L ◦ H) + √ log(2/δ) 2n , which holds with probability at least 1 − δ. For ﬁnite H = {h1, . . . , hK}, the arguments used in the case of binary classiﬁcation carry over seamlessly. Theorem 11.1. Let H = {h1, . . . , hK} be a set of functions h : X → [−1, 1], and let L be a loss function taking values in [0, 1]. Then Rn(L ◦ H) ≤ √ 2 log(K) n . Proof. Fix z = (z1, . . . , zn). Then by Massart’s Lemma (Lemma 8.4 in Lecture 8), the empirical Rademacher complexity is bounded by ˆRz(L ◦ H) ≤ r · √ 2 log(K) n , 57 where r = sup{∥x∥ : x ∈ S}, S = {(g(z1), . . . , g(zn)) : g ∈ L ◦ H}. Since by assumption g(z) = L(h(x), y) ∈ [0, 1], r ≤ √n and the result follows by taking the expectation over Z. For inﬁnite H, we cannot repeat the arguments that lead to the VC inequality in the binary classiﬁcation case, since these arguments were based on the fact that even for inﬁnite H, the number of possible dichotomies is ﬁnite. This limitation can be circumvented by approximating L ◦ H by a ﬁnite subset that is sufﬁciently dense. This leads to the concept of covering numbers. Covering Numbers Recall that a metric on a set S is a function d : S × S → R≥0 such that d(x, y) = 0 if and only if x = y, d(x, y) = d(y, x), and d(x, y) + d(y, z) ≤ d(x, z). A pseudo-metric is deﬁned like a metric, but replacing the ﬁrst condition with the looser requirement that d(x, x) = 0 (that is, d(x, y) = 0 may also be possible if x ̸= y). Deﬁnition 11.2. Given a pseudo-metric space (S, d), an ϵ-net is a subset T ⊂ S such that for every x ∈ S there exists y ∈ T with d(x, y) ≤ ϵ. The covering number corresponding to S and ϵ is the smallest cardinality of an ϵ-net: N (S, d, ϵ) = inf{|T | : T is an ϵ-net}. Example 11.3. Consider the Euclidean unit ball Bd = Bd 2 = {x ∈ Rd : ∥x∥2 ≤ 1}. We construct an ϵ-net T as follows. Start with an arbitrary point T = {x1} and add points to T as follows: if T = {x1, . . . , xk}, then choose a point xk+1 such that ∥xk+1 − xj∥ > ϵ for all j if possible, and add it to T , and if this is not possible, then stop. This process terminates since Bd 2 is bounded. The resulting set T = {x1, . . . , xN } is an ϵ-net by construction, and the distance between any two points in this set it larger than ϵ. Hence, the balls Bd(xj, ϵ/2) of radius ϵ/2 around the points in T are disjoint. Since the union of these balls is contained in the larger ball (1 + ϵ/2)Bd (the scaling of the unit ball by a factor of (1 + ϵ/2)), we get the volume inequality N vol(Bd(x1, ϵ/2)) = vol ( ⋃ i Bd(xi, ϵ/2) ) ≤ vol ((1 + ϵ/2)Bd) . (11.1) The volume of a ball of radius r is rd · vol(Bd), hence vol(Bd(x1, ϵ/2)) = (ϵ/2) d vol(Bd) and vol((1 + ϵ/2)Bd) = (1 + ϵ/2) d vol(Bd), and we get from (11.1) that N ≤ (1 + ϵ/2)d (ϵ/2)d = ( 2 ϵ + 1)d ≤ ( 3 ϵ )d if ϵ < 1. Of course, if ϵ ≥ 1 then we have an ϵ-net of size 1. We next consider the set L ◦ H with the empirical ℓ1 distance d z 1 (g1, g2) = 1 n n∑ i=1 |g1(zi) − g2(zi)|. We get the following bound for the empirical Rademacher complexity at z. Theorem 11.4. Let H be a hypothesis set of functions taking values in [−1, 1], and let L be a loss function taking values in [0, 1]. Then for any z ∈ Z n we have ˆRz(L ◦ H) ≤ inf ϵ>0 { ϵ + √ 2 log(N (L ◦ H, dz 1 , ϵ)) n } . The covering number takes over the role of VC dimension. Note that the covering number increases as ϵ decreases, and we get a trade-off between small ϵ and small covering number. Proof of Theorem 11.4. Fix z ∈ Z n and ϵ > 0, and let T be a smallest ϵ-net for (L ◦ H, dz 1 ). It follows that for any g ∈ L ◦ H we can ﬁnd a g′ ∈ T such that dz 1 (g, g′) ≤ ϵ. Hence, ˆRz(L ◦ H) = Eσ [ sup g∈L◦H 1 n n∑ i=1 σig(zi) ] ≤ Eσ [ sup g∈L◦H 1 n n∑ i=1 σig(zi) − σig′(zi) ] + Eσ [ 1 n n∑ i=1 σig′(zi) ] ≤ Eσ [ sup g∈L◦H 1 n n∑ i=1 |g(zi) − g′(zi)| ] + Eσ [ sup g∈L◦H 1 n n∑ i=1 σig′(zi) ] ≤ sup g∈L◦H d z 1 (g, g′) + Eσ [ max g′∈T 1 n n∑ i=1 σig′(zi) ] ≤ ϵ + √ 2 log(N (L ◦ H, dz 1 , ϵ)) n , where we used the bound for ﬁnite sets, Theorem 11.1. Since ϵ > 0 was arbitrary, this bounds holds for the inﬁmum among ϵ > 0. The bound derived is for the set L ◦ H. Under a boundedness condition on the loss function, we can bound the Rademacher complexity of this set in terms of that of H. Theorem 11.5. If L : Y × Y → [0, 1], then ˆRz(L ◦ H) ≤ 2 ˆRx(H), where x = (x1, . . . , xn) and z = (z1, . . . , zn) with zi = (xi, yi). In light of this result, we conclude this section with a bound on ˆRz(H) for a speciﬁc class of functions. In what follows we denote by Bd p ⊂ Rd the unit ball with respect to the p-norm. In particular, Bd 1 = {x ∈ Rd : ∥x∥1 = d∑ i=1 |xi| ≤ 1}, Bd ∞ = {x ∈ Rd : ∥x∥∞ = max i |xi| ≤ 1}. Notice that the set Bd ∞ is a hypercube. We now consider X = Bd 1 and the class of functions H = {h : X → Y : h(x) = ⟨a, x⟩, a ∈ Bd ∞}. By the Hölder inequality, the functions h satisfy |h(x)| = |⟨a, x⟩| ≤ ∥a∥∞∥x∥1 ≤ 1. Two functions h, g ∈ H are thus represented by two vectors a, b ∈ Rd, and for their distance we have d x 1 (h, g) = 1 n n∑ i=1 |h(xi) − g(xi)| = 1 n n∑ i=1 |⟨a − b, xi⟩|. From the Hölder inequality we get |⟨a − b, x⟩| ≤ ∥a − b∥∞∥x∥1, and since by assumption each xi ∈ Bd 1 , we have ∥xi∥1 ≤ 1 and d x 1 (h, g) ≤ ∥a − b∥∞. An ϵ-net therefore corresponds to a set T = {b1, . . . , bN } ⊂ Bd ∞ such that for every a ∈ Bd ∞ there exists a j such that ∥a − bj∥∞ ≤ ϵ. It follows from Exercise 4.1 that the covering number is bounded by N (H, d x 1 , ϵ) ≤ ( 3 ϵ )d . We thus get the bound ˆRx(H) ≤ inf ϵ>0 { ϵ + √ 2d log(3/ϵ) n } . If n is sufﬁciently large, then setting ϵ = 3 √ d log(n)/n < 1, we get the bound ˆRx(H) ≤ 4 √ d log(n) n . Note that the resulting bound does not depend on x. It is possible to remove the logarithmic factor log(n) using a more sophisticated technique called chaining. Notes 12 Model Selection Given an input space X , an output space Y, a class H of functions h : X → Y, a loss function L : Y ×Y → R≥0, and data S = {(xi, yi)}n i=1, we would like to solve the Empirical Risk Minimization (ERM) problem minimize ˆR(h) = 1 n n∑ i=1 L(h(xi), yi) subject to h ∈ H. (A) This is an example of a constrained optimization problem. The function to be minimized is the objective function and a solution ˆhS of (A) is called a minimizer. Several problems can arise when trying to solve this minimization problem. 1. The problem (A) may be hard to solve. This can be the case if the class H is large, the number of samples n is large, or when the objective function is not differentiable or not even continuous. 2. Do we even want to solve (A)? If the class H is large we may ﬁnd a minimizer ˆhS that ﬁts the data well but does not generalize. Since a certain generalization error is unavoidable, we can often replace (A) with a surrogate that is computationally easier to handle and provides a solution that is close enough to the one we are looking for. The choice of such an approximation is also informed by the choice of H. We therefore ﬁrst study the problem of ﬁnding a suitable class H, also known as model section. Model Selection Consider now the set of inputs again as a set of pairs of random variables S = {(Xi, Yi)}n i=1 ⊂ X × Y, so that ˆhS is a random variable. Ideally we would like the generalization risk R(ˆhS) to be close to the Bayes risk R∗, which is the best possible generalization risk. Recall the decomposition R(ˆhS) − R∗ = R(ˆhS) − inf h∈H R(h) | {z } Estimation error + inf h∈H R(h) − R∗ | {z } Approximation error (12.1) of the excess risk. Denote by h the minimizer of R(h) in H. In previous lectures we saw that R(ˆhS) − R(h) ≤ 2 sup h∈H \f \f \fR(h) − ˆR(h) \f \f \f , (12.2) 61 and therefore any bound that holds for the right-hand side with high probability (such as those based on VC dimension or covering numbers) also holds for the left-hand side. If H is large, then the bound may not be good enough, and in addition the minimizer ˆhS may be hard to compute. If, on the other hand, H is too small, then the approximation error can be large. One way to address this issue is to consider a nested family of sets Hk ⊂ Hk+1 of increasing complexity and to choose a k with optimal overall performance. We illustrate this using an example. Example 12.1. Let T ⊂ R2 be a disk and let (X, Y ) be a pair of random variables on R2 × {0, 1} such that fT (x) = E[Y |X = x] = { 1 x ∈ T 0 x ̸∈ T Consider the unit loss function, so that R(h) = P{h(X) ̸= Y } for some function h : R2 → {0, 1}. The function fT is the Bayes classiﬁer, as it satisﬁes R(fT ) = R∗ = 0. For any hypothesis set H we can combine (12.1) and the bound (12.2) to get R(ˆhS) ≤ 2 sup h∈H \f \f \fR(h) − ˆR(h)\f \f \f | {z } Bound on stimation error + inf h∈H R(h) | {z } Approximation error . Let Hk denote the set of indicator functions of regions bounded bys convex polygons with at most k sides (see Figure 25.1). To bound the estimation error, we use the fact that the VC dimension of the class of convex k-gons is 2k + 1 (see Problem Set 4), and get the bound sup h∈Hk | ˆR(h) − R(h)| ≤ √ (4k + 2) log(n) n + √ log(2/δ) 2n (12.3) with probability 1 − δ (we simpliﬁed the logarithmic term in the ﬁrst part of the bound). For the approximation error, we look at the well-known problem of approximating the circle with a regular polygon. The area enclosed by a regular k-gon inscribed in a circle of radius r is r2(k/2) sin(2π/k), so the area of the complement in the disk is πr2 − r2(k/2) sin(2π/k) = O(k−2), (12.4) where the equality follows from the Taylor expansion of the sine function. If the underlying probability distribution on R2 is the uniform distribution on a larger set or can be approximated as such, then (12.4) gives an upper bound for the approximation error (it can be less), and combined with (12.3) illustrates the estimation-approximation trade-off. The larger the number of sides of the polygons, the smaller the approximation error becomes, but the estimation error can become large due to overﬁtting. Thus even if the unknown shape we want to learn is a circle, if the number of samples is small we may be better off restricting to simpler models! This also has the additional advantage of saving computational cost. There are general strategies for optimizing for k. One theoretical method is known as structural risk minimization (SRM). In this model, the parameter k enters into the optimization problem to be solved. An alternative, practical approach is cross-validation. In this approach, the training set S is subdivided into a smaller training set and a validation set. In a nutshell, the ERM problem is solved for different parameters k on the training set, and the one that performs best on the validation set is chosen. 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 Figure 12.1: Learning a circle with polygons. The left panel shows the estimation error when trying to learn the shape using polygons with at most 4 and with at most 8 sides from the data. This is the error typically incurred by empirical risk minimization. The right panel illustrates the approximation error. This error measures how good we can approximate the ground truth with our function class. Part II Optimization 65 13 Optimization “[N]othing at all takes place in the universe in which some rule of maximum or minimum does not appear.” — Leonhard Euler Mathematical optimization, traditionally also known as mathematical programming, is the theory of optimal decision making. Other than in machine learning, optimization problems arise in a large variety of contexts, including scheduling and logistics problems, ﬁnance, optimal control and signal processing. The underlying mathematical problem always amounts to ﬁnding parameters that minimize (cost) or maximize (utility) an objective function in the presence or absence of a set of constraints. In the context of machine learning, the objective function is usually related to the empirical risk, but we ﬁrst take a step back and consider optimization problems in greater generality. What is an optimization problem? A general mathematical optimization problem is a problem of the form minimizef (x) subject to x ∈ Ω (13.1) where f : Rd → R is a real-valued objective function and Ω ⊆ Rd is a set deﬁning the constraints. If Ω = Rd, then the problem is an unconstrained optimization problem. Among all x ∈ Ω, we seek one with smallest f -value. Typically, the constraint set Ω will consist of such x ∈ Rd that satisfy certain equations and inequalities, f1(x) ≤ 0, . . . , fm(x) ≤ 0, g1(x) = 0, . . . , gp(x) = 0. A vector x∗ satisfying the constraints is called an optimum, a solution, or a minimizer of the problem (13.1), if f (x∗) ≤ f (x) for all other x that satisfy the constraints. Note that replacing f by −f , we could equivalently state the problem as a maximization problem. Optimality conditions In what follows we will study the unconstrained problem minimize f (x), (13.2) 67 where x ∈ Rd. Optimality conditions aim to identify properties that potential minimizers need to satisfy in relation to f (x). We will review the well known local optimality conditions for differentiable functions from calculus. We ﬁrst identify different types of minimizers. Deﬁnition 13.1. A point x∗ ∈ Rd is a • global minimizer of (13.2) if for all x ∈ Rd, f (x∗) ≤ f (x); • a local minimizer, if there is an open neighbourhood U of x∗ such that f (x∗) ≤ f (x) for all x ∈ U ; • a strict local minimizer, if there is an open neighbourhood U of x∗ such that f (x∗) < f (x) for all x ∈ U ; • an isolated minimizer if there is an open neighbourhood U of x∗ such that x∗ is the only local minimizer in U . Without any further assumptions on f , ﬁnding a minimizer is a hopeless task: we simply can not examine the function at all points in Rd. The situation becomes more tractable if we assume some smoothness conditions. Recall that Ck(U ) denotes the set of functions that are k times continuously differentiable on some set U . The following ﬁrst-order necessary condition for optimality is well known. We write ∇f (x) for the gradient of f at x, i.e., the vector ∇f (x) = ( ∂f ∂x1 (x), . . . , ∂f ∂xd (x) )⊤ Theorem 13.2. Let x∗ be a local minimizer of f and assume that f ∈ C1(U ) for a neighbourhood of U of x∗. Then ∇f (x∗) = 0. There are simple examples that show that this is not a sufﬁcient condition: maxima and saddle points will also have a vanishing gradient. If we have access to second-order information, in form of the second derivative, or Hessian, of f , then we can say more. Recall that the Hessian of f at x, ∇2f (x), is the d × d symmetric matrix given by the second derivatives, ∇2f (x) = ( ∂2f ∂xi∂xj ) 1≤i,j≤d . In the one-variable case we have learned that if x∗ is a local minimizer of f ∈ C2([a, b]), then f ′(x∗) = 0 and f ′′(x∗) ≥ 0. Moreover, the conditions f ′(x∗) = 0 and f ′′(x∗) > 0 guarantee that we have a local minimizer. These conditions generalise to higher dimension, but ﬁrst we need to know what f ′′(x) > 0 when we have more than one variable. Recall also that a matrix A is positive semideﬁnite, written A ≽ 0, if for every x ∈ Rd, x⊤Ax ≥ 0, and positive deﬁnite, written A ≻ 0, if x⊤Ax > 0. The property that the Hessian matrix is positive semideﬁnite is a multivariate generalization of the property that the second derivative is nonnegative. The known conditions for a minimizer involving the second derivative generalize accordingly. Theorem 13.3. Let f ∈ C2(U ) for some open set U and x∗ ∈ U . If x∗ is a local minimizer, then ∇f (x∗) = 0 and ∇2f (x∗) is positive semideﬁnite. Conversely, if ∇f (x∗) = 0 and ∇2f (x∗) is positive deﬁnite, then x∗ is a strict local minimizer. Unfortunately, the above criteria are not able to identify global minimizers, as differentiability is a local property. For convex functions, however, local optimality implies global optimality. Examples We present two examples of optimization problems that can be interpretred as machine learning problems, but have mainly been studied outside of the context of machine learning. The examples below come with associated Python code and it is not expected that you understand them in detail; they are merely intended to illustrate some of the problems that optimization deals with, and how they can be solved. Example 13.4. Suppose we want to understand the relationship of a quantity y (for example, sales data) to a series of predictors x1, . . . , xp (for example, advertising budget in different media). We can often assume the relationship to be approximately linear, with distribution Y = β0 + β1X1 + · · · + βpXp + ε, with noise ϵ that satisﬁes E[ϵ] = 0. As function class we take H = {h : h(x) = β0 + β1X1 + · · · + βpXp, β = (β0, . . . , βp) ∈ R p+1}. (13.3) The goal is to determine the model parameters β0, . . . , βp from data. To determine these, we can collect n ≥ p sample realizations (from observations or experiments), {(yi, xi1, . . . , xip), 1 ≤ i ≤ n}. and minimize the empirical risk with respect to the (normalized) ℓ2 loss function: ˆR(h) = 1 2n n∑ i=1(yi − (β0 + β1xi1 + · · · + βpxip)) 2. Collecting the data in matrices and vectors, y =    y1 ... yn  |  , X =    1 x11 · · · x1p ... ... . . . ... 1 xn1 · · · xnp  |  , β =      β0 β1 ... βp  | | |  , we can write the empirical risk concisely as ˆR(h) = 1 2n ∥y − Xβ∥2. Minimizing over h ∈ H means minimizing over vectors β ∈ Rp+1, and the best β is then the vector that solves the unconstrained optimization problem minimize 1 2n ∥Xβ − y∥2 2. This is an example of an optimization problem with variables β, no constraints (all β are valid candidates and the constraint set is Ω = Rp+1), and a quadratic objective function f (β) = 1 2n ∥Xβ − y∥2 2 = 1 2n (Xβ − y)⊤(Xβ − y) = 1 2n β⊤X ⊤Xβ − 2y⊤Xβ + y⊤y, (13.4) where X ⊤ is the matrix transpose. Quadratic functions of the form (13.4) are convex, so this is a convex optimization problem. If the columns of X are linearly independent (which, by the way, requires there to be more data than the number of parameters p), this simple optimization problem has a unique closed form solution, β∗ = (X ⊤X) −1X ⊤y. (13.5) In practice one would not compute β∗ by evaluating (13.5). There are more efﬁcient methods available, such as gradient descent, the conjugate gradient method, and several variations of these. It is important to note that even in this simple example, solving the optimization problem can be problematic if the number of samples is large. To illustrate the least squares setting using a concrete example, assume that we have data relating the basal metabolic rate (energy expenditure per time unit) in mammals to their mass.1 The model we use is Y = β0 + β1X, with Y the basal metabolic rate and X the mass. Using data for 573 mammals from the PanTHERIA database2, we can assemble the vector y and the matrix X ∈ Rn×(p+1) in order to compute the β = (β0, β1)⊤. Here, p = 1 and n = 573. We illustrate how to solve this problem in Python. As usual, we ﬁrst have to import some relevant libraries: numpy for numerical computation, pandas for loading and transforming datasets, cvxpy for convex optimization, and matplotlib for plotting. In [1]: # Import some important Python modules import numpy as np import pandas as pd from cvxpy import * import matplotlib.pyplot as plt We next have to load the data. The data is saved in a table with 573 rows and 2 columns, where the ﬁrst column list the mass and the second the basal metabolic rate. In [2]: # Load data into numpy array bmr = pd.read_csv(’../../data/bmr.csv’,header=None).as_matrix() # We can find out the dimension of the data bmr.shape Out [2]: (573, 2) To see the ﬁrst three and the last three rows of the dataset, we can use the \"print\" command. 1This example is from the episode “Size Matters” of the BBC series Wonders of Life. 2http://esapubs.org/archive/ecol/E090/184/#data In [3]: print(bmr[0:3,:]) [[ 13.108 10.604 ] [ 9.3918 8.2158] [ 10.366 9.3285]] To visualise the whole dataset, we can make a scatterplot by interpreting each row as a coordinate on the plane, and marking it with a dot. In [4]: # Display scatterplot of data (plot all the rows as points) bmr1 = plt.plot(bmr[:,0],bmr[:,1],’o’) plt.xlabel(\"Mass\") plt.ylabel(\"Basal metabolic rate\") plt.show() 0 2 4 6 8 10 12 14 Mass 0 2 4 6 8 10 12Basal metabolic rate The plot above suggests that the relation of the basal metabolic rate to the mass is linear, i.e., of the form Y = β0 + β1X, where X is the mass and Y the BMR. We can ﬁnd β0 and β1 by solving an optimization problem as described above. We ﬁrst have to assemble the matrix X and the vector y. In [5]: n = bmr.shape[0] p = 1 X = np.concatenate((np.ones((n,1)),bmr[:,0:p]),axis=1) y = bmr[:,-1] In [6]: # Create a (p+1) vector of variables Beta = Variable(p+1) # Create sum-of-squares objective function objective = Minimize(sum_entries(square(X*Beta - y))) # Create problem and solve it prob = Problem(objective) prob.solve() print(\"status: \", prob.status) print(\"optimal value: \", prob.value) print(\"optimal variables: \", Beta[0].value, Beta[1].value) status: optimal optimal value: 152.736200529558 optimal variables: 1.3620698558275837 0.7016170245505547 Now that we solved the problem and have the values β0 = 1.362 and β1 = 0.702, we can plot the line and see how it ﬁts the data. In [6]: plt.plot(bmr[:,0],bmr[:,1],’o’) xx = np.linspace(0,14,100) bmr = plt.plot(xx, Beta[0].value+Beta[1].value*xx, color=’red’,\\ linewidth=2) plt.show() 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 Even though for illustration purposes we used the CVXPY package, this particular problem can be solved directly using the least squares solver in numpy. In [7]: import numpy.linalg as la beta = la.lstsq(X,y) print(beta[0]) [ 1.36206997 0.70161692] Example 13.5. (Image inpainting) Even problems in image processing that do not appear to be machine learning problems can be cast as such. An image can be viewed as an m × n matrix U , with each entry uij corresponding to a light intensity (for greyscale images), or a colour vector, represented by a triple of red, green and blue intensities (usually with values between 0 and 255 each). For simplicity the following discussion assumes a greyscale image. For computational purposes, the matrix of an image is often viewed as an mn-dimensional vector u, with the columns of the matrix stacked on top of each other. In the image inpainting problem, one aims to learn the true value of missing or corrupted entries of an image. There are different approaches to this problem. A conceptually simple approach is to replace the image with the closest image among a set of images satisfying typical properties. But what are typical properties of a typical image? Some properties that come to mind are: • Images tend to have large homogeneous areas in which the colour doesn’t change much; • Images have approximately low rank, when interpreted as matrices. Total variation image analysis takes advantage of the ﬁrst property. The total variation or TV-norm is the sum of the norm of the horizontal and vertical differences, ∥U ∥TV = m∑ i=1 n∑ j=1 √ (ui+1,j − ui,j)2 + (ui,j+1 − ui,j)2, where we set entries with out-of-bounds indices to 0. The TV-norm naturally increases with increased variation or sharp edges in an image. Consider for example the two following matrices (imagine that they represent a 3 × 3 pixel block taken from an image). U1 =  0 17 3 7 32 0 2 9 27   , U2   1 1 3 1 0 0 0 0 2   The left matrix has TV-norm ∥U1∥TV = 200.637, while the right one has TV-norm ∥U2∥TV = 14.721 (verify this!) Intuitively, we would expect a natural image with artifacts added to it to have a higher TV norm. Now let U be an image with entries uij, and let Ω ⊂ [m] × [n] = {(i, j) | 1 ≤ i ≤ m, 1 ≤ j ≤ n} be the set of indices where the original image and the corrupted image coincide (all the other entries are missing). One could attempt to ﬁnd the image with the smallest TV-norm that coincides with the known pixels uij for (i, j) ∈ Ω. This is an optimization problem of the form minimize ∥X∥TV subject to xij = uij for (i, j) ∈ Ω. The TV-norm is an example of a convex function and the constraints are linear conditions which deﬁne a convex set. This is again an example of a convex optimization problem and can be solved efﬁciently by a range of algorithms. For the time being we will not go into the algorithms but solve it using CVXPY. The example below is based on an example from the CVXPY Tutorial3, and it is recommended to look at this tutorial for other interesting examples! Warning: the example below uses some more advanced Python programming, it is not necessary to understand. In our ﬁrst piece of code below, we load the image and a version of the image with text written on it, and display the images. The Python Image Library (PIL) is used for this purpose. In [9]: from PIL import Image # Load the images and convert to numpy arrays for processing. U = np.array(Image.open(\"../images/oculus.png\")) Ucorr = np.array(Image.open(\"../images/oculus-corr.png\")) # Display the images fig, ax = plt.subplots(1, 2,figsize=(10, 5)) ax[0].imshow(U); ax[0].set_title(\"Original Image\") ax[0].axis(’off’) ax[1].imshow(Ucorr); ax[1].set_title(\"Corrupted Image\") ax[1].axis(’off’); Original Image Corrupted Image After having the images at our disposal, we determine which entries of the corrupted image are known. We store these in a mask M , with entries mijk = 1 if the colour k of the (i, j)-th pixel is known, and 0 otherwise. In [10]: # Each image is now an m x n x 3 array, with each pixel # represented by three numbers between 0 and 255, # corresponding to red, green and blue rows, cols, colours = U.shape # Create a mask: this is a matrix with a 1 if the corresponding # pixel is known, and zero else M = np.zeros((rows, cols, colours)) for i in range(rows): for j in range(cols): for k in range(colours): if U[i, j, k] == Ucorr[i, j, k]: M[i, j, k] = 1 3http://www.cvxpy.org/en/latest/tutorial/index.html We are now ready to solve the optimization problem using CVXPY. As the problem is rather big (more than a million variables), it is important to choose a good solver that will solve the problem to sufﬁcient accuracy in an acceptable amount of time. For the example at hand, we choose the SCS solver, which can be speciﬁed when calling the solve function. In [11]: # Determine the variables and constraints variables = [] constraints = [] for k in range(colours): X = Variable(rows, cols) # Add variables variables.append(X) # Add constraints by multiplying the relevant variable matrix # elementwise with the mask constraints.append(mul_elemwise(M[:, :, k], X) == \\ (M[:, :, k], Ucorr[:, :, k])) # Create a problem instance with objective = Minimize(tv(variables[0],variables[1],variables[2])) # Create a problem instance and solve it using the SCS solver prob = Problem(objective, constraints) prob.solve(verbose=True, solver=SCS) Out [11]: 8263910.812250629 Now that we solved the optimization problem, we have a solution stored in ’variables’. We have to transform this back into an image and display the result. In [12]: # Load variable values into a single array. Urec = np.zeros((rows, cols, colours), dtype=np.uint8) for i in range(colours): Urec[:, :, i] = variables[i].value fig, ax = plt.subplots(1, 2,figsize=(10, 5)) # Display the inpainted image. ax[0].imshow(Urec); ax[0].set_title(\"Inpainted Image\") ax[0].axis(’off’) ax[1].imshow(np.abs(Ucorr[:,:,0:3] - Urec)); ax[1].set_title(\"Difference Image\") ax[1].axis(’off’); Inpainted Image Difference Image Another typical structure of images is that the singular values of the image, considered as matrix, decay quickly. The singular value decomposition (SVD) of a matrix A ∈ Rm×n is the matrix product A = U ΣV T , where U ∈ Rm×m and V ∈ Rn×n are orthogonal matrices, and Σ ∈ Rm×n is a diagonal matrix with entries σ1, . . . , σmin{m,n} on the diagonal. Instead of minimizing the TV-norm of an image X, one may instead try to minimize the Schatten 1-norm, deﬁned as the sum of the singular values, ∥U ∥S1 = σ1 + · · · + σmin{m,n}. The problem is then minimize ∥X∥S1 subject to xij = uij for (i, j) ∈ Ω. This is an instance of a type of convex optimization problem known as semideﬁnite programming. Alternatively, one may also use the 1-norm of the image applied to a discrete cosine transform (DCT) or a discrete wavelet transform (DWT). As this examples (and many more to come) shows: there is no unique choice of loss function, and hence of the objective function, for a particular problem. These choices depend on model assumptions and require some knowledge of the problem one is trying to solve. We conclude by applying the total variation inpainting procedure to set a parrot free. Caged parrot Free parrot Notes 14 Convexity Convexity is a central theme in optimization. The reason is that convex optimization problems have many favourable properties, such as lower computational complexity and the property that local minima are also global minima. Despite being a seemingly special property, convex optimization problems arise surprisingly often. Convex functions Deﬁnition 14.1. A set C ⊆ Rd is convex if for all x, y ∈ C and λ ∈ [0, 1], the line λx + (1 − λ)y ∈ C. A convex body is a convex set that is closed and bounded. Deﬁnition 14.2. Let S ⊆ Rd. A function f : S → R is called convex if S is convex and for all x, y ∈ S and λ ∈ [0, 1], f (λx + (1 − λ)y) ≤ λf (x) + (1 − λ)f (y). The function f is called strictly convex if f (λx + (1 − λ)y) < λf (x) + (1 − λ)f (y). A function f is called concave, if −f is convex. Figure 14.1 illustrates what a convex function of one variable looks like. The graph of the function lies below any line connecting two points on it. A function f has a domain dom(f ), which is where the function is deﬁned. For example, if f (x) = log(x), then dom(f ) = R+, the positive integers. The deﬁnition of a convex function thus states that the domain of f is a convex set S. We can also restrict a function on a smaller domain, even though the function could be deﬁned more generally. For example, f (x) = x3 is a convex function if restricted to the domain R≥0, but is not convex on R. A convex optimization problem is an optimization problem in which the set of constraints Ω and the function f are convex. While most general optimization problems are practically intractable, convex optimization problems can be solved efﬁciently, and still cover a surprisingly large range of applications! Convex function have pleasant properties, while at the same time covering many of the functions that arise in applications. Perhaps the most important property is that local minima are global minima. Theorem 14.3. Let f : Rd → R be a convex function. Then any local minimizer of f is a global minimizer. Proof. Let x∗ be a local minimizer and assume that it is not a global minimizer. Then there exists a vector y ∈ Rd such that f (y) < f (x∗). Since f is convex, for any λ ∈ [0, 1] and x = λy + (1 − λ)x∗ we have f (x) ≤ λf (y) + (1 − λ)f (x ∗) < λf (x ∗) + (1 − λ)f (x ∗) = f (x ∗). 77 x y (x, f(x)) (y, f(y)) Figure 14.1: A convex set and a convex function This holds for all x on the line segment connecting y and x∗. Since every open neighbourhood U of x∗ contains a bit of this line segment, this means that every open neighbourhood U of x∗ contains an x ̸= x∗ such that f (x) ≤ f (x∗), in contradiction to the assumption that x∗ is a local minimizer. It follows that x∗ has to be a global minimizer. Remark 14.4. Note that in the above theorem we made no assumptions about the differentiability of the function f ! In fact, while a convex function is always continuous, it need not be differentiable. The function f (x) = |x| is a typical example: it is convex, but not differentiable at x = 0. Example 14.5. Afﬁne functions f (x) = ⟨x, a⟩ + b and the exponential function ex are examples of convex functions. Example 14.6. In optimization we will often work with functions of matrices, where an m × n matrix is considered as a vector in Rm×n ∼= Rmn. If the matrix is symmetric, that is, if A⊤ = A, then we only care about the upper diagonal entries, and we consider the space S n of symmetric matrices as a vector space of dimension d = n(n + 1)/2 (the number of entries on and above the main diagonal). Important functions on symmetric matrices that are convex are the operator norm ∥A∥2, deﬁned as ∥A∥2 := max x : ∥x∥=1 ∥Ax∥2 ∥x∥2 , or the function log det(X), deﬁned on the set of positive semideﬁnite symmetric matrices S d +. There are useful ways of characterising convexity using differentiability. Theorem 14.7. 1. Let f ∈ C1(Rd). Then f is convex if and only if for all x, y ∈ Rd, f (y) ≥ f (x) + ∇f (x) ⊤(y − x). 2. Let f ∈ C2(Rd). Then f is convex if and only if ∇2f (x) is positive semideﬁnite for all x. If ∇2f (x) is positive deﬁnite for all x, then f is strictly convex. Example 14.8. Consider a quadratic function of the form f (x) = 1 2 x⊤Ax + b⊤x + c, where A ∈ Rn×n is symmetric. Writing out the product, we get x T Ax = (x1 · · · xn)    a11 · · · a1n ... . . . ... an1 · · · ann  |     x1 ... xn  |  = (x1 · · · xn)    a11x1 + · · · + a1nxn ... an1x1 + · · · + annxn  |  = n∑ i=1 n∑ j=1 aijxixj. Because A is symmetric, we have aij = aji, and the above product simpliﬁes to xT Ax = n∑ i=1 aiix2 i + 2 ∑ 1≤i<j≤n aijxixj. This is a quadratic function, because it involves products of the xi. The gradient and the Hessian of f (x) are found by computing the partial derivatives of f : ∂f ∂xi = n∑ j=1 aijxj + bi, ∂2f ∂xi∂xj = aij. In summary, we have ∇f (x) = Ax + b, ∇2f (x) = A. Using the previous theorem, we see that f is convex if and only if A is positive semideﬁnite. A typical example for such a function is f (x) = ∥Ax − b∥ 2 = (Ax − b) T (Ax − b) = x T AT Ax − 2bT Ax + bT b. The matrix AT A is always symmetric and positive semideﬁnite (why?) so that the function f is convex. A convenient way to visualise a function f : R2 → R is through contour plots. A level set of the function f is a set of the form {x | f (x) = c}, where c is the level. Each such level set is a curve in R2, and a contour plot is a plot of a collection of such curves for various c. If one colours the areas between adjacent curves, one gets a plot as in the following ﬁgure. A convex function is has the property that there is only one sink in the contour plot. Notes 15 Lagrangian Duality In this lecture we study optimality conditions for convex problems of the form minimize x f (x) subject to f (x) ≤ 0 h(x) = 0, (1) where x ∈ Rn, f = (f1, . . . , fm)⊤, h = (h1, . . . , hp)⊤, and the inequalities are componentwise. We assume that f and the fi are convex, and the hj are linear. It is also customary to write the conditions h(x) = 0 as Ax = b, with hj(x) = a⊤ j x − bj, a⊤ j being the j-th row of A. The feasible set of (1) is the set F = {x | fi(x) ≤ 0, Ax = b} It is easy to see that F is convex if the fi are convex. If the objective f and the fi are also linear, then (1) is called a linear programming problem, and F is a polyhedron. Such problems have been studied extensively and can be solved with efﬁcient algorithms such as the simplex method. A ﬁrst-order optimality condition We ﬁrst generalize the standard ﬁrst-order optimality conditions for differentiable functions to the setting of constrained convex optimization. Theorem 15.1. Let f ∈ C1(Rn) be a convex, differentiable function, and consider a convex optimization problem of the form (1). Then x∗ is an optimal point of the optimization problem minimize f (x) subject to x ∈ F if and only if for all y ∈ F, ⟨∇f (x ∗), y − x ∗⟩ ≥ 0, (15.1) where F is the feasible set of the problem. Proof. Suppose x∗ is such that (1) holds. Then, since f is a convex function, for all y ∈ F we have, f (y) ≥ f (x ∗) + ⟨∇f (x ∗), y − x∗⟩ ≥ f (x∗), 81 which shows that x∗ is a minimizer in F. To show the opposite direction, assume that x∗ is a minimizer but that (15.1) does not hold. This means that there exists a y ∈ F such that ⟨∇f (x∗), y − x∗⟩ < 0. Since both x∗ and y are in F and F is convex, any point z(λ) = (1 − λ)x∗ + λy with λ ∈ [0, 1] is also in F. At λ = 0 we have df dλ f (z(λ))|λ=0 = ⟨∇f (x∗), y − x∗⟩ < 0. Since the derivative at λ = 0 is negative, the function f (z(λ)) is decreasing at λ = 0, and therefore, for small λ > 0, f (z(λ)) < f (z(0)) = f (x∗), in contradiction to the assumption that x∗ is a minimizer. Example 15.2. In the absence of constraints, F = Rn, and the statement says that ∀y ∈ R n : ⟨∇f (x∗), y − x ∗⟩ ≥ 0. Given y such that ⟨∇f (x∗), y − x∗⟩ ≥ 0, then replacing y by 2x−y we also have the converse inequality, and therefore the optimality condition is equivalent to saying that ∇f (x∗) = 0. We therefore recover the well-known ﬁrst order optimality condition. Geometrically, the ﬁrst order optimality condition means that the set {x | ⟨∇f (x ∗), x⟩ = ⟨∇f (x ∗), x∗⟩} deﬁnes a supporting hyperplane to the set F. −∇f (x∗) x∗y Figure 15.1: Optimality condition Lagrangian duality Recall the method of Lagrange multipliers. Given two functions f (x, y) and h(x, y), if the problem minimize f (x, y) subject to h(x, y) = 0 has a solution (x∗, y∗), then there exists a parameter λ, the Lagrange multiplier, such that ∇f (x∗, y∗) = λ∇h(x∗, y∗). (15.2) In other words, if we deﬁne the Lagrangian as L(x, y, λ) = f (x, y) − λh(x, y), then (15.2) says that ∇L(x∗, y∗, λ) = 0 for some λ. The intuition is as follows. The set M = {(x, y) ∈ R2 | h(x, y) = 0} is a curve in R2, and the gradient ∇h(x, y) is perpendicular to M at every point (x, y) ∈ M . For someone living inside M , a vector that is perpendicular to M is not visible, it is zero. Therefore the gradient ∇f (x, y) is zero as viewed from within M if it is perpendicular to M , or equivalently, a multiple of ∇h(x, y). Alternatively, we can view the graph of f (x, y) in three dimensions. A maximum or minimum of f (x, y) along the curve deﬁned by h(x, y) = 0 will be a point at which the direction of steepest ascent ∇f (x, y) is perpendicular to the curve h(x, y) = 0. Example 15.3. Consider the function f (x, y) = x2y with the constraint h(x, y) = x2 + y2 − 3 (a circle of radius √3). The Lagrangian is the function L(x, y, λ) = x2y − λ(x 2 + y2 − 3). Computing the partial derivatives gives the three equations ∂ ∂x L = 2xy − 2λx = 0 ∂ ∂y L = x2 − 2λy = 0 ∂ ∂λ L = x2 + y2 − 3 = 0. From the second equation we get λ = x2 2y , and the ﬁrst and third equations become 2xy − x3 y = 0 x2 + y2 − 3 = 0. Solving this system, we get six critical point (± √2, ±1), (0, ± √3). To ﬁnd out which one of these is the minimizers, we just evaluate the function f on each of these. We now turn to convex problems of the more general form form minimizef (x) subject to f (x) ≤ 0 h(x) = 0, (15.3) Denote by D the domain of all the functions f, fi, hj, i.e., D = dom(f ) ∩ dom(f1) ∩ · · · ∩ dom(fm) ∩ dom(h1) ∩ · · · ∩ dom(hp). Assume that D is not empty and let p∗ be the optimal value of (15.3). The Lagrangian of the system is deﬁned as L(x, λ, µ) = f (x) + λ⊤f (x) + µ ⊤h(x) = f (x) + m∑ i=1 λifi(x) + p∑ i=1 µihi(x). The vectors λ and µ are called the dual variables or Lagrange multipliers of the system. The domain of L is D × Rm × Rp. Deﬁnition 15.4. The Lagrange dual of the problem (15.3) is the function g(λ, µ) = inf x∈D L(x, λ, µ). If the Lagrangian L is unbounded from below, then the value is −∞. The Lagrangian L is linear in the λi and µj variables. The inﬁmum of a family of linear functions is concave, so that the Lagrange dual is a concave function. Therefore the negative −g(λ, µ) is a convex function. Lemma 15.5. For any µ ∈ Rp and λ ≥ 0 we have g(λ, µ) ≤ p∗. Proof. Let x∗ be a feasible point for (15.3), that is, fi(x∗) ≤ 0, hj(x ∗) = 0, 1 ≤ i ≤ m, 1 ≤ j ≤ p. Then for λ ≥ 0 and any µ, since each hj(x∗) = 0 and λjfj(x∗) ≤ 0, L(x ∗, λ, µ) = f (x∗) + m∑ i=1 λifi(x ∗) + p∑ j=1 µjhj(x ∗) ≤ f (x ∗). In particular, g(λ, µ) = inf x L(x, λ, µ) ≤ L(x ∗, λ, µ) ≤ f (x∗). Since this holds for all feasible x∗, it holds in particular for the x∗ that minimizes (15.3), for which f (x∗) = p∗. A point (λ, µ) with λ ≥ 0 and (λ, µ) ∈ dom(g) is called a feasible point of the dual problem. The Lagrange dual problem of the optimization problem (15.3) is the problem maximize g(λ, µ) subject to λ ≥ 0. (15.4) If q∗ is the optimal value of (15.4), then q∗ ≤ p∗. In the special case of linear programming we actually have q∗ = p∗. We will see that under certain conditions, we have q∗ = p∗ for more general problems, but this is not always the case. Notes 16 KKT Conditions For convex problems of the form minimize f (x) subject to f (x) ≤ 0 Ax = b, (1) we introduced the Lagrangian L(x, λ, µ) and deﬁned the Lagrange dual as g(λ, µ) = inf x∈D L(x, λ, µ). We saw that g(λ, µ) is a lower bound on the optimal value of (1). Note that here we wrote the conditions hj(x) = 0 as system of linear equations Ax = b, since for the problem to be convex, we require that the hj be linear functions. We will derive conditions under which the lower bound provided by the Lagrange dual matches the upper bound, and derive a system of equations and inequalities that certify optimality, the Karush-Kuhn-Tucker (KKT) conditions. These conditions can be seen as generalizations of the ﬁrst-order optimality conditions to the setting when equality and inequality constraints are present. Constraint qualiﬁcation Consider a linear programming problem of the form minimize ⟨c, x⟩ subject to Ax = b x ≥ 0. The inequality constraints are −xi ≤ 0, while the equality constraints are a⊤ i x − bi. The Lagrangian has the form L(x, λ, µ) = ⟨c, x⟩ − n∑ i=1 λixi + m∑ j=1 µj(a ⊤ j x − bj) = (c − λ + A⊤µ)⊤x − b⊤µ. The inﬁmum over x of this function is −∞ unless c − λ + A⊤µ = 0. The Lagrange dual is therefore g(λ, µ) = { −µ⊤b if c − λ + A⊤µ = 0 −∞ else. 85 From Lemma 15.5 we conclude that max{−b⊤µ | c − λ + A⊤µ = 0, λ ≥ 0} ≤ min{c⊤x | Ax = 0, x ≥ 0}. Note that if we write y = −µ and s = λ, then we get the dual version of the linear programming problem we started out with, and in this case we know that max λ≥0,µ g(λ, µ) = p∗. In the example of linear programming, we have seen that the optimal value of the dual problem is equal to the optimal value of the primal problem. In general, we have d ∗ = sup λ>0,µ g(λ, µ) ≤ inf x∈D{f (x) | fi(x) ≤ 0, Ax = b} = p∗. Once certain conditions, called constraint qualiﬁcations, hold, we can ensure that strong duality holds, which means d∗ = p∗. One particular such constraint qualiﬁcation is Slater’s Theorem. Theorem 16.1. (Slater conditions) Assume that the interior of the domain D of (1) is non-empty, that the problem (1) is convex, and that there exists a point x ∈ D such that fi(x) < 0, 1 ≤ i ≤ m, Ax = b, 1 ≤ j ≤ p. Then d∗ = p∗, the primal optimal value coincides with the dual optimal value. Example 16.2. The problem minimize e−x subject to x2/y ≤ 0, y ≥ 0 is an example of a convex problem that does not satisfy strong duality. Example 16.3. Consider the problem minimize x ⊤x subject to Ax = b. The Lagrangian is L(x, µ) = x⊤x + µ⊤(Ax − b). For any µ, we can ﬁnd the inﬁmum g(µ) = inf x L(x, µ) by setting the derivative of the Lagrangian to x to zero: ∇xL(x, µ) = 2x + A⊤µ = 0, which gives the solution x = − 1 2 A⊤µ. The dual function is therefore g(µ) = − 1 4 µ ⊤A⊤Aµ − b ⊤µ. As the negative of a positive semideﬁnite quadratic function, it is concave. Moreover, we get the lower bound − 1 4 µ ⊤A⊤Aµ − b⊤µ ≤ inf{x ⊤x | Ax = b}. The problem we started out with is convex, and if we assume that there exists a feasible primal point, then the above inequality is in fact an equality by Slater’s conditions. Karush-Kuhn-Tucker optimality conditions Consider now a not necessarily convex problem of the form minimize f (x) subject to f (x) ≤ 0 h(x) = 0, (16.1) If p∗ is the optimal solution of (16.1) and (λ, µ) dual variables, then we have seen that (this holds even in the non-convex case) p ∗ ≥ g(λ, µ). From this is follows that for any primal feasible point x, f (x) − p ∗ ≤ f (x) − g(λ, µ). The difference f (x) − g(λ, µ) between the primal objective function at a primal feasible point and the dual objective function at a dual feasible point is called the duality gap at x and (λ, µ). For any such points we know that p∗, q∗ ∈ [g(λ, µ), f (x)], and if the gap is small we have a good approximation of the primal and dual optimal values. The duality gap can be used in iterative algorithms to deﬁne stopping criteria: if the algorithm generates a sequence of primal-dual variables (xk, λk, µk), then we can stop if the duality gap is less than, say, a predeﬁned tolerance ε. Now suppose that we have points (x∗, λ∗, µ∗) such that the duality gap is zero. Then f (x ∗) = g(λ∗, µ ∗) = inf x  f (x) + m∑ i=1 λ∗ i fi(x) + p∑ j=1 µ∗ j hj(x)   ≤ f (x ∗) + m∑ i=1 λ ∗ i fi(x∗) + p∑ j=1 µ∗ j hj(x ∗) ≤ f (x ∗), where the last inequality follows from the fact that hj(x∗) = 0 and λ∗ i fi(x∗) ≤ 0 for 1 ≤ j ≤ p and 1 ≤ i ≤ m. It follows that the inequalities are in fact equalities. From the identity f (x∗) = f (x∗) + m∑ i=1 λ ∗ i fi(x∗) and λ∗ i ≥ 0 and fi(x∗) ≤ 0 we also conclude that at such optimal points, λ∗ i fi(x∗) = 0, 1 ≤ i ≤ m. This condition is known as complementary slackness. From the above we also see that x∗ minimizes the Lagrangian L(x, λ∗, µ∗), so that the gradient of that function is zero: ∇xL(x∗, λ∗, µ∗) = 0. Collecting these conditions (primal and dual feasibility, complementary slackness, vanishing gradient), we arrive at a set of optimality conditions known as the Karush-Kuhn-Tucker (KKT) conditions. Theorem 16.4. (KKT conditions) Let x∗ and (λ∗, µ∗) be primal and dual optimal solutions of (16.1) with zero duality gap. The the following conditions are satisﬁed: f (x ∗) ≤ 0 h(x ∗) = 0 λ∗ ≥ 0 λ∗ i fi(x ∗) = 0, 1 ≤ i ≤ m ∇xf (x ∗) + m∑ i=1 λ∗ i ∇xfi(x∗) + p∑ j=1 µ∗ j ∇xhj(x ∗) = 0. Moreover, if the problem is convex and the Slater Conditions (Theorem 16.1) are satisﬁed, then any points satisfying the KKT conditions have zero duality gap. Notes 17 Support Vector Machines I In this lecture we return to the task of classiﬁcation. As seen earlier, examples include spam ﬁlters, letter recognition, or text classiﬁcation. In this lecture we introduce a popular method for classiﬁcation, Support Vector Machines (SVMs), from the point of view of convex optimization. Linear Support Vector Machines In the simplest case there is a set of labels Y = {−1, 1} and the set of training points {x1, . . . , xn} ⊂ Rd is linearly separable: this means that there exists an afﬁne hyperplane h(x) = w⊤x + b such that h(xi) > 0 if yi = 1 and h(xj) < 0 if yj = −1. We call the points for which yi = 1 positive, and the ones for which yj = −1 negative. The problem of ﬁnding such a hyperplane can be posed as a linear programming feasibility problem as follows: we look for a vector of weights w and a bias term b (together a (p + 1)-dimensional vector) such that w⊤xi + b ≥ 1, for yi = 1, w⊤xj + b ≤ −1, for yj = −1. Note that we can replace the +1 and −1 with any other positve or negative quantity by rescaling the w and b, so this is just convention. We can also describe the two inequalities concisely as yi(w⊤xi + b) − 1 ≥ 0. (17.1) A hyperplane separating the two point sets will in general not be unique. As we want to use the linear classiﬁer on new, yet unknown data, we want to ﬁnd a separating hyperplane with best possible margin. Let δ+ and δ− denote the distance of a separating hyperplane to the closest positive and closest negative point, respectively. The quantity δ = δ+ + δ− is then called the margin of the classiﬁer, and we want to ﬁnd a hyperplane with largest possible margin. We next show that the margin for a separating hyperplane that satisﬁes (17.1) is δ = 2/∥w∥2. Given a hyperplane H described in (17.1) and a point x such that we have the equality w⊤x + b = 1 (the point is as close as possible to the hyperplane, also called a support vector), the distance of that point to the hyperplane can be computed by ﬁrst taking the difference of x with a point p on H (an anchor), and then computing the dot product of x − p with the unit vector w/∥w∥ normal to H. As anchor point p we can just choose a multiple cw that is on the plane, i.e., that satisﬁes ⟨w, cw⟩+b = 89 Figure 17.1: A hyperplane separating two sets of points with margin and support vectors. x w ∥w∥ H Figure 17.2: Computing the distance to the hyperplane 0. This implies that c = −b/∥w∥2, and consequently p = −(b/∥w∥2)w. The distance is then δ+ = ⟨x + b ∥w∥2 w, w ∥w∥ ⟩ = ⟨x, w⟩ ∥w∥ + b ∥w∥2 ⟨w, w ∥w∥ ⟩ = 1 − b ∥w∥ + b ∥w∥ = 1 ∥w∥ . Similarly, we get δ− = 1/∥w∥. The margin of this particular separating hyperplane is thus δ = 2/∥w∥. If we want to ﬁnd a hyperplane with largest margin, we thus have to solve the quadratic optimization problem minimize w,b 1 2 ∥w∥ 2 subject to yi(w⊤xi + b) − 1 ≥ 0, 1 ≤ i ≤ n. Note that b is also an unknown variable in this problem! The factor 1/2 in the objective function is just to make the gradient look nicer. The Lagrangian of this problem is L(w, b, λ) = 1 2 ∥w∥ 2 − m∑ i=1 λiyiw⊤xi − λiyib + λi = 1 2 w⊤w − λ⊤Xw − bλ⊤y + m∑ i=1 λi, where we denote by X the matrix with the yix⊤ i as rows. We can then write the conditions on the gradient with respect to w and b of the Lagrangian as ∇wL(w, b, λ) = w − X ⊤λ = 0 ∂L ∂b (w, b, λ) = y⊤λ = 0. (17.2) If yT λ ̸= 0, then the conditions (17.2) cannot be satisﬁed and the problem is unbounded from below. If yT λ = 0, then the ﬁrst condition in (17.2) is necessary and sufﬁcient for a minimizer. Replacing w by X ⊤λ and λ⊤y by 0 in the Lagrangian function then gives the expression for the Lagrange dual g(λ), g(λ) = { − 1 2 λ⊤XX ⊤λ + ∑m i=1 λi if yT λ = 0 −∞ else. Finally, maximizing this function and changing the sign, so that the maximum becomes a minimum, we can formulate the Lagrange dual optimization problem as minimize 1 2 λ⊤XX ⊤λ − λ⊤e subject to λ ≥ 0, (17.3) where e is the vector of all ones. Notes 18 Support Vector Machines II The linear separation problem was introduced as the problem of ﬁnding a hyperplane H = {x ∈ R d : wT x + b = 0} that would separate data points xi with label yi = 1 from data points xj with label yj = −1. For convenience, we collect our data in matrices and vectors, where X ∈ Rn×d is the matrix with rows yix⊤ i and y the vectors of labels yi. Assuming that the data is linearly separable, the problem of ﬁnding a separating hyperplane of maximal margin can be formulated as minimize 1 2 ∥w∥ 2 subject to e − by − X T x ≤ 0, (P) where e is the vector of all ones. The Lagrange dual optimization problem is minimize 1 2 λ⊤XX ⊤λ − λ⊤e subject to λ ≥ 0. (D) Note that there is one dual variable λi per data point xi. We can ﬁnd the optimal value by solving the dual problem (D), but that does not give us automatically the weights w and the bias b. We can ﬁnd the weights by w = X ⊤λ. As for b, this is best determined from the KKT conditions of the problem. These can be written by combining the constraints of the primal problem with the conditions on the gradient of the Lagrangian, the condition λ ≥ 0, and complementary slackness as Xw + by − e ≥ 0 λ ≥ 0 λi(1 − yi(w⊤xi + b)) = 0 for 1 ≤ i ≤ n w − X ⊤λ = 0 y⊤λ = 0. To get b, we can choose one of the equations in which λi ̸= 0, and then ﬁnd b by setting b = yi(1 − yiw⊤xi). With the KKT conditions written down, we can go about solving the problem of ﬁnding a maximum margin linear classiﬁer using methods such as the barrier method. 93 Extensions So far we looked at the particularly simple case where (a) the data falls into two classes, (b) the points can actually be well separated, and (c) they can be separated by an afﬁne hyperplane. In reality, these three assumptions may not hold. We brieﬂy discuss extensions of the basic model to account for the three situations just mentioned. Non-exact separation What happens when the data can not be separated by a hyperplane? In this case the constraints can not be satisﬁed: there is no feasible solution to the problem. We can still modify the problem to allow for misclassiﬁcation: we want to ﬁnd a hyperplane that separates the two point sets as good as possible, but we allow for some mistakes. One approach is to add an additional set of n slack variables s1, . . . , sn, and modify the constraints to w⊤xi + b ≥ 1 − si, for yi = 1, w⊤xj + b ≤ −1 + sj, for yj = −1, si ≥ 0. The i-th data point can land on the wrong side of the hyperplane if si > 1, and consequently the sum∑n i=1 si is an upper bound on the number of errors possible. If we want to minimize the number of misclassiﬁed points, we may want to minimize this upper bound, so a sensible choice for objective function would be to add a multiple of this sum. The new problem thus becomes minimize 1 2 ∥w∥ 2 + µ n∑ j=1 sj subject to yi(w⊤xi + b) − 1 + si ≥ 0, 1 ≤ i ≤ n si ≥ 0, 1 ≤ i ≤ n, for some parameter µ. The Lagrangian of this problem and the KKT conditions can be derived in a similar way as in the separable case and are left as an exercise. Non-linear separation and kernels The key to extending SVMs from linear to non-linear separation is the observation that the dual form of the optimization problem (D) depends only on the dot products ⟨xi, xj⟩ of the data points. In fact, the (i, j)-th entry of the matrix XX ⊤ is precisely ⟨xi, xj⟩! If we map our data into a higher (possibly inﬁnite) dimensional space H, ϕ : R p → H, and consider the data points ϕ(xi), 1 ≤ i ≤ n, then applying the support vector machine to these higher dimensional vectors will only depend on the dot products K(xi, xj) = ⟨ϕ(xi), ϕ(xj)⟩. The function K is called a kernel function. A typical example, often used in practice, is the Gaussian radial basis function (RBF), K(x, y) = e−∥x−y∥2/2σ2. Note that we don’t need to know how the function ϕ looks like! In the equation for the hyperplane we simply replace w⊤x with K(w, x). The only difference now is that the function ceases to be linear in x: we get a non-linear decision boundary. Multiple classes One is often interested in classifying data into more than two classes. There are two simple ways in which support vector machines can be extended for such problems: one-vs-one and one-vs-rest. In the one-vs-one case, given k classes, we train one classiﬁer for each pair of classes in the training data, obtaining a total of k(k − 1)/2 classiﬁers. When it comes to prediction, we apply each of the classiﬁers to our test data and choose the class that was chosen the most among all the classiﬁers. In the one-vs-rest approach, each train k binary classiﬁers: in each one, one class corresponds to a chosen class, and the second class corresponds to the rest. By associating conﬁdence scores to each classiﬁer, we choose the one with the highest conﬁdence score. Example 18.1. An example that uses all three extensions mentioned is handwritten digit recognition. Sup- pose we have a series of pixels, each representing a number, and associated labels {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. We would like to train a support vector machine to recognize new digits. Given the knowledge we have, we can implement this task using standard optimization software such as CVXPY. Luckily, there are packages that have this functionality already implemented, such as the SCIKIT-LEARN package for Python. We illustrate its functioning below. The code also illustrates some standard procedures when tackling a machine learning problem: • Separate the data set randomly into training data and test data; • Create a support vector classiﬁer with optional parameters; • Train (using FIT) the classiﬁer with the training data; • Predict the response using the test data and compare with the true response; • Report the results. An important aspect to keep in mind is that when testing the performance using the test data, we should compare the classiﬁcation accuracy to a naive baseline: if, for example, 80% of the test data is classiﬁed as +1, then making a prediction of +1 for all the data will give us an accuracy of 80%; in this case, we would want our classiﬁer to perform considerably better than getting the right answer 80% of the time! In [15]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline from sklearn import svm, datasets, metrics from sklearn.model_selection import train_test_split In [16]: digits = datasets.load_digits() # Display images and labels images_and_labels = list(zip(digits.images, digits.target)) for index, (image, label) in enumerate(images_and_labels[:4]): plt.subplot(2, 4, index + 1) plt.axis(’off’) plt.imshow(image, cmap=plt.cm.gray_r, interpolation=’nearest’) plt.title(’Training: %i’ % label) # Turn images into 1-D arrays n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) # Create classifier svc = svm.SVC(gamma=0.001) # Randomly split data into train and test set X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size = 0.4, random_state=0) svc.fit(X_train, y_train) Out [2]: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel=’rbf’, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Now apply prediction to test set and report performance. In [3]: predicted = svc.predict(X_test) print(\"Classification report for classifier %s:\\n%s\\n\" % (svc, metrics.classification_report(y_test, predicted))) Out [3]: Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,vdecision_function_shape=None, degree=3, gamma=0.001, kernel=’rbf’, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False): precision recall f1-score support 0 1.00 1.00 1.00 60 1 0.97 1.00 0.99 73 2 1.00 0.97 0.99 71 3 1.00 1.00 1.00 70 4 1.00 1.00 1.00 63 5 1.00 0.98 0.99 89 6 0.99 1.00 0.99 76 7 0.98 1.00 0.99 65 8 1.00 0.99 0.99 78 9 0.99 1.00 0.99 74 avg / total 0.99 0.99 0.99 719 In [4]: import skimage from skimage import data from skimage.transform import resize from skimage import io import os Now try this out on some original data! In [5]: mydigit1 = io.imread(’images/digit9.png’) mydigit2 = io.imread(’images/digit4.png’) plt.figure(figsize=(8, 4)) plt.subplot(1,2,1) plt.imshow(mydigit1, cmap=plt.cm.gray_r, interpolation=’nearest’) plt.axis(’off’) plt.subplot(1,2,2) plt.imshow(mydigit2, cmap=plt.cm.gray_r, interpolation=’nearest’) plt.axis(’off’) plt.show() In [6]: smalldigit1 = resize(mydigit1, (8,8)) smalldigit2 = resize(mydigit2, (8,8)) mydigits = np.concatenate((np.round(15*(np.ones((8,8))- smalldigit1[:,:,0])).reshape((64,1)).T, np.round(15*(np.ones((8,8))- smalldigit2[:,:,0])).reshape((64,1)).T),axis=0) # After some preprocessing, make prediction guess = svc.predict(mydigits) print guess [9 4] Notes 19 Iterative Algorithms Most modern optimization methods are iterative: they generate a sequence of points x0, x1, . . . in Rd in the hope that this sequences will converge to a local or global minimizer x∗ of a function f (x). A typical rule for generating such a sequence would be to start with a vector x0, chosen by an educated guess, and then for k ≥ 0, move from step k to k + 1 by xk+1 = xk + αkpk, in a way that ensures that f (xk+1) ≤ f (xk). The parameter αk is called the step length, while pk is the search direction. The are many ways in which the direction pk and the step length αk can be chosen. If we take pk = −∇f (xk), (19.1) then we take a step in the direction of steepest descent and the resulting method is (unsurprisingly) called gradient descent. If there is second-order information available, then we can take steps of the form pk = −∇2f (xk) −1∇f (xk). (19.2) The resulting method is called Newton’s Method. If applicable, Newton’s method tends to converge faster to a solution, but the computation at each step is more expensive. Gradient descent In the method of gradient descent, the search direction is chosen as pk = −∇f (xk). To see why this makes sense, let p be a direction and consider the Taylor expansion f (xk + αp) = f (xk) + α⟨p, ∇f (xk)⟩ + O(α2). Considering this as a function of α, the rate of change in direction p at xk is the derivative of this function at α = 0, df (xk + αp) dα |α=0 = ⟨p, ∇f (xk)⟩, also known as the directional derivative of f at xk in the direction p. This formula indicates that the rate of change is negative, and we have a descent direction, if ⟨p, ∇f (xk)⟩ < 0. 99 The Cauchy-Schwarz inequality gives the bounds −∥p∥2∥∇f (xk)∥2 ≤ ⟨p, ∇f (xk)⟩ ≤ ∥p∥2∥∇f (xk)∥2. We see that the rate of change is the smallest when the ﬁrst inequality is an equality, which happens if p = −α∇f (xk) for some α > 0. For a visual interpretation of what it means to be a descent direction, note that the angle θ between a vector p and the gradient ∇f (x) at a point x is given by (see Preliminaries, Page 9) ⟨x, ∇f (x)⟩ = ∥p∥2∥∇f (x)∥2 cos(θ). This is negative if the vector p forms and angle greater than π/2 with the gradient. Recall that the gradient points in the direction of steepest ascent, and is orthogonal to the level sets. If you are standing on the slope of a mountain, walking along the level set lines will not change your elevation, the gradient points to the steepest upward direction, and the negative gradient to the steepest descent. Figure 19.1: A descent direction Any multiple α∇f (xk) points in the direction of steepest descent, but we have to choose a sensible parameter α to ensure that we make sufﬁcient progress, but at the same time don’t overshoot. Ideally, we would choose the value αk that minimizes f (xk − αk∇f (xk)). While ﬁnding such a minimizer is in general not easy (see Section Lecture 4 for alternatives), for quadratic functions in can be given in closed form. Linear least squares Consider a function of the form f (x) = 1 2 ∥Ax − b∥ 2 2. The Hessian is symmetric and positive semideﬁnite, with the gradient given by ∇f (x) = A⊤(Ax − b). The method of gradient descent proceeds as xk+1 = xk − αkA⊤(Axk − b). To ﬁnd the best αk, we compute the minimum of the function α 7→ ϕ(α) = f (xk − αA ⊤(Axk − b)). (19.3) If we set rk := A⊤(b − Axk) = −∇f (xk) and compute the minimum of (19.3) by setting the derivative to zero, ϕ′(α) = d dα f (xk + αrk) = ⟨∇f (xk + αrk), rk⟩ = ⟨A ⊤(A(xk + αrk) − b), rk⟩ = ⟨A ⊤(Axk − b), rk⟩ + α2⟨A⊤Ark, rk⟩ = −r⊤ k rk + αr⊤ k A⊤Ark = 0, we get the step length αk = r⊤ k rk r⊤ k A⊤Ark = ∥rk∥2 2 ∥Ark∥2 2 . Note also that when we have rk and αk, we can compute the next rk as rk+1 = A⊤(b − Axk+1) = A⊤(b − A(xk + αkrk)) = A⊤(b − Axk − αkArk) = rk − αkA ⊤Ark. The gradient descent algorithm for the linear least squares problem proceeds by ﬁrst computing r0 = A⊤(b − Ax0), and then at each step αk = r⊤ k rk r⊤ k A⊤Ark xk+1 = xk + αkrk rk+1 = rk − αkA⊤Ark. Does this work? How do we know when to stop? It is worth noting that the residual satisﬁes r = 0 if and only if x is a stationary point, in our case, a minimizer. One criteria for stopping could then be to check whether ∥rk∥2 ≤ ε for some given tolerance ε > 0. One potential problem with this criterion is that the function can become ﬂat long before reaching a minimum, so an alternative stopping method would be to stop when the difference between two successive points, ∥xk+1 − xk∥2, becomes smaller than some ε > 0. Example 19.1. We plot the trajectory of gradient descent with the data A =  2 0 1 3 0 1   , b =   1 −1 0   . As can be seen from the plot, we always move in the direction orthogonal to a level set, and stop at a point where we are tangent to a level set. Figure 19.2: Trajectory of gradient descent Step length selection While for a quadratic function of the form ∥Ax−b∥2 it was possible to ﬁnd a step length αk by minimizing the function in the direction of steepest descent, in general this may not be possible or even desirable. The step length is often called the learning rate in machine learning. A good step length • is not too small (so that the algorithm does not take too long); • is not too large (we might end up at a point with larger function value); • is easy to compute. There are conditions (such as the Armijo-Goldstein or the Wolfe conditions) that ensure a sufﬁcient decrease at each step. Another common approach is backtracking: in this method one uses a high initial value of α (for example, α = 1), and then decreases it until the sufﬁcient descent condition is satisﬁed. 20 Convergence Iterative algorithms for solving a problem of the form minimize f (x), x ∈ Rd (20.1) generate a sequence of vectors x0, x1, . . . in the hope that this sequence converges to a (local or global) minimizer x∗ of (20.1). In this lecture we study what it means for a sequence to converge, and how to quantify the speed of convergence. We then study the convergence of of gradient descent for quadratic functions and for convex functions satisfying certain smoothness assumptions. Convergence of iterative methods A sequence of vectors {xk}k∈N0 ⊂ Rd converges to x∗ with respect to a norm ∥·∥ as k → ∞, written xk → x, if the sequence of numbers ∥xk − x∗∥ converges to zero. Iterative algorithms will typically not ﬁnd the exact solution to a problem like (20.1). In fact, computers are not capable of telling very small numbers (say, 2−53 in double precision arithmetic) from 0, so ﬁnding a numerically exact solution is in general not possible. In addition, in machine learning, high accuracy is not necessary or even desirable due to the unavoidable statistical error. Deﬁnition 20.1. Assume that a sequence of vectors {xk}k∈N0 converges to x∗. Then the sequence is said to converge (a) linearly (or Q-linear, Q for Quotient), if there exist an r ∈ (0, 1) such that for sufﬁciently large k, ∥xk+1 − x∗∥ ≤ r∥xk − x ∗∥. (b) superlinearly, if lim k→∞ ∥xk+1 − x∗∥ ∥xk − x∗∥ = 0, (c) with order p, if there exists a constant M > 0, such that for sufﬁciently large k, ∥xk+1 − x ∗∥ ≤ M ∥xk − x∗∥ p. The case p = 2 is called quadratic convergence. 103 These deﬁnitions depend on the choice of a norm, but any two norms on Rd are equivalent, convergence with respect to one norm implies convergence with respect to any other norm. Note that the deﬁnitions above start with the assumption that the sequence {xk} converges to x∗. Therefore, for sufﬁciently large k, ∥xk − x∗∥ < 1 and if {xk} converges with order of convergence p > 1, then ∥xk+1 − x∗∥ ∥xk − x∗∥p−1 ≤ M ∥xk − x∗∥ < M. This shows that convergence of order p implies convergence of any lower order and also superlinear convergence. Example 20.2. Consider the sequence of numbers xk = 1/2rk for some r > 1. Clearly, xk → x∗ = 0 as k → ∞. Moreover, xk+1 = 1 2rk+1 = 1 2rkr = ( 1 2rk )r = x r k, which shows that the sequence has rate of convergence r. Convergence of gradient descent for least squares Throughout this section, ∥·∥ refers to the 2-norm. We study the convergence of gradient descent for the least squares problem minimize f (x) = 1 2 ∥Ax − b∥ 2, (20.2) where A ∈ Rm×n with m ≥ n is a matrix of full rank. The function f (x) is convex, since it is a quadratic function with positive semi-deﬁnite Hessian AT A. Gradient descent produces a sequence of vectors by the rule xk+1 = xk + αkrk, where the step length αk and the residual rk are given by αk = ∥rk∥2 ∥Ark∥2 , rk = A⊤(b − Axk) = −∇f (xk). At the minimizer x∗, the residual is r = −∇f (x∗) = 0 and if the sequence xk converges to x∗, the norms of the residuals converge to 0. Conversely, the residual is related to the difference xk − x∗ by rk = A ⊤(b − Axk) = A⊤(b − Axk − (b − Ax∗)) = A ⊤A(xk − x ∗). (20.3) Therefore ∥xk − x ∗∥ = ∥(A ⊤A) −1rk∥ ≤ ∥(A⊤A) −1∥∥rk∥, where ∥B∥ = maxx̸=0 ∥Bx∥/∥x∥ is the operator norm of a matrix B with respect to the 2-norm. Consequently, if the sequence ∥rk∥ converges to zero, so does the sequence ∥xk − x∗∥. A reasonable criterion to stop the algorithm is therefore when the residual norm ∥rk∥ is smaller than a predeﬁned tolerance ε > 0. The following theorem (whose proof we omit) shows that the gradient descent method for linear least squares converges linearly with respect to the A norm. The statement involves the condition number of A1. This quantity is deﬁned as κ(A) = ∥A∥ · ∥A†∥, 1The concept of condition number, introduced by Alan Turing while in Manchester, is one of the most important ideas in numerical analysis, as it is indispensable in studying the performance of numerical algorithms. where A† is the pseudoinverse of A. If A ∈ Rm×n with m ≥ n and linearly independent columns, it is deﬁned as A† = (A⊤A)−1A⊤. The condition number is always greater than or equal to one. Theorem 20.3. The error in the k + 1-th iterate is bounded by ∥xk+1 − x ∗∥ ≤ ( κ2(A) − 1 κ2(A) + 1 ) ∥xk − x∗∥. In particular, the gradient descent algorithm converges linearly. We can deduce Theorem 20.3 as a special case of a more general convergence result from convex function satisfying certain smoothness assumptions. Notes 21 Gradient Descent In this lecture we will derive a convergence result for gradient descent applied to a convex function f ∈ C1(Rd). Convergence results in optimization are often stated in terms of the difference f (xk) − f ∗, where f ∗ = f (x∗) and x∗ is a minimizer of f . By the convexity of f , we have f (x k) − f (x ∗) ≤ ⟨∇f (x k), x k − x ∗⟩ ≤ ∥∇f (x k)∥ · ∥x k − x∗∥, (21.1) which allows to relate the convergence of f (xk) − f ∗ to the convergence of ∥xk − x∗∥. In order to guarantee good convergence rates, we need some additional smoothness and boundedness conditions. Gradient descent for smooth convex functions The most common smoothness condition in optimization is Lipschitz continuity, applied to a function or to its gradient. Deﬁnition 21.1. A function f : Rd → Rk is called Lipschitz continuous with Lipschitz constant L > 0, if for all x, y ∈ Rd, ∥f (x) − f (y)∥ ≤ L · ∥x − y∥. A function f : Rd → R is called β-smooth for some β > 0 if the gradient is Lipschitz continuous: ∥∇f (x) − ∇f (y)∥ ≤ β · ∥x − y∥ for all x, y ∈ Rd. Lipschitz continuity lies somewhere between continuity and differentiability. If f ∈ C1(Rd) is Lipschitz continuous with Lipschitz constant L, then ∥∇f (x)∥ ≤ L. Similarly, β-smoothness implies that ∥∇2f (x)∥ ≤ β. Recall from Lecture 14 that a function f ∈ C1(Rd) is convex if and only if f (y) + ⟨∇f (y), x − y⟩ ≤ f (x) (21.2) for all x, y ∈ Rd. The following result shows that β-smoothness is equivalent to a quadratic upper bound on the difference between the function value f (x) and its linear lower bound (21.2). Lemma 21.2. Let f ∈ C1(Rd) be β-smooth and convex. Then for any x, y ∈ Rd, f (x) ≤ f (y) + ⟨∇f (y), x − y⟩ + β 2 ∥x − y∥ 2. (21.3) 107 Conversely, if a convex function f ∈ C1(Rd) satisﬁes (21.3), then for all x, y ∈ Rd, 1 β ∥∇f (x) − ∇f (y)∥2 ≤ ⟨∇f (x) − ∇f (y), x − y⟩. (21.4) In particular, f is β-smooth. Proof. The ﬁrst inequality follows from the convexity assumption. For the second inequality, represent f (x) − f (y) as an integral: f (x) − f (y) = ∫ 1 0 ⟨∇f (y + t(x − y)), x − y⟩ dt. We can then write f (x) − f (y) − ⟨∇f (y), x − y⟩ = ∫ 1 0 ⟨∇f (y + t(x − y)), x − y⟩ − ⟨∇f (y), x − y⟩ dt ≤ ∫ 1 0 ∥∇f (y + t(x − y)) − ∇f (y)∥ · ∥x − y∥ dt ≤ β ∫ 1 0 t∥x − y∥ 2 dt = β 2 ∥x − y∥ 2, where the ﬁrst inequality follows from applying Cauchy-Schwartz, and the second from the assumption of β-smoothness. For the second claim, assume that f satisﬁes the bound (21.3). For any x, y, z ∈ Rd we have f (x) − f (y) = f (x) − f (z) + f (z) − f (y) ≤ ⟨∇f (x), x − z⟩ + ⟨∇f (y), z − y⟩ + β 2 ∥z − y∥ 2, where we used the convexity of f to bound f (x) − f (z) and the β-smoothness to bound f (z) − f (y). If we now set z = y − (1/β)(∇f (y) − ∇f (x)) and simplify the resulting expression, we get f (x) − f (y) ≤ ⟨∇f (x), x − y⟩ − 1 2β ∥∇f (x) − ∇f (y)∥2. Adding this expression to the same one with the roles of x and y exchanged, we get 0 ≤ (∇f (x) − ⟨∇f (y)), x − y⟩ − 1 β ∥∇f (x) − ∇f (y)∥2. The fact that this implies β-smoothness of f follows from the Cauchy-Schwartz inequality. We can, and will, use (21.3) and (21.4) as alternative deﬁnitions of β-smoothness. Theorem 21.3. Let f ∈ C1(Rd) be a function that is β-smooth and convex. Then for any x0 ∈ Rd, the iterates {xk} generated by gradient descent with constant step length 1/β satisfy f (x k) − f ∗ ≤ 2β k ∥x 0 − x ∗∥ 2, where f ∗ = f (x∗) and x∗ is a minimizer of f . Proof. Observe ﬁrst that ∥x k+1 − x ∗∥ 2 = ∥xk − (1/β)∇f (x k) − x∗∥2 = ∥x k − x ∗∥ 2 − 2 β ⟨∇f (x k), x k − x∗⟩ + 1 β2 ∥∇f (xk)∥2 (21.4) ≤ ∥xk − x ∗∥2 − 1 β2 ∥∇f (xk)∥ 2 ≤ ∥x k − x ∗∥ 2, where in addition to (21.4) we used the fact that ∇f (x∗) = 0. In particular, since ∥xk − x∗∥ is non-increasing, we get from (21.1) that f (x k) − f ∗ ≤ ∥∇f (x k)∥ · ∥x0 − x∗∥. (21.5) Using (21.3) with y = xk and x = xk − (1/β)∇f (xk) = xk+1, we get f (x k+1) − f (x k) ≤ − 1 2β ∥∇f (xk)∥2. Set ∆k = f (xk) − f ∗, so that f (xk+1) − f (xk) = ∆k+1 − ∆k. Then ∆k+1 − ∆k ≤ − 1 2β ∥∇f (xk)∥ 2 (21.5) ≤ − 1 2β∥x0 − x∗∥2 ∆ 2 k, (21.6) where we used (21.5) to lower-bound ∥∇f (xk)∥ in the second inequality. In particular, we see that ∆k+1 ≤ ∆k. We can rearrange the inequality (21.6) to ∆k+1 + ∆2 k 2β∥x0 − x∗∥ ≤ ∆k ⇒ 1 ∆k + 1 2β∥x0 − x∗∥ ∆k ∆k+1 ≤ 1 ∆k+1 ⇒ 1 ∆k + 1 2β∥x0 − x∗∥ ≤ 1 ∆k+1 , where for the ﬁrst implication we divided both sides by ∆k∆k+1, and for the second implication we used that ∆k/∆k+1 ≥ 1. Applying the same bound recursively to 1/∆k, we get 1 ∆k+1 ≥ k + 1 2β∥x0 − x∗∥ + 1 ∆0 ≥ k + 1 2β∥x0 − x∗∥ . Taking the inverse, and shifting the index from k + 1 to k, we get f (xk) − f ∗ = ∆k ≤ 2β∥x0 − x∗∥ k , as claimed. We can get even better convergence when assuming strong convexity. Deﬁnition 21.4. A function f ∈ C1(Rd) is called α-strongly convex for some α > 0 if for every x, y ∈ Rd, f (y) + ⟨∇f (y), x − y⟩ + α 2 ∥x − y∥2 ≤ f (x). If α = 0 this is just the derivative characterization of convexity. Note that a function f is α-strongly convex if and only if the function f (x) − α∥x∥2/2 is convex. Remark 21.5. The difference Df (x, y) := f (x) − f (y) − ⟨∇f (y), x − y⟩ is called the Bregman divergence associated to f . To say that a function f ∈ C1(Rd) is α-strongly convex and β-smooth is to say that for any x, y ∈ Rd, α 2 ∥x − y∥ 2 ≤ Df (x, y) ≤ β 2 ∥x − y∥ 2. This means that we can, locally, upper and lower bound the function by quadratic functions. In particular, β ≥ α. Theorem 21.6. Let f ∈ C1(Rd) be a function that is α-strongly convex and β-smooth. Then for any x0 ∈ Rd, the iterates of gradient descent with constant step length 2/(α + β) satisfy ∥x k+1 − x ∗∥ ≤ ( κ − 1 κ + 1 ) ∥x k − x∗∥, where κ = β/α. Example 21.7. Assume that α = β (and therefore κ = 1) in Theorem 21.6. Then f (x) = α 2 ∥x∥ 2. The gradient is ∇f (x) = αx. Starting with x0 ∈ Rd, gradient descent with step length 2/(α + β) = 1/α gives x1 = x 0 − x 0 = 0 = x ∗, so that this converges in a single iteration. Example 21.8. Let f (x) = 1 2 ∥Ax − b∥2 for A ∈ Rm×n with m ≥ n, and assume A has full rank. The difference between the function and its approximation is 1 2 (∥Ax − b∥2 − ∥Ay − b∥2) − (Ay − b) T A(x − y) = 1 2 ∥A(x − y)∥2. (21.7) The largest and smallest singular values of the matrix A are deﬁned as σ1(A) = max ∥x∥=1 ∥Ax∥, σn(A) = min ∥x∥=1 ∥Ax∥. The term (21.7) is therefore bounded from above and below by the squares of the largest singular value and by the smallest singular value of A: σ2 1(A)∥x − y∥ 2 ≥ ∥A(x − y)∥2 ≥ σ2 n(A)∥x − y∥ 2. A well-known characterization of the condition number of a matrix is κ(A) = σ1(A)/σn(A), and from this we recover the convergence result from Lecture 20. The proof of Theorem 21.6 relies on the following auxiliary result. Lemma 21.9. Let f be α-strongly convex and β smooth. Then for any x, y ∈ Rd, ⟨∇f (x) − ∇f (y), x − y⟩ ≥ αβ α + β ∥x − y∥ 2 + 1 α + β ∥∇f (x) − ∇f (y)∥2. Proof. Set ϕ(x) := f (x) − α 2 ∥x∥2. Since f is α-strongly convex, ϕ(x) is convex. Moreover, ∇ϕ(x) = ∇f (x) − αx. We therefore get ϕ(x) − ϕ(y) − ⟨∇ϕ(y), x − y⟩ ≤ f (x) − f (y) − ⟨∇f (y), x − y⟩ − α 2 (∥x∥ 2 − ∥y∥ 2) + α⟨y, x − y⟩ Lemma 21.2 ≤ β 2 ∥x − y∥ 2 − α 2 (∥x∥ 2 + ∥y∥ 2 − 2⟨y, x⟩) = β − α 2 ∥x − y∥2. (21.8) From Lemma 21.2 it follows that ϕ is β-smooth and satisﬁes the inequality ⟨∇ϕ(x) − ∇ϕ(y), x − y⟩ ≥ 1 β − α ∥∇ϕ(x) − ∇ϕ(y)∥2. Replacing ϕ in this expression, we get ⟨∇f (x) − ∇f (y) − αx + αy, x − y⟩ ≥ 1 β − α ∥∇f (x) − ∇f (y) − αx + αy∥2. The left-hand side of this inequality gives ⟨∇f (x) − ∇f (y) − αx + αy, x − y⟩ = ⟨∇f (x) − ∇f (y), x − y⟩ − α∥x − y∥2. (21.9) The right-hand side, on the other hand, gives 1 β − α ∥∇f (x) − ∇f (y) − αx + αy∥ 2 = 1 β − α (∥∇f (x) − ∇f (y)∥2 + α2∥x − y∥2 − 2α⟨∇f (x) − ∇f (y), x − y⟩). (21.10) Collecting the terms in (27.2) and (21.10) involving ⟨∇f (x) − ∇f (y), x − y⟩ on the left, and the terms involving ∥∇f (x) − ∇f (y)∥2 and ∥x − y∥2 on the right, we get α + β β − α ⟨∇f (x) − ∇f (y), x − y⟩ ≥ αβ β − α ∥x − y∥ 2 + 1 β − α ∥∇f (x) − ∇f (y)∥ 2. Multiplying this expression with (β − α)/(α + β) gives the desired inequality. Proof of Theorem 21.6. Set η := 2/(α + β). Since ∇f (x∗) = 0, we will omit this term whenever it appears in the following (so that we can think of ∇f (xk) − ∇f (x∗) whenever we see ∇f (xk)). ∥x k+1 − x ∗∥ 2 = ∥xk − η∇f (xk) − x∗∥2 = ∥(xk − x ∗) − η∇f (x k)∥2 = ∥xk − x ∗∥ 2 − 2η⟨∇f (xk), xk − x ∗⟩ + η2∥∇f (x k)∥2 ≤ ( β − α β + α )2 ∥x k − x∗∥2, where in the inequality we used Lemma 21.9 to bound the term ⟨∇f (xk), xk − x∗⟩, and simpliﬁed the resulting expression. The claim now follows by dividing the numerator and the denominator by α. Using the bound ( κ − 1 κ + 1 )2 = ( 1 − κ − 2 κ + 1 )2 ≤ e −4/(κ+1), and the inequality f (xk) − f ∗ ≤ (β/2)∥xk − x∗∥2 from the β-smoothness assumption, we get the convergence bound f (x k) − f ∗ ≤ β∥x0 − x∗∥2 2 · e − 4 κ+1 k, which is a considerable improvement over the 1/k convergence bound when only assuming β-smoothness. In particular, the number of iterations to reach accuracy ϵ is of order O(log(1/ϵ)). Notes The convergence of gradient descent under various smoothness assumptions is a classic theme in convex optimization. The presentation in this chapter is based on [4]. A standard reference for many of the tools used in the analysis of gradient descent (and a wealth of additional information) [19]. 22 Extensions of Gradient Descent We have seen that for a β-smooth convex function f ∈ C1(Rd), the sequence of iterates {xk}k≥0 generated by gradient descent with step length 1/β satisﬁes f (x k+1) − f (x ∗) = O(1/k), where x∗ is a minimizer of f . This implies that we need a number of iterations of order O(1/ϵ) to reach accuracy ϵ. If in addition the function f is α-strongly convex, then we get linear convergence with ratio (κ − 1)/(κ + 1), where κ = β/α. For the convergence of the function value, this implies f (x k+1) − f (x ∗) = O(e−4k/(κ+1)) shows that only O(log(1/ϵ)) iterations are needed to reach accuracy ϵ. In this lecture we will have a look at two extensions of gradient descent. The ﬁrst is an accelerated version, while the second extension covers common situations in which the function to be minimized is not differentiable. Acceleration Accelerated gradient descent, proposed by Y. Nesterov, begins with initial values y0 = x0 = x−1 ∈ Rd and the proceeds as follows for k ≥ 0: yk = xk + k − 1 k + 2 (x k − x k−1) x k+1 = yk − η∇f (yk) The method can be interpreted as carrying over some momentum from previous iterates: instead of only taking into account the current iterate, the gradient step is based on a combination of the current and the previous step. This method has favourable convergence properties. Theorem 22.1. Let f ∈ C1(Rd) be convex and β-smooth. Then accelerated gradient descent with step length 1/β converges to a minimizer x∗ of f with rate f (x k) − f (x∗) ≤ 2∥x0 − x∗∥2 β(k + 1)2 . There are lower bounds that show that this rate is optimal for gradient methods. 113 Proximal gradients The objective functions arising in machine learning often have the form f (x) = g(x) + h(x), where both g and h are convex, but only g is differentiable. The term h(x) is typically a regularization term. Example 22.2. Consider the function f (x) = 1 2 ∥Ax − b∥ 2 + λ∥x∥1, where ∥x∥1 = ∑d i=1 |xi| and λ > 0. The problem of minimizing this function is often referred to as the LASSO problem in statistics. The purpose of the regularization term is to encourage sparse solutions. These are solutions that have only few entries that are signiﬁcantly larger than 0. Deﬁnition 22.3. Let f be a convex function. The subdifferential of f at x is the set ∂f (x) = {g ∈ Rd : ∀y ∈ Rd, f (x) + gT (y − x) ≤ f (y)}. The elements of ∂f (x) are called subgradients. If f is differentiable at x, then there exists only one subgradient, which coincides with the gradient. Example 22.4. Consider the function f (x) = |x|. The differential is then ∂f (x) =  | | 1 x > 0 [−1, 1] x = 0 −1 x < 0 Example 22.5. The subdifferential is additive, in the sence that if A and B are matrices and f (x) = g(Ax) + h(Bx), then ∂f (x 0) = AT ∂g(Ax) + BT ∂h(Bx). A special case is the 1-norm. Here, we can write ∥x∥1 = d∑ i=1 |Πi(x)|, where Πi(x) = xi is the projection on the i-th coordinate. It follows that the subdifferential of the 1-norm can be described as ∂∥x∥1 = {z : zi = sign(xi) if xi ̸= 0, zj ∈ [−1, 1] if xj = 0}. Using the subdifferential, we have the following optimality condition. Theorem 22.6. Let f : Rd → R be a convex function. Then x is a global minimizer of f if and only if 0 ∈ ∂f (x). For composite functions f (x) = g(x) + h(x) with g ∈ C1(Rd), this means that −∇g(x) ∈ ∂h(x). There are different possible strategies for generalizing gradient descent to make it work with non- smooth functions. One would be to simply pick a subgradient at each step and follow that direction. Note that this may not be a descent direction. One common strategy for composite functions is to perform a gradient descent step based on the smooth function g, and then project onto the subgradient of h. Projection onto the subgradient is done via the proximal operator proxh(x) = arg min y 1 2 ∥x − y∥2 + h(y). Note that x∗ := proxh(x) satisﬁes x∗ − x ∈ ∂h(x∗). The proximal gradient method for minimizing a function of the form f (x) = g(x) + h(x) starts with a vector x0, and then for k ≥ 0 proceeds by computing x k+1 = proxηh (xk − η∇g(x k)) . Example 22.7. Recall the image inpainting problem from Lecture 13. An image can be viewed as an m × n matrix U , with each entry uij corresponding to a light intensity (for greyscale images), or a colour vector, represented by a triple of red, green and blue intensities (usually with values between 0 and 255 each). For simplicity the following discussion assumes a greyscale image. For computational purposes, the matrix of an image is often viewed as an mn-dimensional vector u, with the columns of the matrix stacked on top of each other. In the image inpainting problem, one aims to learn the true value of missing or corrupted entries of an image. There are different approaches to this problem. A conceptually simple approach is to replace the image with the closest image among a set of images satisfying typical properties. But what are typical properties of a typical image? Some properties that come to mind are: • Images tend to have large homogeneous areas in which the colour doesn’t change much; • Images have approximately low rank, when interpreted as matrices. Total variation image analysis takes advantage of the ﬁrst property. The total variation or TV-norm is the sum of the norm of the horizontal and vertical differences, ∥U ∥TV = m∑ i=1 n∑ j=1 √ (ui+1,j − ui,j)2 + (ui,j+1 − ui,j)2, where we set entries with out-of-bounds indices to 0. Now let U be an image with entries uij, and let Ω ⊂ [m] × [n] = {(i, j) | 1 ≤ i ≤ m, 1 ≤ j ≤ n} be the set of indices where the original image and the corrupted image coincide (all the other entries are missing). One could attempt to ﬁnd the image with the smallest TV-norm that coincides with the known pixels uij for (i, j) ∈ Ω. This is an optimization problem of the form minimize ∥X∥TV subject to xij = uij for (i, j) ∈ Ω. Alternatively (see Exercise 8.3), one can solve a regularized problem minimize ∥AX − UΩ∥ 2 + λ∥X∥TV, where A represents the linear map projects X onto the entries indexed by Ω. This problem can be solved using proximal gradient methods. 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Figure 22.1: Soft thresholding Even though it seems that a proximal gradient method would require solving an optimization problem within an optimization problem, closed form expressions are known in many cases. Example 22.8. Consider the function h(|x|) = λ|x| for some λ > 0. Then proxh(x) = Tλ(x) :=  | | x − λ x ≥ λ 0 x ∈ [−λ, λ] −x + λ x ≤ −λ This is known as soft thresholding (see Figure 22.1). If a function h has the form h(x) = d∑ i=1 hi(xi), then the proximal mapping associated to h has the form proxh(x) = (proxh1(x1), . . . , proxhd(xd)) T . It follows that if h(x) = λ∥x∥1, then we can apply the proximal operator by applying the soft thresholding operator Tλ to each coordinate of x. For the proximal gradient method it is possible to obtain similar convergence results as for gradient descent. Notes Accelerated gradient descent goes back to Nesterov’s work [20]. A more in depth analysis can be found in [19] and [4]. An interesting interpretation of accelerated gradient descent in terms of differential equations is given in [26]. The proximal operator is discussed in detail in Chapter 6 of [1]. 23 Stochastic Gradient Descent In this lecture we introduce Stochastic Gradient Descent (SGD), a probabilistic version of gradient descent that has been around since the 1950s, and that has become popular in the context of data science and machine learning. To motivate the algorithm, consider a set of functions H = {hw : w ∈ Rd}, where each such function depends on d parameters. Also consider a smooth loss functions L, or a smooth approximation of a loss function. Given samples {(xi, yi}n i=1 ⊂ X × Y, deﬁne the functions fi(w) = L(hw(xi), yi), i ∈ {1, . . . , n}. The problem of ﬁnding functions that minimize the empirical risk is minimize w∈Rd 1 n n∑ i=1 fi(w). The fi are often assumed to be convex and smooth. In addition one often considers a regularization term R(w). In what follows, we abstract from the machine learning context and consider purely the associated optimization problem. Hence, as usual when dealing only with optimization problems, we switch notation and denote the variables to be optimized over by x. Stochastic Gradient Descent We consider an objective function of the form f (x) = 1 n n∑ i=1 fi(x). (23.1) In what follows we assume that the functions fi are convex and differentiable. If n is large, then computing the gradient can be very expensive. However, and considering the machine learning context, where f (x) is an estimator of the generalization risk Eξ[fξ(x)] of a family of functions fξ parametrized by a random vector ξ, we can shift the focus to ﬁnding an unbiased estimator of the gradient. Quite trivially, choosing an index j uniformly at random and computing the gradient of fj(x) gives such an unbiased estimator by deﬁnition: EU [fU (x)] = 1 n n∑ i=1 fi(x), 117 where P{U = j} = 1/n for j ∈ [n] = {1, . . . , n}. The Stochastic Gradient Descent (SGD) algorithm proceeds as follows. Begin with x0 ∈ Rd. At each step k, compute an unbiased estimator gk of the gradient at xk: E[gk | x k] = ∇f (x k). Next, take a step in direction gk: x k+1 = x k − ηkgk, where ηk is a step length (or learning rate in machine learning jargon). While there are many variants of stochastic gradient descent, we consider the simplest version in which gk is chosen by picking one of the gradients ∇fi(x) uniformly at random, and we refer to this as SGD with uniform sampling. A commonly used generalization is mini-batch sampling, where one chooses a small set of indices I ⊂ {1, . . . , n} at random, instead of only one. We also restrict to the smooth setting without a regularization term; in the non-smooth setting one would apply a proximal operator. Since SGD involves random choices, convergence results are stated in terms of the expected value. Let U be a random variable with distribution P{U = i} = 1/n for i ∈ [n]. Then EU [∇fU (x)] = 1 n n∑ i=1 ∇fi(x) = ∇f (x), so that ∇fU is an unbiased estimator of ∇f . Assuming that f has a unique minimizer x∗, we deﬁne the empirical variance at the optimal point x∗ as σ2 = EU [∥∇fU (x ∗)∥2] = 1 n n∑ i=1 ∥∇fi(x ∗)∥ 2. (23.2) We can now state the convergence result for stochastic gradient descent. Theorem 23.1. Assume the function f is α-strongly convex and that the fi are convex and β-smooth for i ∈ [n] and 4β > α. Assume f has a unique minimizer x∗ and deﬁne the variance as in (23.2). Then for any staring point x0, the sequence of iterates {xk} generated by SGD with uniform sampling and step length η = 1/(2β) satisﬁes E[∥xk − x ∗∥2] ≤ ( 1 − α 4β )k ∥x 0 − x ∗∥2 + 2σ2 αβ . Proof. As in the analysis of gradient descent, we get ∥x k+1 − x ∗∥ 2 = ∥xk − x ∗∥ 2 − 2η⟨∇fU (x k), x k − x∗⟩ + η2∥∇fU (xk)∥ 2. Taking the expectation conditional on xk, we get E[∥x k+1 − x ∗∥ 2 | xk] = ∥x k − x ∗∥ 2 − 2η⟨∇f (xk), xk − x ∗⟩ + η2 E[∥∇fU (x k)∥2 | x k], (23.3) where we used the fact that the expectation satisﬁes E[∇fU (x)] = ∇f (x). For the last term we use the bound E[∥∇fU (x k)∥2 | xk] = E[∥∇fU (x k) − ∇fU (x ∗) + ∇fU (x ∗)∥ 2] ≤ 2 E[∥∇fU (x k) − ∇fU (x ∗)∥ 2] + 2 E[∥∇fU (x ∗)∥2] = 2 E[∥∇fU (x k) − ∇fU (x ∗)∥ 2] + 2σ2. Using the characterization of β smoothness from Lecture 21, namely 1 β ∥∇fi(x) − ∇fi(y)∥2 ≤ ⟨∇fi(x) − ∇fi(y), x − y⟩, (23.4) we get that E[∥∇fU (xk) − ∇fU (x ∗)∥ 2 | xk] = 1 n n∑ i=1 ∥∇fi(xk) − ∇fi(x ∗)∥ 2 (23.4) ≤ β n n∑ i=1⟨∇fi(x k) − ∇fi(x∗), x k − x∗⟩ = β n n∑ i=1⟨∇fi(xk), xk − x ∗⟩ − β n n∑ i=1⟨∇fi(x∗), x k − x∗⟩ = β⟨∇f (x), x k − x ∗⟩, where we used that ∇f (x∗) = 0 for the last equality. Hence, we get the bound E[∥∇fU (x k)∥2 | xk] ≤ 2β⟨∇f (x), x k − x∗⟩ + 2σ2. Plugging this into (23.3), we get E[∥x k+1 − x ∗∥ 2 | xk] ≤ ∥x k − x∗∥ 2 − (2η − 2η2β)⟨∇f (x k), x k − x∗⟩ + 2η2σ2. Using the α-strong convexity, we get the bound ⟨∇f (xk), xk − x ∗⟩ ≥ f (x k) − f (x∗) + α 2 ∥xk − x ∗∥ 2 ≥ α 2 ∥xk − x ∗∥2, since f (xk) ≥ f (x∗), so that we get E[∥xk+1 − x∗∥2 | x k] ≤ (1 − ηα(1 − ηβ))∥x k − x∗∥ 2 + 2η2σ2. With step length η = 1/(2β), and taking the expected value over all previous iterates, we get E[∥x k+1 − x ∗∥ 2] ≤ ( 1 − α 4β ) E[∥x k − x∗∥2] + σ2 2β2 . Applying this bound recursively (and moving the index down), we get E[∥x k − x ∗∥ 2] ≤ (1 − α 4β )k ∥x 0 − x ∗∥ 2 + σ2 2β2 k−1∑ j=0 ( 1 − α 4β )j ≤ (1 − α 4β )k ∥x 0 − x ∗∥ 2 + 2σ2 αβ , where we used that 4β > α. Example 23.2. Consider the problem of logistic regression, where the aim is to minimize the objective function f (w) = n∑ i=1 (log (1 + exp(xT i w) ) − yixT i w) over a vector of weights w. This problem arises in the context of a binary classiﬁcation problem with data pairs (xi, yi) and yi ∈ {0, 1}. Setting p := exT w 1 + exT w , the resulting classiﬁer is the function h(w) = { 1 p > 1/2 0 p ≤ 1/2 The function f is convex (Exercise 7.6(a)), and the gradient is ∇f (x) = −X T (y − p(w)), where X ∈ Rn×d is the matrix with the xT i as rows, y = (y1, . . . , yn)T , and p(w) ∈ Rn has coordinates pi(w) = exp(xT i w) 1 + exp(xT i w) , 1 ≤ i ≤ n. We can apply different versions of gradient descent to this problem. Figure 1 shows the typical paths of gradient descent and of stochastic gradient descent for a problem with 100 data points. Note that using a naive approach to computing the gradient, one would need to compute 100 gradients at each step. Stochastic gradient descent, on the other hand, fails to converge due to the variance of the gradient estimator (see Figure 2). 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Figure 23.1: The path of gradient descent and of stochastic gradient descent with constant step length. Extensions The version of SGD described here is the most basic one. There are many possible extensions to the methods. The include considering different sampling schemes, including mini-batching and importance sampling. These sampling strategies have the effect of reducing the variance σ2. In addition, improve- ments can be made in the step length selection and when dealing with non-smooth function, where the proximal operator comes into play. 0 5 10 15 20 25 30 35 40 0.0 0.5 1.0 1.5 2.0 0 50 100 150 200 250 300 350 400 0.5 1.0 1.5 2.0 Figure 23.2: Convergence of gradient descent and SGD. Notes The origins of stochastic gradient descent go back to the work of Robbins and Monro in 1951 [21]. The algorithm has been rediscovered many times, and gained popularity due to its effectiveness in training deep neural networks on large data sets, where gradient computations are very expensive. Despite its simplicity, a systematic and rigorous analysis has not been available until recently. The presentation in this chapter is based loosely on the papers [11] and [2]. A more general and systematic analysis of SGD that includes non-smooth objectives is given in [10]. These works also discuss general sampling techniques, not just uniform sampling. Part III Deep Learning 123 24 Neural Networks Neural Networks are a powerful class of functions with a wide range of applications in machine learning and data science. Originally introduced as simpliﬁed models of neurons in the brain, nowadays the biological motivation plays a less prominent role. Instead, the popularity of neural networks owes to their ability to combine generality with computational tractability: while neural networks can approximate most reasonable functions to arbitrary accuracy, their structure is still simple enough so that they can be trained efﬁciently by gradient descent. Connectivity and Activation We begin by considering the problem of binary classiﬁcation using a linear function. Given a vector of weights w ∈ Rd and a bias term b ∈ R, deﬁne the classiﬁer hw,b(x) = 1{wT x + b} = { 1 wT x + b > 0 0 wT x + b ≤ 0 We already encountered this problem when studying linear support vector machines. Visually, we can represent this classiﬁer by a node that takes d inputs (x1, . . . , xd), and outputs 0 or 1: Such a unit is called a perceptron. One interpretation is that the node represents a neuron that ﬁres if a certain linear combination of inputs, wT x, exceeds a threshold −b. It is sometimes useful to approximate the indicator with a smooth function, and a convenient candidate is the sigmoid σ(x) = 1 1 + e−x . We can then replace the function hw,b with the smooth function g(x) = σ(wT x + b). A convenient property of the sigmoid function is that the derivative has the form σ′(x) = σ(x)(1 − σ(x)), 125 so that the gradient of g can be computed as ∇g(x) = σ(wT x + b)(1 − σ(wT x + b)) · w. Other activation functions that are commonly used are the hyperbolic tangent, tanh(x), and the rectiﬁable linear unit (ReLU), max{x, 0}. Figure 24.1 illustrates these different activations functions. 10 5 0 5 10 0.0 0.2 0.4 0.6 0.8 1.0 Sigmoid 10 5 0 5 10 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 tanh 10 5 0 5 10 0 2 4 6 8 10 ReLU Figure 24.1: Activation functions A feedforward neural network arises by combining various perceptrons by feeding the outputs of a series of perceptrons into a new perceptrons, see Figure 25.5. Figure 24.2: A fully connected neural network. We interpret a neural network as consisting of different layers. To the k-th layer we associated a linear map W k : Rdk−1 → Rdk and a bias vector bk ∈ Rdk . One then applies the activation function componentwise, to obtain a map σ(W kx + bk). The ﬁrst layer is the input layer, while the last layer is the output layer. Hence, a neural network with ℓ layers is a function of the form F ℓ(x), where the F k are recursively deﬁned as F 1(x) = σ(W 1x + b 1) F k+1(x) = σ(W (k+1)F k(x) + bk+1), and W k ∈ Rdi×dk−1, bk ∈ Rdk for 1 ≤ k ≤ ℓ (with d0 = d). The layers between the input and output layer are called the hidden layers. A neural network is therefore just a parametrized function that depends on a possibly large number of parameters. If we ﬁx the architecture, that is, the number of layers ℓ and the number of nodes d = (d0, d1, . . . , dℓ), where di represents the number of nodes in each layer, we get a a hypothesis class Hd = {F : Rd → Rdℓ : F is a NN with format d}. We can use neural networks for binary classiﬁcation tasks, for example, by setting dℓ = 1 (only one output layer), and declaring an output to be of class 1 if F (x) > 1/2 and 0 otherwise. Alternatively, we can have two outputs and classify according to whether the ﬁrst output is larger than the second. We can train the neural network on data (xi, yi), i ∈ {1, . . . , n}, using our favourite loss function. Neural networks are particularly attractive for two reasons: 1. The class of neural networks is rich enough to capture almost all functions of interest (see Lecture 25). 2. The structure is simple enough to train using gradient descent, by a computational implementation of the chain rule known as backpropagation. We discuss the computational aspect, backpropagation, ﬁrst. Backpropagation Denote by W and b the concatenation of all the weight matrices and bias vectors. We denote by wk ij the (i, j)-th entry of the k-th matrix, and by bk i the i-th entry of the k-th bias vector. Given n data points {xi}n i=1 with corresponding outputs {yi}n i=1 and a smooth loss function L, the task is to minimize the function f (W , b) = 1 n n∑ i=1 L(F ℓ(xi), yi). Gradiend descent, or stochastic gradient descent, requires evaluating the gradient of a function of the form fi(W , b) = L(F ℓ(xi), yi). We will describe a method for computing the gradient of such a function efﬁciently. In what follows, set x = xi and y = yi. Also write a0 = x, and for k ∈ {1, . . . , ℓ}, zk = W ka k−1 + b k, a k = σ(zk). (24.1) In particular, aℓ = F ℓ(xi) is the output of the neural network on input x. Moreover, set C = C(W , b) = L(aℓ, y) for the loss function. For every layer k and coordinate j ∈ {1, . . . , dk}, deﬁne the sensitivities δk j := ∂C ∂zk j , where zk j is the j-th coordinate of zk. Thus δk j measures the sensitivity of the loss function to the input at the j-th node of the k-th layer. Denote by δk ∈ Rdk the vector of δk j for j ∈ {1, . . . , dk}. The partial derivatives of C can be computed in terms of these quantities. In what follows, we denote by x ◦ y the componentwise product, that is, the vector with entries xiyi. Proposition 24.1. For a neural network with ℓ layers and k ∈ {1, . . . , ℓ}, we have ∂C ∂wk ij = δk i ak−1 j , ∂C ∂bk i = δk i (24.2) for i, j ∈ {1, . . . , dk}. Moreover, the sensitivities δk i can be computed as follows: δℓ = σ′(zℓ) ◦ ∇aℓL(a ℓ, y), δk = σ′(zk) ◦ (W k+1)T δk+1 (24.3) for k ∈ {1, . . . , ℓ − 1}. Proof. We begin by showing (24.2). For δℓ, note that by the chain rule, we have δℓ i = ∂L(aℓ, y) ∂zk i = dℓ∑ j=1 ∂L(aℓ, y) ∂aℓ j ∂aℓ j ∂zk i = ∂L(ak, y) ∂ak i · σ′(zℓ i ). For k < ℓ, we compute δk i in terms of the δk+1 j as follows: δk i = ∂C ∂zk i = dk+1∑ j=1 ∂C ∂zk+1 j ∂zk+1 j ∂zk i = dk+1∑ j=1 δk+1 j · ∂zk+1 j ∂zk i . For the summands in the last expression we use zk+1 j = dk∑ s=1 wk+1 js σ(zk s ) + b k+1 j , so that the derivatives evaluate to ∂zk+1 j ∂zk i = wk+1 ji · σ′(zk i ). Putting everything together, we arrive at δk i = dk+1∑ j=1 δk+1 j · wk+1 ji · σ′(zk i ) = σ′(zk i ) · ((W k+1) T σk+1) i . The expressions for the partial derivatives of C with respect to the weights W and the bias b are computed in a straight-forward way using the chain rule. More precisely, at the k-th layer write zk i = dk−1∑ j=1 wk ija k−1 j + b k i . The claimed expressions for the derivatives then follow by applying the chain rule, ∂C ∂wk ij = ∂C ∂zk i ∂zk i ∂wk ij = δk i · a k−1 j , and similarly for the derivative with respect to bk i . For the common quadratic loss function L(a ℓ, y) = 1 2 ∥a ℓ − y∥2, we get ∇aℓL(a ℓ, y) = a ℓ − y, which can be computed easily from the function value F ℓ(x) and y. Other differentiable loss functions may lead to different terms. Given initial weights W and bias terms b, we can compute the values ak and zk using a forward pass, that is, by applying (24.1) recursively. We can then compute the sensitivities δk j using (24.3), and the partial derivatives of the loss function using (24.2), starting at layer ℓ. This way of computing the gradients is called backpropagation. We note that choosing the sigmoid δ as activation function, the computation of the derivative σ′(x) is easy. The whole process of computing the gradient of a neural network thus reduces to a simple sequence of matrix-vector product operations and sigmoids. Example 24.2. Consider the following setting with n = 10 points. We train a neural network with four layers and dimensions d = (d0, d1, d2, d3) = (2, 2, 3, 2) using stochastic gradient descent. The neural network outputs a vector in R2 and classiﬁes an input point according to whether the ﬁrst coordinate is greater or smaller than the second coordinate. Figure 24.3 shows the decision boundary by the neural network based on 10 training points, and the display on the right shows the error per iteration of stochastic gradient descent. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0 20000 40000 60000 80000 100000 0.00 0.05 0.10 0.15 0.20 0.25 0.30Figure 24.3: Training a neural network for classiﬁcation and the error of stochastic gradient descent. Notes The study of neural networks has its origin in pioneering work by McCulloch and Pitts in the 1940s. Another seminal work in the history of artiﬁcial neural networks is the work by Rosenblatt on the Perceptron [22]. The idea of backpropagation was explicitly named in [23] and is closely related to automatic differentiation. Automatic differentiation has been discovered independently many times, and an interesting overview is given here: [12]. The content of this lecture is based on the excellent tutorial [13]. A comprehensive modern treatment of the subject can be found in the book [8]. 25 Universal Approximation In the last lecture we saw that neural networks can be trained efﬁciently using backpropagation. We now turn to studying the expressive power of neural networks. Approximation in one dimension The idea of approximation is best illustrated in one dimension. Given a function f ∈ C([0, 1]), is it possible to ﬁnd, for every ϵ > 0, a neural network F that approximates f to accuracy ϵ, in the sense that ∥f − F ∥∞ = supx∈[0,1] |f (x) − F (x)| < ϵ? We begin by noting that any continuous function on [0, 1] can be approximated by step functions, as shown in Figure 25.1 0.0 0.2 0.4 0.6 0.8 1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.8 1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Figure 25.1: Approximation of a continuous function by a step function. We next observe that any step function can be approximated by a combination of sigmoid functions of the form σ(wx + b) = 1 1 + e−(wx+b) . It is convenient to parametrize such as sigmoid in terms of the location where σ(wx + b) = 1/2, which is given by s = −b/w. Thus for a given weight w and location s, the function gw,s(x) = σ(w(x − s)) 131 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 w=10 w=20 w=100 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 s=0.2 s=0.4 s=0.6 Figure 25.2: Sigmoid approximating a step function at various location and with various weights. approximates the step function from 0 to 1 at location s. The approximation improves when increasing w, see Figure 25.2. We can now form linear combinations of such functions to approximate step functions (piecewise constant functions) with multiple steps, as shown in Figure 25.3. 0.0 0.2 0.4 0.6 0.8 1.0 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 w=10 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 w=100 Figure 25.3: The function 0.5gw,−0.167 + 0.3gw,0.33 − 0.5gw,0.67 with weights w = 10 and w = 100. We can therefore use the approximation of the step function by sigmoids and the approximation of an arbitrary continuous function on a closed interval by sigmoids to approximate any such function by sigmoids, see Figure 25.4. We note that there may be some ambiguities on how to approximate a function by step functions. For example, one could take the maximum value, minimum value, or the function value at the midpoint of a function on an interval as the value of the piecewise constant function on that interval. Also, when approximating the piecewise constant functions by sigmoids, one has to take of the approximation error at the left boundary, possibly by centering the ﬁrst sigmoids at a negative value. All these considerations are not essential, and in the end one arrives at a function of the form m∑ i=1 αiσi(wix + bi), (25.1) and such functions can approximate any continuous function on the unit interval to arbitrary accuracy. Such a function is essentially a neural network with one hidden layer, if we drop the sigmoid at the output layer. 0.0 0.2 0.4 0.6 0.8 1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.8 1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Figure 25.4: Approximation of a function by sigmoids with weights w = 100 and w = 1000. Figure 25.5: A neural network with one hidden layer. Universal Approximation Let I d = [0, 1]d ⊂ Rd denote the unit cube and let C(I d) be the set of continuous functions on I d with the norm ∥f ∥∞ = supx∈I d |f (x)|. Theorem 25.1. (Cybenko’s Universal Approximation Theorem) Let f ∈ C(I d). For any ϵ > 0 there exists a function g of the form g(x) = m∑ i=1 αiσ(wT i x + bi), with weights wi ∈ Rd and αi, bi ∈ R, such that ∥f − g∥∞ < ϵ The theorem states that the linear subspace of C(I d) spanned by functions of the form σ(wT x + b) is dense in C(I d). The proof is an immediate consequence of some standard results in Measure Theory and Functional Analysis, which we state here1. In what follows, if S ⊂ X is a subset of a normed space, we denote by S the closure of S in X, i.e., the set of all limit points of elements of S. Theorem 25.2. ((Consequence of ) Hahn-Banach) Let X be a normed vector space, S ⊂ X a linear subspace, and x0 ∈ X. Then x0 ∈ S if and only if for all linear functionals L on X, L(f ) = 0 for all f ∈ S implies f (x0) = 0. In the following theorem, a signed measure is deﬁned just like a measure, but without the non- negativity requirement. 1If you took Measure Theory or Functional Analysis and ever wondered “what is this good for”, here you go! Theorem 25.3. (Riesz Representation Theorem) Let L be a bounded linear functional on C(I d). Then there exists a signed measure µ on I d such that for every f ∈ C(I d), L(f ) = ∫ I d f dµ(x). In addition to the Hahn-Banach and Riesz Representation Theorem, we need to know that the sigmoid function σ(x) has the property that it is discriminatory. The proof relies on a Fourier Transform argument. Lemma 25.4. Let σ ∈ C(R) be a continuous function with values in [0, 1], such that limx→∞ σ(x) = 1 and limx→−∞ σ(x) = 0. If µ is a signed measure on C(I d), and if for every w ∈ Rd and b ∈ R we have ∫ I d σ(wT x + b) dµ(x) = 0, then µ = 0. Proof of Theorem 25.1. Let S be the set of functions of the form g(x) = m∑ i=1 αiσ(wT i x + bi) on I d. Clearly, S is a linear subspace of C(I d). We will show that S is dense in C(I d). Assume to the contrary that S is not dense, i.e., that the closure S of S is a proper linear subspace of C(I d). By the Hahn-Banach Theorem 25.2, there exists a non-zero linear functional L on C(I d) such that L|S = 0. By the Riesz Reprentation Theorem 25.3, there exists a regular Borel measure µ and a function h such that L(f ) = ∫ I d f (x) dµ(x) for all f ∈ C(I d). This means, however, that for any weights w and bias terms b we have ∫ I d σ(wT x + b) dµ(x) = 0, since σ(wT x + b) ∈ L. By Lemma 25.4, it follows that µ = 0 and hence L = 0, contradicting the fact that L ̸= 0 established earlier. Hence, S = C(I d). Note that even though the approximating neural network has only one hidden layer, there is no bound on the size of this layer: it could be arbitrary large. Intuitively, we would expected the complexity of the approximating network to depend on the complexity of the function that we would like to approximate. Notes The Universal Approximation Theorem was derived by Cybenko in [6]. Since then, the result has been generalized in several directions, among others also to different activation functions such as ReLU. An instructive visual overview of approximation in one dimension can be found in http: //neuralnetworksanddeeplearning.com/chap4.html. 26 Convolutional Neural Networks The neural networks considered so far are called fully connected, since each node in one layer is connected to each node in the following layer. A particularly useful and widespread architecture for image data is the class of Convolutional Neural Networks, or CNN, which will be discussed in this lecture. In practice, one often considers special types of connectivity. It is also common to use activation functions other than the sigmoid, one example being the Rectiﬁed Linear Unit (ReLU). In convolutional neural networks, many of the linear maps in each layer correspond to the mathematical operation of convolution. These are then usually combined with ReLU activation functions, pooling layers, and fully connected layers. Convolutions In the context of functions, the convolution of two real valued functions f and g is deﬁned as the function f ∗ g(y) = ∫ ∞ −∞ f (x) · g(y − x) dx. In a discrete setting, given a vector x ∈ Rd and a sequence g = (g−d+1, . . . , gd−1)T , the convolution is deﬁned as the vector y = (y1, . . . , yd)T , where yk = d∑ i=1 xi · gk−i, k ∈ {1, . . . , d} This deﬁnition also applies to inﬁnite sequences, but we will not use that here. In terms of matrices,    y1 y2... yd  |  =      g0 g−1 · · · g−d+1 g1 g0 · · · g−d+2 ... ... . . . ... gd−1 gd−2 · · · g0  | | |     x1 x2... xd  |  The sequence g is called a ﬁlter in signal processing. Typically, multiplying a d × d matrix with a d-dimensional vector requires d2 multiplications. If, however, the matrix has a special structure, then it is possible to perform this multiplication much faster, without having to explicitly represent the matrix as two-dimensional array in memory. 135 Example 26.1. The cyclic convolution corresponds to multiplication with a circulant matrix C =      c0 c1 · · · cd−1 cd−1 c0 · · · cd−2 ... ... . . . ... c1 c2 · · · c0  | | |  This is the same as the convolution with a sequence g as above, with ci = g−i and gi = gd−1 for i ∈ {1, . . . , d − 1}. A special case of a circulant matrix is the Discrete Fourier Transform (DFT), in which cjk = exp(2πi · jk/n). The DFT plays an important role in signal and image processing, and versions of it, such as the two- dimensional Discrete Cosine Transform, are the basis of the image compression standard JPEG. One of the most inﬂuential and wide-spread algorithms, the Fast Fourier Transform (FFT), computes a DFT with O(d log(d)) operations, and this is essentially optimal. FFT-based algorithms can also be applied to carry out cyclic convolutions, since circulants are diagonalized by DFT matrices. Example 26.2. In many applications, only a few of the entries of g will be non-zero. Perhaps the easiest and most illustrative example is the difference matrix D =        −1 1 0 · · · 0 0 −1 1 · · · 0 0 0 −1 · · · 0 ... ... ... . . . ... 0 0 0 · · · −1  | | | | |  . This matrix maps a vector x to the vector of differences xi+1 − xi. This transformation detects changes in the vector, and a vector that is constant over a large range of coordinates is mapped to a sparse vector. Again, computing y = Dx requires just d − 1 subtractions and no multiplications, and it is not necessary to store D as matrix in memory. Given a convolution G, one often only considers a subset of the rows, such as every second or every third row. If only every k-th row is considered, then we say that the ﬁlter has stride length k. If the support (the number of non-zero entries) of g is small, then one can interpret the convolution operation as taking the inner product of g with a particular section of x, and then moving this section, or “window”, by k entries if the stride length is k. Each such operation can be seen as extracting certain information from a part of x. Convolution Layers and Image Data In the context of colour image data, the vector x will not be seen as a single vector in some Rd, but as a tensor with format N × M × 3. Here, the image is considered to be a N × M matrix, with each entry corresponding to a pixel, and each pixel is represented by a 3-tuple consisting of the red, blue and green components. A convolution ﬁlter will therefore operate on a subset of this tensor, typically a n × m × 3 window. Typical parameters are N = M = 32 and n = m = 5. Each 5 × 5 × 3 block is mapped linearly to an entry of a 28 × 28 matrix in the same way (that is, applying the same ﬁlter). The ﬁlter applied is called a kernel and the resulting map is interpreted as       ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗       · · · ... . . .       ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗       · · · ... . . .       ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗       · · · ... . . . [∗] · · · ... . . . representing a feature. Speciﬁcally, instead of writing it as a vector g, we can write it as a matrix K, and then have, if we denote the input image by X, Y = X ∗ K, Yij = ∑ k,ℓ Xkℓ · Ki−k,j−ℓ. In a convolutional layer, various different kernels or convolutions are applied to the image. This can be seen as extracting several features from an image. Figure 26.1 shows and example from a seminal paper by Krizhevsky et. al. that classiﬁed the 1.2 million images from the ImageNet LSVRC-2010 database to record accuracy. Figure 26.1: Filters for a convolutional layer from an image classiﬁcation tasks, Krizhevsky et. al. Convolutional networks are inspired by biology, where the brain scans different parts of an image and extracts certain types of structure from local patches, and then combines the pieces. Softmax For a classiﬁcation problem with k classes, it is common to have k output and pick the class corresponding to the largest entry. A transformation that is often used on the output data is the softmax operation. Suppose the output consists of k entries v1, . . . , vk. Then one computes vector with entries − log ( evi ∑k j=1 evj ) . Example Convolutional layers are often combined with pooling layers. In a pooling layer, a few entries (typically a 4 × 4 submatrix) are combined into one entry, either my taking the maximum (max pooling) or by averaging. This operation reduces the size of the data. In addition, towards the end one often adds a fully connected layer. A prototypical example is the inﬂuential LeNet-5 architecture introduced by Yann LeCun and coauthors in 1998 and applied to digit recognition, see Figure 26.2. The following code shows an Figure 26.2: Architecture of LeNet-5 from LeCunn et. al. (1998) implementation of the LeNet-5 architecture in Python using TensorFlow and the Keras frontend. In [1]: (X_train, y_train), (X_test, y_test) = mnist.load_data() # Adjust format to get 60000 x 28 x 28 x 1 tensor for training # and 10000 x 28 x 28 x 1 tensor for testing X_train = X_train[..., np.newaxis].astype(’float32’) / 255 X_test = X_test[..., np.newaxis].astype(’float32’) / 255 # Change output data type to categorial / 10 classes y_train = keras.utils.to_categorical(y_train, 10) y_test = keras.utils.to_categorical(y_test, 10) # Build up CNN according to LeNet-5 model = Sequential() model.add(Conv2D(6, (5, 5), activation=’relu’, padding=’same’, input_shape=(28, 28, 1))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(16, (5, 5), activation=’relu’)) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(120, (5, 5), activation=’relu’)) model.add(Flatten()) model.add(Dense(84, activation=’relu’)) model.add(Dense(10, activation=’softmax’)) # Now the magic happens... model.compile(loss=’categorical_crossentropy’, optimizer=’sgd’, metrics=[’accuracy’]) model.fit(X_train, y_train, batch_size=128, epochs=20, validation_split=0.10) # Evaluate output output = model.evaluate(X_test, y_test) print(’Accuracy: {0:.2f}’.format(output[1])) Out [1]: 10000/10000 [==============================] - 3s 318us/step Accuracy: 0.9855 Notes There are various excellent sources and tutorials on convolutional neural networks. One example is Chapter 9 of [8] and Chapter 7 of the survey [13] which also contains a worked through example in MATLAB. Two seminal papers in the ﬁeld are [15] and [14]. 27 Robustness There is no unique way to construct a classiﬁer. A good classiﬁer is expected to have small generalization error. In addition, we expect a good classiﬁer to be robust: by this, we mean that a small perturbation of an object should not move it to a different class. In this lecture we discuss how to determine the smallest perturbation that would make an object change class, discuss a method of generating such adversarial perturbations, and derive some theoretical limits on robustness. Adversarial Perturbations Let h : Rd → Y be any classiﬁer. Deﬁne the smallest perturbation that moves a data point into a different class as ∆(x) = inf r {∥r∥ : h(x + r) ̸= h(x)}. (27.1) Given a probability distribution on the input spaces, deﬁne the robustness as ρ(h) = E [ ∆h(X) ∥X∥ ] . We begin by discussing the special case of binary classiﬁcation using a hyperplane, which we already saw in Lecture 17 in the context of linear support vector machines (Figure 27.1). The aim was to ﬁnd a hyperplane that separates two sets of points and has maximal margin. Assume we are given linearly separable points x1, . . . , xn ∈ Rd with labels yi ∈ {−1, 1} for i ∈ {1, . . . , n}. A separating hyperplane is deﬁned as a hyperplane H = {x ∈ Rd : wT x + b = 0}, where w ∈ Rd and b ∈ R are such that for all i ∈ {1, . . . , d}, wT xi + b > 0 if yi = 1 wT xi + b < 0 if yi = −1. Deﬁne the classiﬁer h(x) = sign(wT x + b), and let H+ and H− be the open half-spaces where h(x) = 1 and h(x) = −1. Given any point x ̸∈ H, we deﬁne the smallest perturbation and the distance to H as r∗ = argmin 1 2 ∥r∥ 2 subject to wT (x + r) + b = 0. 141 Figure 27.1: A hyperplane separating two sets of points with margin and support vectors. We can obtain a closed form solution either geometrically (as in Lecture 17), or by considering the Lagrangian, since the function to be minimized is convex: L(r, λ) = 1 2 ∥r∥ 2 + λ(wT (x + r) + b). Taking the gradient with respect to r, we get the relation ∇rL(r, λ) = r + λw = 0 ⇒ r∗ = −λ ∗w. (27.2) To determine the Lagrange multiplier, we take the inner product with w and obtain wT r∗ = −λ∗∥w∥2 ⇒ λ∗ = wT r∗ ∥w∥2 . Using the fact that wT r∗ = −(wT x + b), and replacing λ∗ in (27.2), we get r∗ = − wT x + b ∥w∥2 · w, ∆(x) = |wT x + b| ∥w∥ . Note that this result is compatible with the one derived in Lecture 17, where we normalized w and b such that wT x + b = 1. It follows that in order to change a data point x from being classiﬁed as 1 to being classiﬁed as −1, we just have to add (1 + ϵ)r∗ for a small value of ϵ. The setting generalizes to classiﬁcation with more than one class. Assume that we have k linear functions f1, . . . , fk, with fi(x) = wT i x + bi, and a classiﬁer h : Rd → {1, . . . , k} that assigns to each x the index j of the largest value fj(x) (this corresponds to the one-to-many setting for multiple classiﬁcation). Let x be such that maxj fj(x) = fk(x) and deﬁne the linear functions gi(x) := fi(x) − fk(x) = (wi − wk)T x + (bi − bk), i ∈ {1, . . . , k − 1}. Then x ∈ ⋂ 1≤i≤k−1{y : gi(y) < 0}. The intersection of half-spaces is a polyhedron, and x is in the interior of the polyhedron P delineated by the hyperplanes Hi = {y : gi(x) = 0} (see Figure 27.2). H1 H2 H3 H4 H5 x Figure 27.2: The distance to misclassiﬁcation is the radius of the largest enclosed ball in a polyhedron P . A perturbation x + r ceases to be in class k as soon as gj(x + r) > 0 for some j, and the smallest length of such an r equals the radius of the largest enclosed ball in the polyhedron. Formally, noting that wj − wk = ∇gj(x), we get ˆj := arg min j |gj(x)| ∥∇gj(w)∥ , r∗ = − gˆj(x) ∥∇gˆj(x)∥2 · ∇gˆj(x). (27.3) The formulation (27.3) suggests how to approach the case where fi are non-linear functions. In this case, we work with the ﬁrst-order approximation at a point x: fj(y) ≈ fj(x) + ∇fj(x) T (y − x). The DeepFool Algorithm is an iterative greedy algorithm that starts with a point x0, and then for each i ≥ 0, determines xi+1 as the closest point in the boundary of the polyhedron deﬁned by the linearizations of the fj around xi. Once we have determined class in which x lives, we can also use any other suitable algorithm for solving the constrained optimization problem. Figure 27.3 shows an image of a handwritten “4” from the MNIST digits database. A CNN that was trained to accuracy 98% on 10000 test samples correctly identiﬁes the digit, but the perturbed image, while still clearly depicting a 4, is mistaken for an 9 by the same network. Overall, the accuracy of the network drops to 67% when perturbing the whole test data by the same magnitude as that of the perturbation in Figure 27.3. Fundamental Limits To study the effect of random perturbations, we consider a generative model for our data. Let Z = Rm and let Z be a Gaussian random variable with mean 0 and unit covariance on Z. Consider a data generating process g : Z → X , where X = Rd is the data space. Given a classiﬁer f : X → {1, . . . , k}, denote by Ci = f −1(i) the region of X corresponding to class i, i ∈ {1, . . . , k}. Moreover, denote by h = f ◦ g : Z → {1, . . . , k} the composite map. In addition to the error ∆(x), deﬁned in (27.1), we deﬁne the in-distribution error for x = g(z) as ˆ∆(x) = inf{∥g(z + r) − g(z)∥ : r ∈ R m, h(z + r) ̸= h(z)}. Figure 27.3: A correctly identiﬁed 4 and a perturbed image, identiﬁed as a 9. Since ˆ∆ is based on the distance between x in the image of g, we clearly have ∆(x) ≤ ˆ∆(x). In the following, we assume that X = g(Z), that is, X is distributed on X with the push-forward measure of the Gaussian on Z under g. Theorem 27.1. (Fawzi3) Let f : Rd → {1, . . . , k} be any classiﬁer, and let g : Rm → Rd be L-Lipschitz continuous. Then for all ϵ > 0, P{ ˆ∆(X) ≤ ϵ} ≥ 1 − √ π 2 e − ϵ2 2L2 . A consequence of this result is that when the Lipschitz constant L is small with respect to ϵ, then with high probability one will be close to a boundary between classes. Notes The content of this lecture is based on the two papers [18] and [7]. Additional material can be found in [17]. 28 Generative Adversarial Nets In Machine Learning, one distinguishes between discrimitative and generative models. Given an input space X , an output space Y, and a probability distribution on X × Y, the discriminative setting is concerned with the regression function f (x) = E[Y | X = x]. More generally, the goal is to learn the distribution of Y conditioned on X. A generative model, on the other hand, seeks to understand the conditional distribution ρX|Y =y, or more generally, the joint distribution ρX,Y on the product of the input with the output space. A typical example of such a model is the problem of estimating the parameters of a mixture distribution, where the observed data is assumed to come from one of two or more distributions. Understanding the distribution that gave rise to a large set of training data may allow to sample from that distribution, and generate additional “realistic” data that shares many features with the original training data. In this lecture we introduce a framework that has allowed to generate realistic, synthetic data based on neural networks. Generative Adversarial Nets We consider a data space X with probability distribution ρX . A generator is a measurable map G : Z → X , where Z is the latent space with probability measure ρZ. Thus the generator generates data in the input space. The push-forward measure of ρZ on X (i.e., the density of the random variable G(Z), where Z is distributed according to ρZ) is denoted by ρG, and typically differs from ρX . A discriminator is a map D : X → [0, 1]. A Generative Adversarial Net consists of a generator-discriminator pair (G, D), where both G and D are represented by neural networks that are trained according to complementary objectives. The discriminator aims to distinguish training data, sampled from the distribution ρX on X , from data generated by G, or equivalently, sampled from the distribution ρG on X . The generator G aims to make it impossible for D to distinguish between the data it generated and data sampled from ρX . The conﬂicting goals of D and G can be captured using a cost function V (D, G) = EX [log(D(X))] + EZ[log(1 − D(G(Z)))], (28.1) which is a function of the parameters that deﬁne D and G. The goal of D is thus to maximize V (D, G), while the goal of G is to minimize this function. For a ﬁxed generator G, maximizing the expectation (28.1) amounts to maximizing the function ∫ X [log(D(x))ρX (x) + log(1 − D(x))ρG(x)] dx. 145 z G x D D(x) Figure 28.1: Structure of a GAN A short calculation shows that the function log(y)p + log(1 − y)q is maximized at y = p/(p + q), and hence the optimal discriminator is given by D∗(x) = ρX (x) ρX (x) + ρG(x) = 1 − ρG(x) ρX (x) + ρG(x) . (28.2) Remark 28.1. We can interpret the setting as follows. Let X1, X2 be random variables with values in X , where X1 is distributed according to ρX and X2 is distributed according to ρG. Moreover, let Y be a Bernoulli random variable with values in Y = {0, 1} and probability P{Y = 1} = 1/2. Consider then the random variable (X ′, Y ) on X × Y, where X ′ = Y · X1 + (1 − Y ) · X2. We can interpret sampling a point x ∈ X as sampling from the distribution ρX or ρG with equal probability. The conditional densities of X ′ given the values of Y are ρX ′|Y =1(x) = ρX (x), ρX ′|Y =0(x) = ρG(x). Hence, by Bayes rule we get for the regression function (see Lecture 3) E[Y | X = x] = P{Y = 1 | X = x} = P{X = x | Y = 1}P{Y = 1} P{X = x | Y = 1}P{Y = 1} + P{X = x | Y = 0}P{Y = 0} = ρX (x) ρX (x) + ρG(x) = D∗(x). The function D∗(x) thus corresponds to the Bayes classiﬁer, h(x) = { 1 D∗(x) > 1/2 0 D∗(x) ≤ 1/2. In practice, we are only given data samples x1, . . . , xn and an indicator yi, where yi = 1 if xi has been sampled from ρX and yi = 0 if it has been sampled from ρG. The empirical risk to be minimized thus becomes − 1 n n∑ i=1 yi log(D(xi)) + (1 − yi) log(1 − D(xi)), (28.3) which is precisely the log-loss function applied to a classiﬁer that assigns to input data a probability p of coming from ρX . Example 28.2. Let Z = X = R. On Z we take the Gaussian density ρZ(z) = (2π)−1/2 exp(−z2/2), and on X a scaled and shifted normal distribution ρX (x) = (2πσ2)−1/2 exp(−(x − m)2/2σ2). As generator we consider a linear function G(z) = ax + b. Assuming a ﬁxed generator G, the goal of the discriminator is to distinguish between samples coming from the two distributions on X : ρG(x) = 1 √2πa e − (x−b)2 2a2 , ρX (x) = 1 √2πσ e − (x−m)2 2σ2 . In other words, we aim to determine the parameters of a Gaussian mixture distribution, see Figure 28.2. In this example, the function of the form (28.2) can be realized within the class of neural networks with 10 0 10 20 0.00 0.02 0.04 0.06 0.08 0.10 Figure 28.2: A Gaussian mixture distribution with histogram. one hidden layer consisting of two nodes, and normalized radial basis function activation. The optimal discriminator D∗ depends on the generator G. We next aim to minimize V (D∗, G) over all generators. Theorem 28.3. The function V (D∗, G), where D∗ is deﬁned as in (28.1), satisﬁes V (D∗, G) ≥ − log(4), with equality if ρG = ρX . Proof. If ρG = ρX , then V (D∗, G) = − log(4) follows from a direct calculation. To see that this is optimal, write V (D∗, G) = ∫ X log(D∗(x))ρX (x) dx + ∫ X log(1 − D∗(x))ρG(x) dx = ∫ X log ( ρX (x) ρX (x) + ρG(x) ) ρX (x) dx + ∫ X log ( ρG(x) ρX (x) + ρG(x) ) ρG(x) dx = ∫ X log ( 2ρX (x) ρX (x) + ρG(x) ) ρX (x) dx + ∫ X log ( 2ρG(x) ρX (x) + ρG(x) ) ρG(x) dx − log(4) = DKL(ρX ∥ (ρX + ρG)/2) + DKL(ρG∥(ρX + ρG)/2) − log(4), where DKL denotes the Kullback-Leibler divergence of one measure with respect to another. Using Jensen’s Inequality, we see that −DKL(ρX ∥ (ρX + ρG)/2) = ∫ X log ( ρX (x) + ρG(x) 2ρX (x) ) ρX (x) dx ≤ log (∫ X ( ρX (x) + ρG(x) 2ρX (x) ) ρX (x) dx) = log ( 1 2 ∫ X (ρX (x) + ρG(x)) dx ) = log(1) = 0, and similarly with −DKL(ρG ∥ (ρX + ρG)/2). It follows that DKL(ρX ∥ (ρX + ρG)/2) + DKL(ρG ∥ (ρX + ρG)/2) ≥ 0, which completes the proof. In the GAN setting, D(x; w) and G(z; v) are assumed to be neural networks with parameters w and v. The training data for D consists of samples {xi}n i=1 ⊂ X and {G(zj)}n j=1 ⊂ X for random samples zj ∈ Z, and D is trained using stochastic gradient descent applied to the log-loss function (28.3). The training data for G consists of the samples zj, and it is trained using stochastic gradient descent for the loss function 1 n n∑ i=1 log(1 − D(G(zi))). Both networks are trained simultaneously, as shown in Figure 28.3. input : training data {xi}n i=1 output : discriminator D and generator G i = 0, choose w0, v0; while stopping criteria not met for j from 0 to k − 1 wi 0 = wi; sample z1, . . . , zm from prior ρZ on Z; sample xi1, . . . , xim from training data; g = 1 m ∑m ℓ=1 ∇w(log(D(xiℓ; wi j)) + log(1 − D(G(zℓ; vi); wi j))); update wi j+1 = wi j + ηi jg; wi+1 = wi k; sample z1, . . . , zm from prior ρZ on Z; vi+1 = vi − γi 1 m ∑m ℓ=1 ∇v log(1 − D(G(zℓ; vi); wi+1)); Figure 28.3: Training a GAN The choice of learning rates ηi j and γi, as well as the number of iterates k for training D are hyperparameters. Assuming that when given G, at every step it is possible to reach the optimal D∗, then it can be shown that the algorithm converges to an optimal G, with ρG = ρX . In practice, several additional considerations have to be taken into account. For example, one has to decide on a suitable architecture for D and G. In typical imaging applications, one chooses a CNN for D and G. Figure 28.4 shows the result of training such a GAN to generate realistic digits. Figure 28.4: Digits generated by a GAN trained on MNIST digits for 2500 epochs with batch size m = 128. Notes Generative adversarial nets, in the way described here, were introduced by Ian Goodfellow et al in [9]. The GAN setup is also related to the concept of Predictability Minimization, introduced by Schmidhuber in the 1990s. For an overview, see [25]. A wealth of practical information on implementing GANs, as well as other information, can be found under • https://machinelearningmastery.com/start-here/#gans The Magenta project by Google AI provides a framework to generate music and art based, in part, on GANs: • https://magenta.tensorflow.org/ Bibliography [1] A. Beck. First-Order Methods in Optimization. MOS-SIAM Series on Optimization. Society for Industrial and Applied Mathematics, 2017. [2] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223–311, 2018. [3] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp- totic theory of independence. Oxford university press, 2013. [4] Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in Machine Learning, 8(3-4):231–357, 2015. [5] Felipe Cucker and Ding Xuan Zhou. Learning theory: an approximation theory viewpoint, volume 24. Cambridge University Press, 2007. [6] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314, 1989. [7] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classiﬁer. In Advances in Neural Information Processing Systems, pages 1178–1187, 2018. [8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org. [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014. [10] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. A uniﬁed theory of SGD: Variance reduction, sampling, quantization and coordinate descent. arXiv preprint arXiv:1905.11261, 2019. [11] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. Sgd: General analysis and improved rates. arXiv preprint arXiv:1901.09401, 2019. [12] Andreas Griewank. Who invented the reverse mode of differentiation. Documenta Mathematica, Extra Volume ISMP, pages 389–400, 2012. [13] Catherine F Higham and Desmond J Higham. Deep learning: An introduction for applied mathem- aticians. SIAM Review, 61(4):860–891, 2019. 151 [14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu- tional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [15] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [16] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115–133, 1943. [17] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1765–1773, 2017. [18] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574–2582, 2016. [19] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018. [20] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983. [21] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400–407, 1951. [22] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958. [23] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988. [24] Arthur L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, pages 71–105, 1959. [25] Juergen Schmidhuber. Unsupervised minimax: Adversarial curiosity, generative adversarial net- works, and predictability minimization. arXiv preprint arXiv:1906.04493, 2019. [26] Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510–2518, 2014. [27] Terence Tao. Topics in Random Matrix Theory. Graduate studies in mathematics. American Mathematical Society, 2012. [28] Lloyd N Trefethen. Approximation theory and approximation practice, volume 128. Siam, 2013. [29] Vladimir Vapnik. The nature of statistical learning theory. Springer, 2013. [30] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge University Press, 2018. [31] Karl Weierstrass. Über die analytische Darstellbarkeit sogenannter willkürlicher Functionen einer reellen Veränderlichen. Sitzungsberichte der Königlich Preußischen Akademie der Wissenschaften zu Berlin, 2:633–639, 1885.","libVersion":"0.2.3","langs":""}