{"path":"Papers/Papers files/Automating Intervention Discovery from Scientific Literature A Progressive Ontology Prompting and Dual-LLM Framework.pdf","text":"Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework ∗ Yuting Hu 1 , Dancheng Liu 1 , Qingyun Wang 2 , Charles Yu 2 , Chenhui Xu 1 , Qingxiao Zheng 1 , Heng Ji 2 and Jinjun Xiong 1 1University at Buffalo 2University of Illinois at Urbana-Champaign {yhu54,dliu37,cxu26,qingxiao,jinjun}@buffalo.edu, {qingyun4,ctyu2,hengji}@illinois.edu Abstract Identifying effective interventions from the scien- tific literature is challenging due to the high vol- ume of publications, specialized terminology, and inconsistent reporting formats, making manual cu- ration laborious and prone to oversight. To address this challenge, this paper proposes a novel frame- work leveraging large language models (LLMs), which integrates a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On the one hand, the POP algorithm conducts a prioritized breadth-first search (BFS) across a predefined ontology, generating structured prompt templates and action sequences to guide the automatic annotation process. On the other hand, the LLM-Duo system features two special- ized LLM agents, an explorer and an evaluator, working collaboratively and adversarially to con- tinuously refine annotation quality. We show- case the real-world applicability of our framework through a case study focused on speech-language intervention discovery. Experimental results show that our approach surpasses advanced baselines, achieving more accurate and comprehensive anno- tations through a fully automated process. Our ap- proach successfully identified 2,421 interventions from a corpus of 64,177 research articles in the speech-language pathology domain, culminating in the creation of a publicly accessible intervention knowledge base 1 with great potential to benefit the speech-language pathology community. 1 Introduction Evidence-based interventions refer to practices and treat- ments grounded in systematic research and proven effec- tive through controlled studies [Rutten et al., 2021][Melnyk and Fineout-Overholt, 2022]. It emphasizes the use of evi- dence from well-designed and well-conducted research as the foundation for healthcare decision-making [Sackett, 1997]. ∗Corresponding author: Jinjun Xiong (jinjun@buffalo.edu) 1Project website: https://slp.xlabub.com/ Intervention discovery from scientific literature enables re- searchers to keep abreast of the latest advancements and fa- cilitate valuable insights that can significantly enhance the healthcare quality [Usai et al., 2018][Wang et al., 2023a]. However, due to the labor-intensive nature of human review, only a small fraction of intervention knowledge is systemati- cally collected and curated. In healthcare, one of the biggest challenges for healthcare providers is the efficient identifica- tion of relevant intervention evidence from an overwhelming body of research, highlighting the urgent need for automated knowledge extraction tools to streamline the process and en- hance the accessibility of this valuable information. In recent years, large language models (LLMs) have been employed to categorize research papers, extract key findings, summarize complex studies, and create conversational as- sistants for question-answering and note generation, show- ing their impressive ability in understanding and extracting valuable insights from text [Achiam et al., 2023][Li et al., 2024a]. Many studies have utilized LLMs to streamline vari- ous subtasks involved in knowledge graph construction, such as named entity recognition (NER), relation extraction (RE), event extraction (EE), and entity linking (EL) [Wang et al., 2023b][Zhu et al., 2024]. Some research has also explored the collaboration between LLMs and human annotators to improve annotation quality [Kim et al., 2024a][Wang et al., 2024][Tang et al., 2024]. However, extracting intervention knowledge from long-range, domain-specific literature re- mains a significant challenge. On the one hand, developing human-annotated datasets for training deep learning models in NER and RE tasks requires specialized domain expertise to accurately interpret the literature. On the other hand, mining knowledge from long-range documents is a great challenge due to the vast volume of content, the inherent ambiguity of natural language, and the individual bias of human interpre- tation [Ye et al., 2022]. Particularly in healthcare contexts, these challenges are further compounded by the need for spe- cialized therapeutic expertise, labor-intensive manual annota- tion, and difficulties in maintaining consistency and scalabil- ity [Zhao et al., 2021]. In this context, LLMs offer a promis- ing alternative through in-context learning, enabling scalable information extraction without the need for extensive human- labeled data. Despite these advancements, fully automated knowledge graph construction remains a challenge, particu- larly when dealing with long-range documents. Most cur-arXiv:2409.00054v2 [cs.CL] 20 May 2025 rent knowledge graph construction approaches focus on short texts, leaving significant potential for further development in handling more complex, lengthy content. In this paper, we address the challenge of automating in- tervention discovery via LLMs by formulating it as a prompt design and annotation scheduling problem with a predefined intervention ontology graph structure and designing a frame- work leveraging two LLM agents to iteratively enhance the annotation quality. Specifically, we introduce a progres- sive ontology prompting (POP) algorithm that employs an outdegree-prioritized breadth-first search (BFS) across the in- tervention ontology to create a series of prompt templates and action sequences to guide the annotation process con- ducted by LLMs. To enhance the annotation quality, we propose LLM-Duo, an interactive annotation framework by leveraging the power of LLMs while addressing the limita- tions of LLMs. Particularly, it integrates two LLM agents working both collaboratively and adversarially to refine an- notation generation. To showcase the practical impact of our approach, we ap- ply our method in a case study of speech-language interven- tion discovery. We conduct experiments to compare our inter- vention discovery framework with several advanced baselines including long context LLM (i.e., GPT-4-Turbo with 128k context window length), and RAG-based annotation chat- bot with advanced prompting techniques including Chain-of- Thought (CoT) [Wei et al., 2022] and Self-Refine [Madaan et al., 2024]. The experimental results demonstrate that our method not only delivers more accurate and comprehensive annotations over these strong baselines but also significantly accelerates the intervention discovery process. Furthermore, through our framework, we successfully curate a speech- language intervention knowledge base, providing a valuable resource for the speech-language pathology community. To our knowledge, this is the first intervention knowledge base in the speech-language pathology field. Related Work. Traditional approaches to automated knowl- edge discovery typically rely on pipelines to handle various NLP tasks such as named entity recognition, relation ex- traction, coreference resolution, entity linking, and event de- tection [Luan et al., 2018][Martins et al., 2019][Wei et al., 2019][Zhong et al., 2023][Laurenzi et al., 2024]. Recent advancements leverage LLMs to generate relational triplets in zero/few-shot settings for knowledge graph construction, achieving promising results [Wei et al., 2023][Sun et al., 2024][He et al., 2024]. Some studies [Zhang and Soh, 2024][Carta et al., 2023][Vamsi et al., 2024][Zhu et al., 2024] have further streamlined knowledge graph construction by breaking it down into distinct phases, enabling LLMs to in- fer knowledge graph schemas without relying on predefined ontologies. However, these methods are often constrained to short texts or have only been validated on tasks like en- tity and relation extraction using human-annotated datasets, such as DuIE2.0 [Li et al., 2019] and DocRED [Yao et al., 2019], without being proven effective in real-world ap- plications. Moreover, domain-specific knowledge often ex- hibits complex patterns that cannot be captured solely through sentence-level syntactic structures. As a result, most existing approaches [Du et al., 2020][Rossanez et al., 2020][Alam et al., 2023] are limited to handling abstracts and fail to extract and summarize knowledge across long-range contexts. 2 Preliminaries A knowledge graph (KG) is a semantic network structured as an ontology, consisting of concepts and their relationships in a clear, interpretable format at scale [Peng et al., 2023]. For intervention knowledge discovery from literature, LLMs can enhance this process by leveraging their capabilities to understand long-range text. This allows for transforming un- structured data into structured formats, and finally, populating the intervention ontology to create the intervention knowl- edge graph. In our methodology, the intervention KG ontology is crafted by domain experts, which can be represented by a di- rected acyclic graph (DAG) G = (E, R, F). Here E, R, and F are sets of concepts, relationships, and semantic triples re- spectively. F is a collection of triples (h, r, t) with a head concept h ∈ E, a tail concept t ∈ E, and a relation r ∈ R [Gruninger, 1995]. To effectively instruct LLMs to extract intervention knowledge anchored to G automatically, the de- sign of annotation prompts and the query sequences plays a crucial role. We thereby frame the problem of automated in- tervention knowledge discovery via LLMs as one of prompt design and scheduling, described by the following equation: f (G(E, R, F)) = {(P rompti, Orderi)|i ∈ [1, N ]} (1) where f is a function that translates intervention KG ontology into a set of annotation prompts and query sequences for the LLMs. A common case of f is directly prompting LLMs to generate triplets in a zero-shot/few-shot manner by including the whole KG schema within the prompt such as the annota- tion methods used in [Mihindukulasooriya et al., 2023][Kom- mineni et al., 2024]. However, those methods generate an- notations in one shot and ignore the importance of contex- tual correlations between concepts within their surrounding neighborhood, resulting in incomplete annotations. 3 Methodology In this section, we first introduce the POP algorithm that con- verts an intervention KG ontology into a set of annotation prompt templates and query orders, then propose an interac- tive annotation framework based on two LLM agents to en- able more convincing and accurate annotation generations. 3.1 Progressive Ontology Prompting We develop a progressive ontology prompting (POP) algo- rithm that employs a prioritized BFS on the intervention ontology graph G(E, R, F) to generate a set of annotation prompt templates and query sequences for LLMs. In our al- gorithm, the prompt formulation and scheduling follow a pro- gressive manner. As illustrated in Figure 1, the annotation process begins at a source node (i.e., a concept node that only has outgoing edges) and continues by traversing its neigh- boring nodes in the order of a prioritized BFS. To allow for quick accessing a large portion of the graph, we enhance BFS Step2: 'Can you list the patients receive treatments from {#Intervention}?' Step4: 'What are the therapy goals {#Intervention} trying to achieve?' '{#Intervention} conducted {#Therapy Activity}. What are the therapy goals addressed by {#Therapy Activity}?' Step5: 'What disorders does {#Intervention} target at?' '{#Patient} receives treatment from {#Intervention}. What disorders {#Patient} diagnosed with?' ...... Disoder Intervention Patient Therapy Goal Symptom Therapy Activity Treat PresentDiagnose T ar getAt ConductHas SolveFor Step1: 'Can you list the interventions proposed in this paper?' Achieve Disoder Intervention Patient Therapy Goal Symptom Therapy Activity Treat PresentDiagnose T ar getAt ConductHas SolveFor Achieve Disoder Intervention Patient Therapy Goal Symptom Therapy Activity Treat PresentDiagnose T ar getAt ConductHas SolveFor Disoder Intervention Patient Therapy Goal Symptom Therapy Activity Treat PresentDiagnose T ar getAt ConductHas SolveFor AchieveAchieve Disoder Intervention Patient Therapy Goal Symptom Therapy Activity Treat PresentDiagnose T ar getAt ConductHas SolveFor Achieve Step6: '{#Patient} receives treatment from {#Intervention}. What symptoms does {#Patient} have?' '{#Therapy Goas} try to solve {#Disorder}. What symptoms does {#Disorder} present?' ...... Disoder Intervention Patient Therapy Goal Symptom Therapy Activity Treat PresentDiagnose T ar getAt ConductHas SolveFor Achieve Step3: 'Can you list the therapy activities conducted in {#Intervention}?' Figure 1: Illustration of prompt design and scheduling based on the progressive ontology prompting algorithm. by sorting neighboring nodes based on their out-to-in ratio R(v), which is defined by: R(v) = | {(h, r, t) ∈ F} |h = v| | {(h, r, t) ∈ F} |t = v| (2) Our algorithm selects the neighboring node with the max- imum R value to visit in the next step. For instance, in the example of Figure 1, visiting the ‘Patient’ node before the ‘Disorder’ node can provide more context for the ‘Disorder’ concept annotation. For each concept node v, we use the vis- ited nodes within its k hop neighborhood as its context. The P romptv for annotating concept v is crafted based on its con- text and the completed annotations within that context. The action order Orderv for P romptv is determined by the se- quence in which node v is visited during the prioritized BFS traversal. Our algorithm first follows prioritized BFS traversal to cap- ture the local k hop context and visit order of a specific con- cept node v, then composes the annotation prompts P romptv based on its ontology substructure Nk(v) and completed an- notations within its context. This process can be expressed as follows: P romptv ← Tv(Annotation(Nk(v))) (3) Tv ← {P ref ix (Nk−1(u)) ⊕ Question ((v, e, u) | (v, e, u) ∈ F ) | u ∈ N1(v)} (4) , where ⊕ is the concatenation. P romptv represents a set of annotation prompts for node v, generated by applying com- pleted annotations to the prompt template Tv. As illustrated in Figure 1, the prompt template Tv consists of two parts: 1) Question, an annotation question derived from the relation- ship between node v and one of its neighboring nodes u; and 2) Prefix, a description based on the k − 1 hop path of neigh- bor node u. We leverage few-shot learning to task LLMs in generating prompt templates. Figure 2: Iterative annotation with two LLM agents under the LLM- Duo framework. 3.2 LLM-Duo Annotation Framework To guarantee the integrity and reliability of LLM annotations, we propose LLM-Duo, a dual-agent annotation system. The central idea of a multi-agent system is to employ combina- tions of LLMs that can converse with each other to collabo- ratively accomplish tasks [Wu et al., 2023]. Drawing inspira- tion from the multi-agent debate idea in [Kim et al., 2024b], we developed a framework where agents work both collab- oratively and adversarially to enhance the quality of annota- tions. The architecture of LLM-Duo is shown in Figure 2, featur- ing two LLM agents: the explorer and the evaluator. Specif- ically, the explorer is a chatbot performing annotation tasks using zero-shot question answering (QA). To break the con- text window limit of LLMs and ensure the generated anno- tations are faithful to the provided literature content, RAG is employed in explorer to reference relevant sources, minimiz- ing LLM hallucinations. To further improve the accuracy and reliability of the annotations, the evaluator LLM is incorpo- rated to review and validate the explorer’s responses, ensuring higher-quality results. LLM-Duo will be tasked with annotation prompts follow- Question1: What aspects this text is describing?{#Answer1}Question1: One intervention studied in this paper is Auditory-Motor Mapping Training (AMMT). Can you describe this intervention in detail? Answer1: SRT is a control treatment designed to improve speech production accuracy in minimally verbal children with autism spectrum disorder... Aspects1: Objective, Target Population, Approach, Control Group, Outcome Measures, Results, Limitations. Question2: Re-answer my question and cover more aspects except from: {#Aspects1}. Answer2: SRT is a control treatment designed to improve speech production accuracy in minimally verbal children with autism spectrum disorder... 【 SRT involves a series of structured steps to facilitate speech repetition and accuracy. These steps may include...】 Question2: What aspects this text is describing?{#Answer2}. Integrate your ans- wer with {#Aspects1}. Aspects2: Objective, Target Population, Approach, Control Group, Outcome Measures, Results, Limitati-ons,【 Therapeutic Steps, Therapeutic Techniques, Feedback Mechanisms, Duration and Frequency, Multisensory Approach, Individualized Treatment, Maintenance and Generalization, Collaboration with Caregivers.】 Question1: Analyze the following text: 'Answer':{#Answer1}; 'Reason': {#Reason1}; Do you agree with the answer based on the provided reason? Give your argument. Question1: Does this paper present any therapy/ intervention/treatment for individuals having speech or language deficits? Answer1: Yes Reason1: The paper discusses the tests of intonation-based speech treatment for minimally verbal children with autism. Question2: Your answer might be problematic. {#Reason1}. Correct your answer and give strong evidences. Answer1: Disagree Reason1: The 'Reason' does not confirm that any particular methods are proposed or detailed, only that their effects are being tested. Answer2: Yes Reason2: the paper discusses two interventions for minimally verbal children with Autism Spectrum Disorder (ASD): Auditory-Motor Mapping Training (AMMT) and Speech Repetition Therapy (SRT). Question2: Analyze the following text: 'Answer':{#Answer2}; 'Reason': {#Reason2}; Do you agree with the answer based on the provided reason? Give your argument. Answer2: Agree Reason2: The 'Reason' clearly supports the 'Answer' as it states that the paper discusses therapies/interventions for minimally verbal children with Autism Spectrum Disorder. <Rationality> <Completeness> Figure 3: Annotation examples of speech-language intervention discovery using the LLM-Duo framework. ing the sequential order generated by the POP algorithm. During each annotation cycle for a specific concept node, when focusing on concepts that emphasize rationality (e.g., disorder, intervention efficacy), the explorer provides an an- swer and an explanatory rationale to the evaluator. The eval- uator then reviews the reasoning and offers feedback. Based on this feedback, the explorer either refines its answer or, if in disagreement, presents stronger evidence to defend its origi- nal answer and challenge the evaluator’s judgment. For con- cepts that emphasize completeness (e.g., intervention proce- dure, therapy activity), the evaluator extracts the aspects cov- ered in each round of the explorer’s answer, combines them with aspects from previous rounds, and prompts the explorer to expand further beyond the newly integrated aspect collec- tion. This iterative process continues until the annotations reach a consistent and comprehensive state. As the example shown in Figure 3, by facilitating interactive loops between two LLM agents, LLM-Duo enables more accurate and com- plete annotations. 4 Experiments 4.1 Implementation For LLM-Duo, the explorer is a chatbot built on LLM with RAG, implemented with Llamaindex2 framework. We use OpenAI ‘text-embedding-3-large’3 as the embedding model and set the chunk size to 256 tokens with an overlapping size of 128. Particularly, we use ‘FastCoref’ [Otmazgin et al., 2https://www.llamaindex.ai 3https://platform.openai.com/docs/guides/embeddings/embedding-models 2022] to process text chunks for coreference resolution be- fore text embedding. Additionally, we include the document ID as metadata for chunks and apply a metadata filter in the chat engine to ensure that the explorer only answers based on the specific document being annotated. We use Chroma4 as the vector database. We set the retrieval to be on the top 8 text chunks based on similarity scores reranked with Sen- tenceTransformerRerank 5 employing the ‘cross-encoder/ms- marco-MiniLM-L-2-v2’ 6 model in Llamaindex. The evalu- ator is an external LLM who does not share any document context with the explorer. Figure 4: Ontology of speech-language intervention. 4https://github.com/chroma-core/chroma 5https://docs.llamaindex.ai/en/stable/examples/node postprocessor/ SentenceTransformerRerank 6https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-2-v2 4.2 Case Study: Speech-language Intervention Discovery Speech-language therapy provides interventions for individ- uals with speech-language deficits, enhancing their quality of life across various life stages. When choosing an inter- vention, evidence-based practice (EBP) is attractive as it in- tegrates research evidence from literature into the decision- making process to ensure high-quality patient care [Law et al., 1996]. Intervention research, especially studies that offer clear intervention frameworks and comprehensive case stud- ies, are valuable references to guide EBP designs. Interven- tion discovery aims to extensively gather speech-language in- terventions from the literature corpus as references to facili- tate EBP design. It involves identifying relevant studies and extracting essential features of interventions including tar- get disorder, procedure, efficacy, case study, therapy activity, etc., which is extremely labor-intensive for human reviewers, highlighting the efficiency of automating knowledge discov- ery based on LLMs. To verify the effectiveness of our method in a realistic sce- nario, we employ our framework in a speech-language inter- vention discovery setting. The ontology is shown in Figure 4. To enable a large-scale discovery, we cultivate a literature base including 64,177 papers within the domain of speech- language therapy. 4.3 Annotation Baselines The core idea of our automated intervention framework is to leverage the POP algorithm for guiding the annotation pro- cess while utilizing LLM-Duo to refine initial annotations by incorporating external feedback from another LLM. In- stead of setting up another LLM for evaluation, recent stud- ies demonstrate that LLMs can engage in self-correction to enhance their responses autonomously [Liu et al., 2024][Li et al., 2024b]. Notable examples of this include Chain-of- Thought (CoT) [Wei et al., 2022] and Self-Refine [Madaan et al., 2024]. We separately equip the explorer chatbot based on RAG with these two prompting methods for annotation and denote them as CoT-RAG and Self-Refine-RAG. Addi- tionally, in LLM-Duo, a potential substitution of explorer is long-context LLM, which is capable of processing entire doc- ument tokens instead of chunking and retrieval with RAG. We refer to the LLM-Duo system as LLM-Duo-RAG when using explorer built on RAG, and as LLM-Duo-LongContext when utilizing long-context LLMs. Besides, we also compare with methods that directly input paper text to LLMs for zero-shot QA annotation without the evaluation feedback loop, includ- ing ShortContext LLM, LongContext LLM, OpenAI Assis- tant, and RAG. 4.4 Evaluation In the experiment of comparing LLM-Duo with annotation baselines, we report six types of metrics: 1) Consistency Rounds (CR): the number of refine loops the method makes before the annotation generation achieving consistency; 2) Verbosity Index (VI): the number of covered aspects per 1k tokens in the annotations, which is an important metric for emphasizing content completeness; 3) Enumeration Quantity (EQ): the number of items listed in the annotations (i.e., ther- apy activities, therapy goals.); 4) Faithfulness (Faith): the ex- tent of the annotation faithful to the provided literature liter- ature, which is measured by FaithfulnessEvaluator 7 of Lla- maindex. 5) Accuracy (ACC): the percentage of correct an- notations in all LLM-generated annotations. 6) Cover: the percentage of correct LLM-generated annotations to the total mentioned concept entities in the provided literature. 5 Results In this section, we first provide a detailed evaluation of our progressive ontology prompting algorithm and the LLM- Duo annotation framework. Then, we showcase the results of speech-language intervention discoveries using our auto- mated intervention discovery framework. 5.1 POP Algorithm Study Context Size Analysis. In the POP algorithm, the context size k determines the diversity and volume of information in- cluded in the intervention annotation prompt. To assess the impact of context size on annotation quality, we conducted experiments using various k values to generate intervention annotation prompts for LLM-Duo-RAG. As shown in Fig- ure 5a, we annotate the ‘participant’ concept for the experi- ment, which was based on a random selection of 8 speech- language pathology literature. The annotation accuracy is shown in Figure 5b. The re- sults indicate that as the context size k increases, annotation accuracy improves significantly, suggesting that a larger con- text provides more informative prompts, thereby enhancing annotation quality. Moreover, GPT-4-turbo consistently out- performs GPT-3.5-turbo across all k values, demonstrating that more advanced language models can further improve an- notation accuracy. Besides, we inspect the text chunks re- trieved back by different informative annotation queries based on various k values. We report the range of similarity scores and token count distribution of retrieved-back chunks in Fig- ure 5c. The similarity score represents the semantically rel- evancy between retrieved texts to annotation queries. The results show that for k = 1, the retrieved text chunks gen- erally have low similarity to the query, and the token count decreases as the similarity score increases, leading to lower annotation quality. In contrast, higher k values, especially k = 2, yield more relevant retrievals. For k = 2, the to- ken count increases with higher similarity scores, indicating that richer and more relevant content is captured, resulting in improved annotation quality. Prioritized BFS Analysis. In the POP algorithm, we em- ploy the out-to-in ratio to prioritize neighboring nodes dur- ing BFS-based prompt creation and scheduling. This strat- egy ensures that nodes with more outgoing edges are visited first, allowing them to provide more context for annotating other nodes. For example, one annotation sequence follow- ing prioritized POP over the speech-language intervention ontology is ‘TherapyActivity’→‘TherapyGoal’→‘Disorder’. In this section, we compare the ‘Disorder’ annotation results using the POP algorithm with and without prioritization, as 7https://docs.llamaindex.ai/en/stable/examples/evaluation/faithfulness eval (a) Illustration of ‘participant’ annotation under different k values. (b) Range of ‘participant’ annotation accu- racy using LLM-Duo-RAG with GPTs at k=1,2,3. (c) Token count distribution of retrieved-back chunks across varying similarity scores using ‘participant’ annotation queries of k=1,2,3. Figure 5: Evaluation of ‘participant’ annotation with POP of different context sizes. Methods LLM IR ICA IKC CR ACC Cover CR VI EQ Faith CR ACC ShortContext GPT3.5-turbo - 36.9% 50% - 0.0249 5.46 0.9667 - 48.2% OpenAI Assistant GPT4-turbo - 76.1% 69.0% - 0.0631 4.17 0.7857 - 53.3% LongContext GPT4-turbo - 76.3% 57.1% - 0.0919 8.64 1.0 - 61.2% LLM-Duo-LongContext GPT4-turbo 2.17 81.0% 68.7% 2.5 0.0926 8.68 0.8571 1.31 69.6% RAG GPT3.5-turbo - 47.6% 50% - 0.0319 7.96 0.8550 - 48.7% CoT-RAG GPT3.5-turbo 1.04 78.6% 81% 3.18 0.0771 10.37 0.7250 1.07 73.2% Self-Refine-RAG GPT3.5-turbo 1.19 78.5% 54.4% 2.85 0.0694 7.17 0.8125 1.12 54.8% LLM-Duo-RAG GPT3.5-turbo 1.84 100% 86.4% 2.58 0.1159 13.71 0.9285 1.46 85.6% Llama3-instruct-70b 2.71 78.6% 55.6% 2.59 0.0748 9.79 0.8648 1.52 61.0% Mistral-instruct-8x22b 2.30 81.9% 67.5% 2.16 0.0763 9.87 0.8875 1.46 67.2% Table 1: Comparison of annotation results with baselines using different LLMs. well as one-shot annotation without using POP, where the en- tire KG schema is included on a single annotation prompt to extract all triplets. The results are presented in Table 2. We can observe that applying both the POP and prioritized BFS notably enhances annotation performance. LLM-Duo-RAG GPT3.5-turbo GPT4-turbo POP✗ 68.18 72.73 POP✓ Prioritized-BFS✗ 77.28 83.20 POP✓ Prioritized-BFS✓ 81.82 86.37 Table 2: Comparison of annotation results with and without the POP and prioritized BFS. 5.2 LLM-Duo with Baselines In this section, we compare LLM-Duo with several advanced baselines using annotations of 8 randomly selected papers from our speech-language literature corpus. The evaluation focuses on three key dimensions: 1) Intervention Recognition (IR), identifying intervention entities within the literature; 2) Intervention Aspect Summary (IAS), annotating the key as- pects (e.g., procedure, therapy activity, therapy goals) of the intervention by capturing and summarizing relevant informa- tion from the paper; and 3) Intervention Knowledge Com- pletion (IKC), linking interventions to theme classes (e.g., speech awareness, speech articulation, comprehension, foun- dation skills, etc.) and setting concept nodes (e.g., home, healthcare facilities, schools, teletherapy, etc.). We use hu- man annotators for the IR and IKC tasks to generate gold- standard results for comparison. In the IAS task, we only ask human annotators to tag relevant text fragments related to specific intervention aspects due to potential individual bias in human interpretation. The experimental results are reported in Table 1. It should be noted that we implemented ‘ShortContext’ using Llama3-instruct-70b (FP16) and Mistral-instruct-8x22b mod- els (INT8). However, directly prompting these models with full paper text fails to produce annotations in a zero-shot QA setting. Their generations do not align with the annotation questions. The results in Table 1 show that LLM-Duo-RAG outperforms all baselines. Although GPT4-turbo has a 128k context window length and is capable of generating annota- tions, its annotation coverage remains inadequate. Integrat- ing it with the LLM-Duo framework can significantly im- prove both the accuracy and the comprehensiveness of the in- tervention annotations. Additionally, compared with simple RAG, self-correct prompting methods such as CoT and Self- Refine can significantly enhance intervention annotations, but their performance is still worse than LLM-Duo-RAG. Instead of utilizing costly GPT models, LLM-Duo-RAG, which em- ploys open-source models including Llama3-instruct-70b and (a) Ablation studies of remov- ing rerank (wo rerank) and corefe- nence (wo coref) modules in LLM- Duo-RAG. (b) Number of discovered interventions for top 20 dis- orders. (c) Number of discovered interventions for different age groups. Figure 6: Ablation study and annotated speech-language intervention statistics. Disorder Interventions Intervention Examples Aphasia 202 Phonological therapy, Semantic therapy, Syntax Stimulation Program, Melodic Intonation Therapy (MIT), Multimodal Speech Therapy with tDCS, Cross-Language Generalization Therapy (CLGT), Word Learning Paradigm Autism Spectrum Disorder 178 Personalized Idiom Intervention (PII), Classroom-wide peer tutoring, Idiom Isolation Intervention, Hanen More Than Words, Parent-Mediated Communication-Focused Treatment, Picture Exchange Communication System (PECS) Dysphagia 129 Swallowing Maneuver Therapy, Focal Vibration Therapy (FVT), Oral Neuromuscular Training and Vibrational Therapy, Prophylactic Swal- lowing Intervention, High-speed jaw-opening exercise, Palatal Augmentation Prostheses (PAP) Stuttering 91 Lidcombe Program, Syllable-timed speech, Electronic devices for stuttering, Computer software for stuttering, Bone Conduction Delayed Feedback Therapy, Fluency Techniques and Fear Reduction, Cognitive Behavioral Therapy (CBT) Phonological Disorder 74 Nonlinear Phonological Intervention Program, Metronome-paced Speech Therapy, Phonological Awareness and Articulatory Training (PAAT), Phonological Meaning Therapy, Motor-based Intervention Approach AgeGroup Interventions Intervention Examples Children 745 Pharyngeal flap procedure, National Health Service (NHS) 1-week intensive course, Ultrasound Visual Biofeedback (U-VBF), Intensive Speech Therapy, Community-Based Speech Therapy Models, Early Vocal Intervention, Auditory-Verbal Therapy (AVT) School-age Children 571 Intensive Speech Therapy, Early Vocal Intervention, APD intervention, Auditory-Verbal Therapy (AVT), Multisensory Stimulation Therapy, Oral Functional Training (OFT), Rhythmic Reading Training (RRT), Rapid Syllable Transition Treatment (ReST) Older children 366 Semantic Categorization Therapy, Early Vocal Intervention, Rhythmic Reading Training (RRT), Speech Bulb Reduction Program, Intensive speech and language therapy, Peer-Mediated Intervention, Lidcombe Program, Oral Functional Training (OFT) Preschoolers 347 Early Vocal Intervention, Treatment-as-usual, The Lidcombe Program, Oral Functional Training (OFT), Speech Production Therapy with Reward System, Phonological Interventions and Contrast Therapy, Cycles Phonological Remediation Approach Adult 319 Pharyngeal flap procedure, Linguistic Retrieval Therapy (LRT), Oral Hydration Intervention, Physiologic Swallowing Therapy, Myofunctional Intervention (OMT), Orthognathic speech therapy, Eye-Tongue Movement Training, Behavioral Voice Therapy Table 3: Intervention-disorder examples in our discoveries. Mistral-instruct-8x22b, can achieve comparable annotation quality. 5.3 Ablation Study In our implementation, the RAG technique serves as the back- bone of explorer. We employ ‘FastCoref’ for coreference res- olution and rerank retrieved chunks by similarity score using the ‘cross-encoder/ms-marco-MiniLM-L-2-v2 model’. This section presents ablation studies for both components. We re- port the accuracy of intervention recognition in this section. As shown in Figure 6a, the results demonstrate that remov- ing these components significantly decreases annotation ac- curacy, showing the necessity of each module. 5.4 Speech-Language Intervention Discovery Through our automated intervention discovery framework, we identified 2,421 interventions supported by case stud- ies from 64,177 literature in the speech-language pathology domain. The statistics of discovered interventions are pre- sented in Figure 6b and Figure 6c. More intervention ex- amples are provided in Table 3. 19 clinicians and students reviewed our annotations through online Google forms. We have constructed the first intervention knowledge graph in the speech-language pathology domain, which will be made pub- licly accessible upon acceptance. This knowledge graph is anticipated to be a valuable resource for domain experts, fa- cilitating evidence-based clinical decision-making, question- answering, and recommendation systems, ultimately con- tributing to improved healthcare outcomes. 6 Conclusion In this paper, we developed a novel LLM-based framework for automatic intervention discovery from literature, featuring a progressive ontology prompting algorithm and a dual-agent system. The proposed method achieves superior performance compared with advanced baselines, enabling more accurate intervention discoveries. Our approach is adaptable to vari- ous intervention ontologies in healthcare and offers practical value to improve healthcare quality. Acknowledgments The research was supported, in part, by the National AI In- stitute for Exceptional Education (NSF Award #2229873), Center for Early Literacy and Responsible AI (IES Award #R305C240046), FuSe-TG (NSF Award #2235364) and SaTC (NSF Award #2329704). The opinions expressed are those of the authors and do not represent the views of any sponsors. References [Achiam et al., 2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Alt- man, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [Alam et al., 2023] Fakhare Alam, Hamed Babaei Giglou, and Khalid Mahmood Malik. Automated clinical knowl- edge graph generation framework for evidence based medicine. Expert Systems with Applications, 233:120964, 2023. [Carta et al., 2023] Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pom- pianu, and Sandro Gabriele Tiddia. Iterative zero-shot llm prompting for knowledge graph construction. arXiv preprint arXiv:2307.01128, 2023. [Du et al., 2020] Jian Du, Xiaoying Li, et al. A knowledge graph of combined drug therapies using semantic predica- tions from biomedical literature: Algorithm development. JMIR medical informatics, 8(4):e18323, 2020. [Gruninger, 1995] Michael Gruninger. Methodology for the design and evaluation of ontologies. In Proc. IJCAI’95, Workshop on Basic Ontological Issues in Knowledge Shar- ing, 1995. [He et al., 2024] Li He, Hayilang Zhang, Jie Liu, Kang Sun, and Qing Zhang. Zero-shot relation triplet extraction via knowledge-driven llm synthetic data generation. In Inter- national Conference on Intelligent Computing, pages 329– 340. Springer, 2024. [Kim et al., 2024a] Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, and Dan Zhang. Meganno+: A human-llm collaborative annotation system. arXiv preprint arXiv:2402.18050, 2024. [Kim et al., 2024b] Kyungha Kim, Sangyun Lee, Kung- Hsiang Huang, Hou Pong Chan, Manling Li, and Heng Ji. Can llms produce faithful explanations for fact-checking? towards faithful explainable fact-checking via multi-agent debate. arXiv preprint arXiv:2402.07401, 2024. [Kommineni et al., 2024] Vamsi Krishna Kommineni, Bir- gitta K¨onig-Ries, and Sheeba Samuel. From human ex- perts to machines: An llm supported approach to ontol- ogy and knowledge graph construction. arXiv preprint arXiv:2403.08345, 2024. [Laurenzi et al., 2024] Emanuele Laurenzi, Adrian Mathys, and Andreas Martin. An llm-aided enterprise knowledge graph (ekg) engineering process. In Proceedings of the AAAI Symposium Series, volume 3, pages 148–156, 2024. [Law et al., 1996] James Law, Zoe Garrett, Chad Nye, Psy- chosocial Cochrane Developmental, and Learning Prob- lems Group. Speech and language therapy interventions for children with primary speech and language delay or disorder. Cochrane Database of Systematic Reviews, 2015(5), 1996. [Li et al., 2019] Shuangjie Li, Wei He, Yabing Shi, Wenbin Jiang, Haijin Liang, Ye Jiang, Yang Zhang, Yajuan Lyu, and Yong Zhu. Duie: A large-scale chinese dataset for in- formation extraction. In Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9–14, 2019, Proceedings, Part II 8, pages 791–800. Springer, 2019. [Li et al., 2024a] Harry Li, Gabriel Appleby, and Ashley Suh. A preliminary roadmap for llms as assistants in exploring, analyzing, and visualizing knowledge graphs. arXiv preprint arXiv:2404.01425, 2024. [Li et al., 2024b] Loka Li, Zhenhao Chen, Guangyi Chen, Yixuan Zhang, Yusheng Su, Eric Xing, and Kun Zhang. Confidence matters: Revisiting intrinsic self-correction capabilities of large language models, 2024. [Liu et al., 2024] Dancheng Liu, Amir Nassereldine, Ziming Yang, Chenhui Xu, Yuting Hu, Jiajie Li, Utkarsh Kumar, Changjae Lee, and Jinjun Xiong. Large language models have intrinsic self-correction ability, 2024. [Luan et al., 2018] Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of en- tities, relations, and coreference for scientific knowledge graph construction. arXiv preprint arXiv:1808.09602, 2018. [Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. [Mahler and Ramig, 2012] Leslie A Mahler and Lorraine O Ramig. Intensive treatment of dysarthria secondary to stroke. Clinical linguistics & phonetics, 26(8):681–694, 2012. [Martins et al., 2019] Pedro Henrique Martins, Zita Mar- inho, and Andr´e FT Martins. Joint learning of named entity recognition and entity linking. arXiv preprint arXiv:1907.08243, 2019. [Melnyk and Fineout-Overholt, 2022] Bernadette Mazurek Melnyk and Ellen Fineout-Overholt. Evidence-based practice in nursing & healthcare: A guide to best prac- tice. Lippincott Williams & Wilkins, 2022. [Mihindukulasooriya et al., 2023] Nandana Mihinduku- lasooriya, Sanju Tiwari, Carlos F Enguix, and Kusum Lata. Text2kgbench: A benchmark for ontology-driven knowledge graph generation from text. In International Semantic Web Conference, pages 247–265. Springer, 2023. [Otmazgin et al., 2022] Shon Otmazgin, Arie Cattan, and Yoav Goldberg. F-coref: Fast, accurate and easy to use coreference resolution. arXiv preprint arXiv:2209.04280, 2022. [Peng et al., 2023] Ciyuan Peng, Feng Xia, Mehdi Naseri- parsa, and Francesco Osborne. Knowledge graphs: Op- portunities and challenges. Artificial Intelligence Review, 56(11):13071–13102, 2023. [Rossanez et al., 2020] Anderson Rossanez, Julio Cesar Dos Reis, Ricardo da Silva Torres, and H´el`ene de Rib- aupierre. Kgen: a knowledge graph generator from biomedical scientific literature. BMC medical informatics and decision making, 20:1–24, 2020. [Rutten et al., 2021] Lila J Finney Rutten, Xuan Zhu, Aaron L Leppin, Jennifer L Ridgeway, Melanie D Swift, Joan M Griffin, Jennifer L St Sauver, Abinash Virk, and Robert M Jacobson. Evidence-based strategies for clin- ical organizations to address covid-19 vaccine hesitancy. In Mayo clinic proceedings, volume 96, pages 699–707. Elsevier, 2021. [Sackett, 1997] David L Sackett. Evidence-based medicine. In Seminars in perinatology, volume 21, pages 3–5. Else- vier, 1997. [Sun et al., 2024] Qi Sun, Kun Huang, Xiaocui Yang, Rong Tong, Kun Zhang, and Soujanya Poria. Consistency guided knowledge retrieval and denoising in llms for zero- shot document-level relation triplet extraction. In Proceed- ings of the ACM on Web Conference 2024, pages 4407– 4416, 2024. [Tang et al., 2024] Yi Tang, Chia-Ming Chang, and Xi Yang. Pdfchatannotator: A human-llm collaborative multi-modal data annotation tool for pdf-format catalogs. In Proceed- ings of the 29th International Conference on Intelligent User Interfaces, pages 419–430, 2024. [Usai et al., 2018] Antonio Usai, Marco Pironti, Monika Mi- tal, and Chiraz Aouina Mejri. Knowledge discovery out of text data: a systematic review via text mining. Journal of knowledge management, 22(7):1471–1488, 2018. [Vamsi et al., 2024] Krishna Kommineni Vamsi, Vamsi Kr- ishna Kommineni, and Sheeba Samuel. From human ex- perts to machines: An llm supported approach to ontology and knowledge graph construction. 2024. [Wang et al., 2023a] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chan- dak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelli- gence. Nature, 620(7972):47–60, 2023. [Wang et al., 2023b] Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. Gpt-ner: Named entity recognition via large language models. arXiv preprint arXiv:2304.10428, 2023. [Wang et al., 2024] Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan Mitra, and Zhengjie Miao. Human-llm collaborative annotation through effective verification of llm labels. In Proceedings of the CHI Conference on Hu- man Factors in Computing Systems, pages 1–21, 2024. [Wei et al., 2019] Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. A novel cascade binary tagging framework for relational triple extraction. arXiv preprint arXiv:1909.03227, 2019. [Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur- mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. [Wei et al., 2023] Xiang Wei, Xingyu Cui, Ning Cheng, Xi- aobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. Zero-shot infor- mation extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205, 2023. [Wu et al., 2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: En- abling next-gen llm applications via multi-agent conversa- tion framework. arXiv preprint arXiv:2308.08155, 2023. [Yao et al., 2019] Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. Docred: A large-scale document-level relation extraction dataset. arXiv preprint arXiv:1906.06127, 2019. [Ye et al., 2022] Hongbin Ye, Ningyu Zhang, Hui Chen, and Huajun Chen. Generative knowledge graph construction: A review. arXiv preprint arXiv:2210.12714, 2022. [Zhang and Soh, 2024] Bowen Zhang and Harold Soh. Ex- tract, define, canonicalize: An llm-based framework for knowledge graph construction. arXiv preprint arXiv:2404.03868, 2024. [Zhao et al., 2021] Weizhong Zhao, Jinyong Zhang, Jincai Yang, Tingting He, Huifang Ma, and Zhixin Li. A novel joint biomedical event extraction framework via two-level modeling of documents. Information Sciences, 550:27–40, 2021. [Zhong et al., 2023] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on au- tomatic knowledge graph construction. ACM Computing Surveys, 56(4):1–62, 2023. [Zhu et al., 2024] Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Hua- jun Chen, and Ningyu Zhang. Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities. World Wide Web, 27(5):58, 2024. A POP Algorithm A.1 Algorithm Pseudocode Algorithm 1 Progressive Ontology Prompting function ONTOLOGYTRAVERSAL(G, k) Initialize: queue, Order ← [], visited, Khops ← {} Identify the source node s enqueue(queue, (s, 0)), visited[s] ← true, Khops[s] ← [] while not queue.empty() do (current, hop) ← dequeue(queue) Order.append(current) Gather unvisited neighbors of current into neighbors Sort neighbors by out-to-in ratio in descending order for neighbor in neighbors do if not visited[neighbor] then enqueue(queue, (neighbor, hop + 1)) visited[neighbor] ← true Khops[neighbor] ← [] end if if hop + 1 ≤ k then Khops[neighbor].append(current) end if end for end while return Order, Khops end function function MAIN(G, k, T ) Initialize annotations ← {} Order, Khops ← ONTOLOGYTRAVERSAL(G, k) while not Order.empty() do current ← Order.dequeue(), context ← Khops[current] if not context.empty() then discoveries ← {annotations[t] for t in context} prompt ← T (current, Gsub(context), discoveries) else prompt ← T (current) end if annotations[current] ← LLM(prompt) end while end function A.2 Annotation Prompt Template Generation In the POP algorithm, the annotation prompt for a specific concept node is generated from a template T , which is constructed using the node’s context (the k-hop visited neighborhood) and the completed annotations within that context. We carefully design the prompt to task LLM to generate T as follows: The following triplets outline an annotation ontology:{(Intervention, StudiedIn, Case Study), (Intervention, Include, Participant), (Case Study, UsedWith, Frequency)}. All concept nodes have been annotated except from {#Frequency}. Your task is to create all possible annotation prompt templates for the {#Frequency}, leveraging the ontology structure. Example: Ontology: [(Intervention, StudiedIn, Case Study), (Intervention, TargetAt, Disorder), (Case Study, Include, Participant), (Participant, Has, Disorder)]. Annotate: Participant Prompts: T1: Who is the participant in {#case study}? T2: {#Intervention} is studied in {#case study}. Who is the participant in {#case study}? T3: {#Intervention}, targeting {#Disorder}, is studied in {#case study}. Who is the participant in {#case study}? Some example output of GPT3.5 for the above prompt is: T1: What is the frequency used in {#case study}? T2: In the context of {#intervention} studied in {#case study}, what is the frequency of the intervention? T3: {#Intervention} is studied in {#case study} with {#Participant}. What is the frequency of the intervention in {#case study}? Since template T is composed of concept nodes visited before the current node, the final annotation prompt for the current node is derived by incorporating the annotations associated with these concept nodes into T . B Speech-Language Intervention Annotation B.1 Literature Corpus We cultivate a literature corpus of 64,177 research articles within the domain of speech-language pathology to facilitate intervention discovery. To conduct our literature search, we use a collection of carefully selected keywords drawn from a glossary of commonly used terms in speech-language pathology. These keywords include: “speech language therapy, speech language disorder, speech sound disorder, articulation disorder, speech intervention, language intervention, auditory discrimination, auditory processing disorder, phonological awareness, phonological processes, auditory perception, babbling, motor speech disorder, morpheme, phonology, prosody, stuttering, lan- guage impairment, speech language pathologist, speech and language therapist, babbling, expressive language delay, cleft speech disorder, autism spectrum disorder, develop- mental phonological disorder, developmental stuttering, phonological impairment, developmental dysarthria, down syndrome, swallowing disorder, communication impairment, articulation impairment, dyslexia, apraxia, dysarthria, dysphagia, communication disorder, expressive language disorder, dyspraxia, aphasia, augmentative and alternative communication, central auditory processing disorder, cleft lip and palate, down syndrome, fluency disorders, hearing loss, orofacial myofunctional disorders, spoken language disorders, written language disorders, acquired brain in- jury, apraxia of speech, auditory comprehension, literacy impairments, voice difficulties, language-based learning disabilities.” Theme Definition speech awareness work that involves recognizing and understanding speech sounds. It includes phonological awareness (identifying and manipulating sounds), auditory discrimination (distinguishing sounds), and sound identification (recognizing the meaning of sounds). speech articulation work that refers to the physical production of speech sounds using the mouth, lips, tongue, and respiratory system. It focuses on the clarity and accuracy of pronouncing phonemes and forming them into words. comprehension work that aims to improve the understanding of (receptive) language. expressive language work that aims to improve the children’s expressive language, in quantity, vocabulary or structure. self-monitoring work designed to help the patient’s awareness of their speech and language difficulties and how they might be able to overcome them. generalisation work to help make speech and language or therapy gains transferable to other situations and environments. foundation skills work to practise and improve a range of early skills, many of which might be considered foundations for speech and language development. functional communication work focusing on those aspects of communication that help the child’s involvement and participation in life situations; this might be functional language, signing or the use of symbols. adult understanding and empowerment work that helps parents to understand the nature of their child’s speech and language difficulty, what helps to improve it and why. adult-child interaction work on the interaction between the parent/adult and the child. All of the changes to adult/parent-child interactions were emphasised in terms of those that encourage speech and language development. Table 4: Definition of intervention themes. B.2 Annotation Ontology Speech-language pathology provides interventions for indi- viduals with speech-language deficits, improving their qual- ity of life at various stages. When choosing an intervention, evidence-based practice (EBP) is attractive as it integrates re- search evidence from literature into the decision-making pro- cess to ensure high-quality patient care [Law et al., 1996]. Research on interventions, especially those presenting clear frameworks and comprehensive case studies, serves as valu- able guidance for designing EBPs. In this paper, we apply our automated framework to speech-language intervention dis- covery, with the intervention ontology shown in Figure 4. A detailed explanation of the concepts within the ontology is provided below: • Intervention represents a targeted treatment practice de- signed to enhance an individual’s communication skills. • Disorder represents the type of disorder that causes dif- ficulties in an individual’s voice, speech, language, or swallowing functions. • Setting represents a specific environment where inter- ventions are implemented. We identify six key settings: home, healthcare facilities (such as hospitals or rehabil- itation centers), early childhood centers (like nurseries or daycare), schools, clinics and private practices, and teletherapy. • Theme represents the theme of the intervention. As shown in Table 4, we categorize interventions into 10 themes based on their characteristics and therapy goals. • Therapy Activity represents a task designed to address a particular speech or language challenge in an individ- ual, such as using a minimal pairs activity to enhance phonological awareness. • Therapy Goal represents a specific area that the inter- vention is designed to enhance. • Procedure represents a comprehensive description of how the intervention is carried out. • Efficacy represents the conclusion about the effective- ness of the intervention. • Frequency/Dosage/Duration represents the frequen- cy/dosage/ duration of the intervention practiced in the case study that demonstrates its efficacy. • Case Study represents a detailed examination of the in- tervention on a particular individual or group with com- munication disorders. The purpose of a case study is to provide a deep understanding of the patient’s unique needs and assess the intervention’s effectiveness. • Participant represents the individuals or populations that are involved in the case study of intervention. • Age represent the age of experiment participant or claimed target population of the intervention. The age is quantified with a granularity of half a year. We addition- ally convert age to age groups including “newborn, in- fants, toddlers, children, preschoolers, school-age chil- dren, older children, youth, teens, adolescents, adult, young adult, middle aged, aged, senior”. Each specific age may be associated with multiple age groups. For in- stance, an individual aged 13 years could be categorized into the ‘teens,’ ‘adolescents,’ and ‘children’ age groups. • Language represents the speaking language of the ex- periment participant in the case study of the interven- tion. C CoT & Self-Refine Baselines Instead of using an external LLM to provide evaluation feed- back to the explorer, we use CoT and SelfRefine prompting techniques as baselines to task explorer refine annotations independently. The prompts of CoT and SelfRefine are as follows: Background: Your last answer to my question {#init annotation question} is: {#last annotation}. --- *CoT: Prompt_CoT1: \"{#Background} Make a plan to correctly answer my question again.\" Prompt_CoT2: \"{#Background} Make a plan to answer my question again with more comprehensive results.\" --- *SelfRefine: Prompt_SelfRefine_Feedback1: \"{# Background} Reflect your answer. Analyze the correctness of the information provided. Provide critque to help improve the answer . Your feedback:\" Prompt_SelfRefine_Refine1: \"{# Background} Critics: {#feedback}. Based on your last answer and its critics, revise your answer to my question. Your answer:\" Prompt_SelfRefine_Feedback2: \"{# Background} Reflect your answer. Analyze the included aspects in your answer. Provide critque to help make the response more comprehensive. Your feedback:\" Prompt_SelfRefine_Refine2: \"{# Background} Critics: {#feedback}. Based on your last answer and its critics, revise your answer to my question. Your answer:\" D Annotation Example Below is an example of an intervention annotation from the paper ”Intensive Treatment of Dysarthria Secondary to Stroke” [Mahler and Ramig, 2012]. \"Intervention\": \"Lee Silverman Voice Treatment (LSVT LOUD)\", \"Disorder\": [ \"stroke\", \"ataxia\", \"multiple sclerosis\", \"traumatic brain injury (TBI)\" ], \"Procedure\": \"The therapy process of conducting the Lee Silverman Voice Treatment (LSVT LOUD) intervention involves intensive high-effort exercises aimed at increasing vocal loudness to a level within normal limits using healthy and efficient voice techniques. The treatment protocol includes sessions four times a week for 4 weeks, totaling 16 individual one-hour sessions. Each session consists of tasks such as maximal sustained vowel phonation, pitch range exercises, and reading functional phrases at individual target loudness levels. The second half of each session progresses to functional speech tasks, moving from words and phrases to conversation over the course of the 16 sessions. Additionally, participants are assigned daily homework to practice using normal loudness and facilitate generalization of normal loudness outside the treatment room.\", \"Frequency\": \"four times a week\", \"Dosage\": \"one-hour session.\", \"Duration\": \"4 weeks, totaling 16 individual one-hour sessions.\", \"Efficacy\": \"The outcome of the Lee Silverman Voice Treatment (LSVT LOUD) intervention in this study showed positive changes in acoustic variables of speech for all participants with dysarthria secondary to stroke. There were statistically significant increases in vocal dB SPL for sustained vowel phonation and speech tasks, indicating improvements in loudness levels and phonatory stability. Additionally, post- treatment speech samples were rated as having better voice quality and sounding more natural, suggesting an amelioration of dysarthria characteristics. Participants also reported increased confidence in their speech during post-treatment interviews.\", \"Therapy Goal\": [ \"Increase vocal loudness to a level within normal limits\", \"Use healthy and efficient voice techniques\", \"Progress from words and phrases to conversation over 16 sessions\", \"Facilitate generalization of normal loudness outside the treatment room\" ], \"Participant\": \"Four participants (P01 to P04).\", \"Age\": \"Participants in the study ranged in age from 50 to 74 years.\", \"Language\": \"assume to be English.\", \"Case Study\": \"Case studies and experiments regarding the Lee Silverman Voice Treatment (LSVT LOUD) intervention in the paper include studies on people diagnosed with stroke, ataxia, multiple sclerosis, and traumatic brain injury (TBI). These studies have shown improvement in articulatory features as well as loudness.\", \"Therapy Activity\": [ \"Maximal sustained vowel phonation\", \"Pitch range exercises\", \"Reading 10 functional phrases at individual target loudness levels\", \"Functional speech tasks progressing from words and phrases to conversation\" ], \"Setting\": \"home\", \"Theme\": \"speech articulation\" E Speech-Language Intervention Knowledge Base Through our intervention discovery framework, we con- structed the first intervention knowledge graph in the speech- language pathology domain. Our intervention KG contains 33,148 nodes with 324,707 relations. We present some views of our intervention KG in Figure 7 and Figure 8. Figure 7: Partial views of Intervention-Disorder in the intervention KG. Figure 8: Partial views of Intervention-TherapyActivity in intervention KG.","libVersion":"0.3.2","langs":""}